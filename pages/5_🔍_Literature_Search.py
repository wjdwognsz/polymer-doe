"""
ğŸ” literature_search.py - í†µí•© ì—°êµ¬ ìì› ê²€ìƒ‰
ë¬¸í—Œ, ì‹¤í—˜ í”„ë¡œí† ì½œ, ë°ì´í„°, ì˜¤í”ˆ ì‚¬ì´ì–¸ìŠ¤ ë¦¬ì†ŒìŠ¤ë¥¼ í†µí•© ê²€ìƒ‰í•˜ê³  AIë¡œ ë¶„ì„
"""

import streamlit as st
import pandas as pd
import numpy as np
import requests
import json
import re
import asyncio
from datetime import datetime, timedelta
import networkx as nx
import plotly.graph_objects as go
import plotly.express as px
from typing import Dict, List, Optional, Tuple, Any
import base64
from io import BytesIO
import tempfile
import os
from concurrent.futures import ThreadPoolExecutor, as_completed
import hashlib

# ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    from scholarly import scholarly
except ImportError:
    scholarly = None
    
try:
    import arxiv
except ImportError:
    arxiv = None
    
try:
    import PyPDF2
    import fitz  # PyMuPDF
except ImportError:
    PyPDF2 = None
    fitz = None
    
try:
    from github import Github
except ImportError:
    Github = None
    
try:
    import bibtexparser
except ImportError:
    bibtexparser = None

# ë‚´ë¶€ ëª¨ë“ˆ
from utils.auth_manager import check_authentication, get_current_user
from utils.sheets_manager import GoogleSheetsManager
from utils.api_manager import APIManager
from utils.common_ui import render_header, show_success, show_error, show_info, show_warning
from utils.notification_manager import NotificationManager

class IntegratedResearchManager:
    """í†µí•© ì—°êµ¬ ìì› ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.sheets = GoogleSheetsManager()
        self.api = APIManager()
        self.notifier = NotificationManager()
        self.current_user = get_current_user()
        self.project_id = st.session_state.get('current_project', {}).get('id')
        
        # API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self._init_api_clients()
        
        # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
        self._initialize_session_state()
        
    def _initialize_session_state(self):
        """ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”"""
        if 'search_results' not in st.session_state:
            st.session_state.search_results = None
        if 'selected_paper' not in st.session_state:
            st.session_state.selected_paper = None
        if 'selected_protocol' not in st.session_state:
            st.session_state.selected_protocol = None
        if 'search_history' not in st.session_state:
            st.session_state.search_history = []
        if 'saved_resources' not in st.session_state:
            st.session_state.saved_resources = []
        if 'show_ai_details' not in st.session_state:
            st.session_state.show_ai_details = False
            
    def _init_api_clients(self):
        """ì™¸ë¶€ API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        self.github_client = None
        if Github and st.secrets.get('github', {}).get('token'):
            try:
                self.github_client = Github(st.secrets.github.token)
            except:
                pass
                
    def render_page(self):
        """í†µí•© ê²€ìƒ‰ í˜ì´ì§€ ë©”ì¸ ë Œë”ë§"""
        render_header("ğŸ” í†µí•© ì—°êµ¬ ìì› ê²€ìƒ‰", 
                     "ë¬¸í—Œ, í”„ë¡œí† ì½œ, ë°ì´í„°ë¥¼ í•œ ë²ˆì— ê²€ìƒ‰í•˜ê³  AIë¡œ ë¶„ì„í•˜ì„¸ìš”")
        
        # ê²€ìƒ‰ ì¸í„°í˜ì´ìŠ¤
        self._render_search_interface()
        
        # ê²€ìƒ‰ ê²°ê³¼
        if st.session_state.search_results:
            self._render_search_results()
        else:
            # ìµœê·¼ ê²€ìƒ‰ ë° ì €ì¥ëœ ìë£Œ
            col1, col2 = st.columns(2)
            with col1:
                self._render_recent_searches()
            with col2:
                self._render_saved_resources()
                
    def _render_search_interface(self):
        """ê²€ìƒ‰ ì¸í„°í˜ì´ìŠ¤ ë Œë”ë§"""
        with st.container():
            # ìŠ¤ë§ˆíŠ¸ ê²€ìƒ‰ ë°”
            col1, col2 = st.columns([4, 1])
            
            with col1:
                search_query = st.text_area(
                    "ë¬´ì—‡ì„ ì°¾ê³  ê³„ì‹ ê°€ìš”?",
                    placeholder="ì˜ˆ: PLA 3D í”„ë¦°íŒ… ìµœì  ì¡°ê±´, PEDOT:PSS ì „ë„ë„ í–¥ìƒ ë°©ë²•...",
                    height=80,
                    key="search_query"
                )
                
                # ê²€ìƒ‰ ì œì•ˆ
                if search_query and len(search_query) > 3:
                    suggestions = self._get_search_suggestions(search_query)
                    if suggestions:
                        st.info(f"ğŸ’¡ ì¶”ì²œ í‚¤ì›Œë“œ: {', '.join(suggestions)}")
                        
            with col2:
                st.write("")  # ê³µë°±
                search_button = st.button(
                    "ğŸ” í†µí•© ê²€ìƒ‰",
                    type="primary",
                    use_container_width=True,
                    disabled=not search_query
                )
                
            # ê³ ê¸‰ ê²€ìƒ‰ ì˜µì…˜
            with st.expander("ğŸ”§ ê³ ê¸‰ ê²€ìƒ‰ ì˜µì…˜", expanded=False):
                col1, col2, col3, col4 = st.columns(4)
                
                with col1:
                    resource_types = st.multiselect(
                        "ë¦¬ì†ŒìŠ¤ íƒ€ì…",
                        ["ğŸ“„ ë…¼ë¬¸", "ğŸ“‹ íŠ¹í—ˆ", "ğŸ”¬ í”„ë¡œí† ì½œ", 
                         "ğŸ“Š ì‹¤í—˜ë°ì´í„°", "ğŸ’» ì½”ë“œ", "ğŸ§ª ì¬ë£Œì •ë³´"],
                        default=["ğŸ“„ ë…¼ë¬¸", "ğŸ”¬ í”„ë¡œí† ì½œ", "ğŸ“Š ì‹¤í—˜ë°ì´í„°"]
                    )
                    
                    sources = st.multiselect(
                        "ë°ì´í„° ì†ŒìŠ¤",
                        ["Google Scholar", "arXiv", "Patents", "protocols.io", 
                         "GitHub", "Zenodo", "Materials Project"],
                        default=["Google Scholar", "arXiv", "GitHub"]
                    )
                    
                with col2:
                    date_range = st.date_input(
                        "ê¸°ê°„",
                        value=(datetime.now() - timedelta(days=365*2), datetime.now()),
                        format="YYYY-MM-DD"
                    )
                    
                    languages = st.multiselect(
                        "ì–¸ì–´",
                        ["ì˜ì–´", "í•œêµ­ì–´", "ì¼ë³¸ì–´", "ì¤‘êµ­ì–´"],
                        default=["ì˜ì–´", "í•œêµ­ì–´"]
                    )
                    
                with col3:
                    min_citations = st.number_input(
                        "ìµœì†Œ ì¸ìš©ìˆ˜",
                        min_value=0,
                        value=0,
                        step=10
                    )
                    
                    verified_only = st.checkbox("ê²€ì¦ëœ ìë£Œë§Œ")
                    has_raw_data = st.checkbox("ì›ë³¸ ë°ì´í„° í¬í•¨")
                    
                with col4:
                    polymer_types = st.multiselect(
                        "ê³ ë¶„ì ìœ í˜•",
                        ["ì—´ê°€ì†Œì„±", "ì—´ê²½í™”ì„±", "ì—˜ë¼ìŠ¤í† ë¨¸", 
                         "ë°”ì´ì˜¤í´ë¦¬ë¨¸", "ì „ë„ì„±", "ê¸°ëŠ¥ì„±"],
                        help="ê´€ë ¨ ê³ ë¶„ì ìœ í˜• ì„ íƒ"
                    )
                    
                    properties = st.multiselect(
                        "ê´€ì‹¬ ë¬¼ì„±",
                        ["ê¸°ê³„ì ", "ì—´ì ", "ì „ê¸°ì ", "ê´‘í•™ì ", 
                         "í™”í•™ì ", "ìƒë¶„í•´ì„±"],
                        help="ê´€ë ¨ ë¬¼ì„± ë°ì´í„° ìš°ì„  ê²€ìƒ‰"
                    )
                    
        # ê²€ìƒ‰ ì‹¤í–‰
        if search_button and search_query:
            self._execute_integrated_search(
                query=search_query,
                resource_types=resource_types,
                sources=sources,
                filters={
                    'date_range': date_range,
                    'languages': languages,
                    'min_citations': min_citations,
                    'verified_only': verified_only,
                    'has_raw_data': has_raw_data,
                    'polymer_types': polymer_types,
                    'properties': properties
                }
            )
            
    def _execute_integrated_search(self, query: str, resource_types: List[str], 
                                  sources: List[str], filters: Dict):
        """í†µí•© ê²€ìƒ‰ ì‹¤í–‰"""
        with st.spinner("ì—¬ëŸ¬ ì†ŒìŠ¤ì—ì„œ ìë£Œë¥¼ ê²€ìƒ‰í•˜ëŠ” ì¤‘... (15-30ì´ˆ ì†Œìš”)"):
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            results = {
                'papers': [],
                'protocols': [],
                'datasets': [],
                'materials': [],
                'code': [],
                'total_count': 0,
                'search_time': datetime.now()
            }
            
            # ë³‘ë ¬ ê²€ìƒ‰ ì‹¤í–‰
            with ThreadPoolExecutor(max_workers=10) as executor:
                futures = {}
                
                # 1. ë¬¸í—Œ ê²€ìƒ‰ (30%)
                if "ğŸ“„ ë…¼ë¬¸" in resource_types:
                    if "Google Scholar" in sources and scholarly:
                        futures[executor.submit(self._search_google_scholar, query, filters)] = 'scholar'
                    if "arXiv" in sources and arxiv:
                        futures[executor.submit(self._search_arxiv, query, filters)] = 'arxiv'
                        
                if "ğŸ“‹ íŠ¹í—ˆ" in resource_types:
                    if "Patents" in sources:
                        futures[executor.submit(self._search_patents, query, filters)] = 'patents'
                        
                progress_bar.progress(30)
                status_text.text("ğŸ“š í•™ìˆ  ë¬¸í—Œ ê²€ìƒ‰ ì¤‘...")
                
                # 2. í”„ë¡œí† ì½œ ê²€ìƒ‰ (50%)
                if "ğŸ”¬ í”„ë¡œí† ì½œ" in resource_types:
                    if "protocols.io" in sources:
                        futures[executor.submit(self._search_protocols_io, query, filters)] = 'protocols'
                    # PDFì—ì„œ í”„ë¡œí† ì½œ ì¶”ì¶œì€ ë…¼ë¬¸ ê²€ìƒ‰ í›„ ìˆ˜í–‰
                    
                progress_bar.progress(50)
                status_text.text("ğŸ”¬ ì‹¤í—˜ í”„ë¡œí† ì½œ ê²€ìƒ‰ ì¤‘...")
                
                # 3. ë°ì´í„° ë° ì½”ë“œ ê²€ìƒ‰ (70%)
                if "ğŸ“Š ì‹¤í—˜ë°ì´í„°" in resource_types or "ğŸ’» ì½”ë“œ" in resource_types:
                    if "GitHub" in sources and self.github_client:
                        futures[executor.submit(self._search_github, query, filters)] = 'github'
                    if "Zenodo" in sources:
                        futures[executor.submit(self._search_zenodo, query, filters)] = 'zenodo'
                        
                progress_bar.progress(70)
                status_text.text("ğŸ“Š ì‹¤í—˜ ë°ì´í„° ê²€ìƒ‰ ì¤‘...")
                
                # 4. ì¬ë£Œ ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰ (85%)
                if "ğŸ§ª ì¬ë£Œì •ë³´" in resource_types:
                    if "Materials Project" in sources:
                        futures[executor.submit(self._search_materials_project, query, filters)] = 'materials'
                        
                progress_bar.progress(85)
                status_text.text("ğŸ§ª ì¬ë£Œ ë°ì´í„°ë² ì´ìŠ¤ ì¡°íšŒ ì¤‘...")
                
                # ê²°ê³¼ ìˆ˜ì§‘
                for future in as_completed(futures):
                    source = futures[future]
                    try:
                        source_results = future.result()
                        
                        if source in ['scholar', 'arxiv', 'patents']:
                            results['papers'].extend(source_results)
                        elif source == 'protocols':
                            results['protocols'].extend(source_results)
                        elif source in ['github', 'zenodo']:
                            # ë°ì´í„°ì™€ ì½”ë“œ ë¶„ë¥˜
                            for item in source_results:
                                if item.get('type') == 'code':
                                    results['code'].append(item)
                                else:
                                    results['datasets'].append(item)
                        elif source == 'materials':
                            results['materials'].extend(source_results)
                            
                    except Exception as e:
                        show_warning(f"{source} ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                        
            # 5. AI í†µí•© ë¶„ì„ (100%)
            progress_bar.progress(90)
            status_text.text("ğŸ¤– AIê°€ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ê³  ì—°ê²°í•˜ëŠ” ì¤‘...")
            
            # ì¤‘ë³µ ì œê±°
            results = self._deduplicate_results(results)
            
            # AI ë¶„ì„ ë° ì—°ê²°
            results = self._analyze_and_connect_results(results, query)
            
            # ì´ ê°œìˆ˜
            results['total_count'] = sum(len(v) for v in results.values() if isinstance(v, list))
            
            progress_bar.progress(100)
            status_text.text("âœ… ê²€ìƒ‰ ì™„ë£Œ!")
            
            # ê²°ê³¼ ì €ì¥
            st.session_state.search_results = results
            
            # ê²€ìƒ‰ ê¸°ë¡ ì €ì¥
            self._save_search_history(query, results)
            
    def _render_search_results(self):
        """ê²€ìƒ‰ ê²°ê³¼ ë Œë”ë§"""
        results = st.session_state.search_results
        
        # ê²°ê³¼ ìš”ì•½
        col1, col2, col3, col4, col5 = st.columns(5)
        
        with col1:
            st.metric("ì´ ê²°ê³¼", results['total_count'])
        with col2:
            st.metric("ë…¼ë¬¸", len(results.get('papers', [])))
        with col3:
            st.metric("í”„ë¡œí† ì½œ", len(results.get('protocols', [])))
        with col4:
            st.metric("ë°ì´í„°ì…‹", len(results.get('datasets', [])))
        with col5:
            st.metric("ì½”ë“œ", len(results.get('code', [])))
            
        # AI ì¸ì‚¬ì´íŠ¸
        if results.get('ai_insights'):
            self._render_ai_insights(results['ai_insights'])
            
        # ê²°ê³¼ íƒ­
        tabs = st.tabs([
            "ğŸ”— í†µí•© ë·°", 
            "ğŸ“„ ë…¼ë¬¸", 
            "ğŸ”¬ í”„ë¡œí† ì½œ", 
            "ğŸ“Š ë°ì´í„°", 
            "ğŸ’» ì½”ë“œ", 
            "ğŸ§ª ì¬ë£Œ"
        ])
        
        with tabs[0]:
            self._render_integrated_view(results)
        with tabs[1]:
            self._render_papers_tab(results.get('papers', []))
        with tabs[2]:
            self._render_protocols_tab(results.get('protocols', []))
        with tabs[3]:
            self._render_datasets_tab(results.get('datasets', []))
        with tabs[4]:
            self._render_code_tab(results.get('code', []))
        with tabs[5]:
            self._render_materials_tab(results.get('materials', []))
            
    def _render_integrated_view(self, results: Dict):
        """í†µí•© ë·° ë Œë”ë§"""
        st.subheader("ğŸ”— ì—°ê²°ëœ ë¦¬ì†ŒìŠ¤ ê·¸ë£¹")
        
        resource_groups = results.get('resource_groups', [])
        
        if not resource_groups:
            show_info("ì—°ê²°ëœ ë¦¬ì†ŒìŠ¤ ê·¸ë£¹ì´ ì—†ìŠµë‹ˆë‹¤. ê°œë³„ íƒ­ì—ì„œ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            return
            
        for idx, group in enumerate(resource_groups):
            with st.container():
                # ê·¸ë£¹ í—¤ë”
                col1, col2 = st.columns([4, 1])
                
                with col1:
                    st.markdown(f"### {idx + 1}. {group['title']}")
                    st.caption(group['description'])
                    
                with col2:
                    relevance = group.get('relevance_score', 0)
                    st.metric("ê´€ë ¨ë„", f"{relevance:.0%}")
                    
                # ê·¸ë£¹ ë‚´ ë¦¬ì†ŒìŠ¤
                cols = st.columns(3)
                
                # ë…¼ë¬¸
                if group.get('paper'):
                    with cols[0]:
                        self._render_paper_card_mini(group['paper'])
                        
                # í”„ë¡œí† ì½œ
                if group.get('protocol'):
                    with cols[1]:
                        self._render_protocol_card_mini(group['protocol'])
                        
                # ë°ì´í„°ì…‹
                if group.get('dataset'):
                    with cols[2]:
                        self._render_dataset_card_mini(group['dataset'])
                        
                # AI ì„¤ëª…
                if group.get('explanation'):
                    with st.expander("ğŸ¤– AI ë¶„ì„ - ë¦¬ì†ŒìŠ¤ ì—°ê²° ê·¼ê±°"):
                        st.write(group['explanation'])
                        
                st.divider()
                
    def _render_papers_tab(self, papers: List[Dict]):
        """ë…¼ë¬¸ íƒ­ ë Œë”ë§"""
        if not papers:
            show_info("ê²€ìƒ‰ëœ ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
            
        # ì •ë ¬ ì˜µì…˜
        sort_by = st.selectbox(
            "ì •ë ¬",
            ["ê´€ë ¨ë„ìˆœ", "ìµœì‹ ìˆœ", "ì¸ìš©ìˆœ"],
            key="paper_sort"
        )
        
        sorted_papers = self._sort_results(papers, sort_by)
        
        for paper in sorted_papers:
            self._render_paper_card(paper)
            
    def _render_paper_card(self, paper: Dict):
        """ë…¼ë¬¸ ì¹´ë“œ ë Œë”ë§"""
        with st.container():
            # ì œëª© ë° ì €ì
            st.markdown(f"### {paper['title']}")
            st.caption(f"ğŸ‘¥ {', '.join(paper.get('authors', [])[:3])}{'...' if len(paper.get('authors', [])) > 3 else ''}")
            
            # ë©”íƒ€ë°ì´í„°
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.write(f"ğŸ“… {paper.get('year', 'N/A')}")
            with col2:
                st.write(f"ğŸ“– {paper.get('venue', 'N/A')}")
            with col3:
                st.write(f"ğŸ“Š ì¸ìš©: {paper.get('citations', 0)}")
            with col4:
                if paper.get('doi'):
                    st.write(f"ğŸ”— DOI: {paper['doi']}")
                    
            # ì´ˆë¡
            if paper.get('abstract'):
                with st.expander("ì´ˆë¡ ë³´ê¸°"):
                    st.write(paper['abstract'])
                    
            # ì•¡ì…˜ ë²„íŠ¼
            col1, col2, col3, col4, col5 = st.columns(5)
            
            with col1:
                if st.button("ğŸ“„ PDF", key=f"pdf_{paper.get('id', paper['title'][:20])}"):
                    self._open_paper_pdf(paper)
                    
            with col2:
                if st.button("ğŸ”¬ í”„ë¡œí† ì½œ ì¶”ì¶œ", key=f"extract_{paper.get('id', paper['title'][:20])}"):
                    self._extract_protocol_from_paper(paper)
                    
            with col3:
                if st.button("ğŸ’¾ ì €ì¥", key=f"save_{paper.get('id', paper['title'][:20])}"):
                    self._save_resource(paper, 'paper')
                    
            with col4:
                if st.button("ğŸ“ BibTeX", key=f"bib_{paper.get('id', paper['title'][:20])}"):
                    self._show_bibtex(paper)
                    
            with col5:
                if st.button("ğŸ”— ê´€ë ¨", key=f"related_{paper.get('id', paper['title'][:20])}"):
                    self._find_related_resources(paper)
                    
            st.divider()
            
    def _render_protocols_tab(self, protocols: List[Dict]):
        """í”„ë¡œí† ì½œ íƒ­ ë Œë”ë§"""
        if not protocols:
            show_info("ê²€ìƒ‰ëœ í”„ë¡œí† ì½œì´ ì—†ìŠµë‹ˆë‹¤.")
            return
            
        for protocol in protocols:
            self._render_protocol_card(protocol)
            
    def _render_protocol_card(self, protocol: Dict):
        """í”„ë¡œí† ì½œ ì¹´ë“œ ë Œë”ë§"""
        with st.container():
            # ì œëª©
            st.markdown(f"### ğŸ”¬ {protocol['title']}")
            
            # ë©”íƒ€ë°ì´í„°
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.write(f"ğŸ“ ì¶œì²˜: {protocol.get('source', 'N/A')}")
            with col2:
                st.write(f"â±ï¸ ì†Œìš”ì‹œê°„: {protocol.get('duration', 'N/A')}")
            with col3:
                st.write(f"ğŸ’° ë¹„ìš©: {protocol.get('cost', 'N/A')}")
            with col4:
                reproducibility = protocol.get('reproducibility', 0)
                st.write(f"âœ… ì¬í˜„ì„±: {reproducibility:.0%}")
                
            # ì¬ë£Œ ë° ì¥ë¹„
            col1, col2 = st.columns(2)
            
            with col1:
                if protocol.get('materials'):
                    st.write("**ì¬ë£Œ:**")
                    for material in protocol['materials'][:5]:
                        st.write(f"â€¢ {material}")
                    if len(protocol['materials']) > 5:
                        st.write(f"â€¢ ... ì™¸ {len(protocol['materials']) - 5}ê°œ")
                        
            with col2:
                if protocol.get('equipment'):
                    st.write("**ì¥ë¹„:**")
                    for equipment in protocol['equipment'][:5]:
                        st.write(f"â€¢ {equipment}")
                        
            # ì ˆì°¨ ìš”ì•½
            if protocol.get('procedure_summary'):
                with st.expander("ì ˆì°¨ ìš”ì•½"):
                    st.write(protocol['procedure_summary'])
                    
            # ì•¡ì…˜ ë²„íŠ¼
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                if st.button("ğŸ“‹ ìƒì„¸ë³´ê¸°", key=f"detail_{protocol['id']}"):
                    self._show_protocol_detail(protocol)
                    
            with col2:
                if st.button("ğŸ§ª ì‹¤í—˜ ì„¤ê³„", key=f"design_{protocol['id']}"):
                    st.session_state.selected_protocol = protocol
                    st.switch_page("pages/experiment_design.py")
                    
            with col3:
                if st.button("ğŸ’¾ ì €ì¥", key=f"save_prot_{protocol['id']}"):
                    self._save_resource(protocol, 'protocol')
                    
            with col4:
                if st.button("ğŸ¤ ê³µìœ ", key=f"share_{protocol['id']}"):
                    self._share_protocol(protocol)
                    
            st.divider()
            
    def _render_datasets_tab(self, datasets: List[Dict]):
        """ë°ì´í„°ì…‹ íƒ­ ë Œë”ë§"""
        if not datasets:
            show_info("ê²€ìƒ‰ëœ ë°ì´í„°ì…‹ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
            
        for dataset in datasets:
            self._render_dataset_card(dataset)
            
    def _render_dataset_card(self, dataset: Dict):
        """ë°ì´í„°ì…‹ ì¹´ë“œ ë Œë”ë§"""
        with st.container():
            # ì œëª©
            st.markdown(f"### ğŸ“Š {dataset['title']}")
            
            # ë©”íƒ€ë°ì´í„°
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.write(f"ğŸ“ ì¶œì²˜: {dataset.get('source', 'N/A')}")
            with col2:
                st.write(f"ğŸ“ í¬ê¸°: {self._format_file_size(dataset.get('size', 0))}")
            with col3:
                st.write(f"ğŸ“ í˜•ì‹: {', '.join(dataset.get('formats', []))}")
            with col4:
                st.write(f"ğŸ“… ì—…ë°ì´íŠ¸: {dataset.get('updated', 'N/A')}")
                
            # ì„¤ëª…
            if dataset.get('description'):
                st.write(dataset['description'][:200] + "..." if len(dataset['description']) > 200 else dataset['description'])
                
            # ë°ì´í„° êµ¬ì¡°
            if dataset.get('structure'):
                with st.expander("ë°ì´í„° êµ¬ì¡°"):
                    st.json(dataset['structure'])
                    
            # ì•¡ì…˜ ë²„íŠ¼
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                if st.button("â¬‡ï¸ ë‹¤ìš´ë¡œë“œ", key=f"download_{dataset['id']}"):
                    self._download_dataset(dataset)
                    
            with col2:
                if st.button("ğŸ“ˆ ë¶„ì„", key=f"analyze_{dataset['id']}"):
                    st.session_state.selected_dataset = dataset
                    st.switch_page("pages/data_analysis.py")
                    
            with col3:
                if st.button("ğŸ’¾ ì €ì¥", key=f"save_data_{dataset['id']}"):
                    self._save_resource(dataset, 'dataset')
                    
            with col4:
                if st.button("ğŸ”— ë©”íƒ€ë°ì´í„°", key=f"meta_{dataset['id']}"):
                    self._show_metadata(dataset)
                    
            st.divider()
            
    def _render_ai_insights(self, insights: Dict):
        """AI ì¸ì‚¬ì´íŠ¸ ë Œë”ë§"""
        with st.container():
            st.subheader("ğŸ¤– AI ë¶„ì„ ê²°ê³¼")
            
            # í•µì‹¬ ë°œê²¬ì‚¬í•­
            if insights.get('key_findings'):
                st.info(f"**í•µì‹¬ ë°œê²¬:** {insights['key_findings']}")
                
            # ì¶”ì²œ ë¦¬ì†ŒìŠ¤
            if insights.get('recommendations'):
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    if insights['recommendations'].get('best_paper'):
                        paper = insights['recommendations']['best_paper']
                        st.success(f"**ì¶”ì²œ ë…¼ë¬¸:** {paper['title']} (ê´€ë ¨ë„: {paper['score']:.0%})")
                        
                with col2:
                    if insights['recommendations'].get('best_protocol'):
                        protocol = insights['recommendations']['best_protocol']
                        st.success(f"**ì¶”ì²œ í”„ë¡œí† ì½œ:** {protocol['title']} (ì¬í˜„ì„±: {protocol['reproducibility']:.0%})")
                        
                with col3:
                    if insights['recommendations'].get('best_dataset'):
                        dataset = insights['recommendations']['best_dataset']
                        st.success(f"**ì¶”ì²œ ë°ì´í„°ì…‹:** {dataset['title']}")
                        
            # ì£¼ì˜ì‚¬í•­
            if insights.get('warnings'):
                for warning in insights['warnings']:
                    st.warning(f"âš ï¸ {warning}")
                    
            # ìƒì„¸ ë¶„ì„ í† ê¸€
            self._render_ai_response(insights, "literature_analysis")
            
    def _render_ai_response(self, response: Dict, response_type: str = "general"):
        """AI ì‘ë‹µ ë Œë”ë§ (ìƒì„¸ë„ ì œì–´ í¬í•¨)"""
        # ìƒì„¸ ì„¤ëª… í† ê¸€
        show_details = st.session_state.get('show_ai_details', False)
        
        col1, col2 = st.columns([4, 1])
        with col2:
            if st.button("ğŸ” ìƒì„¸", key=f"toggle_{response_type}"):
                st.session_state.show_ai_details = not show_details
                st.rerun()
                
        # ìƒì„¸ ì„¤ëª… (ì¡°ê±´ë¶€ í‘œì‹œ)
        if show_details:
            tabs = st.tabs(["ë¶„ì„ ê³¼ì •", "ì—°ê²° ê·¼ê±°", "ì‹ ë¢°ë„", "í•œê³„ì "])
            
            with tabs[0]:
                st.write("**ë¶„ì„ ê³¼ì •:**")
                st.write(response.get('analysis_process', 'ë¶„ì„ ê³¼ì • ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.'))
                
            with tabs[1]:
                st.write("**ë¦¬ì†ŒìŠ¤ ì—°ê²° ê·¼ê±°:**")
                connections = response.get('connection_reasoning', {})
                for conn_type, reasoning in connections.items():
                    st.write(f"â€¢ {conn_type}: {reasoning}")
                    
            with tabs[2]:
                st.write("**ì‹ ë¢°ë„ í‰ê°€:**")
                confidence = response.get('confidence', {})
                for aspect, score in confidence.items():
                    st.progress(score)
                    st.write(f"{aspect}: {score:.0%}")
                    
            with tabs[3]:
                st.write("**í•œê³„ì  ë° ì£¼ì˜ì‚¬í•­:**")
                limitations = response.get('limitations', [])
                for limitation in limitations:
                    st.write(f"â€¢ {limitation}")
                    
    # === ê²€ìƒ‰ ë©”ì„œë“œ ===
    
    def _search_google_scholar(self, query: str, filters: Dict) -> List[Dict]:
        """Google Scholar ê²€ìƒ‰"""
        results = []
        
        try:
            # scholarly ê²€ìƒ‰
            search_query = scholarly.search_pubs(query)
            
            for i, paper in enumerate(search_query):
                if i >= 20:  # ìµœëŒ€ 20ê°œ
                    break
                    
                # í•„í„° ì ìš©
                year = paper.get('bib', {}).get('pub_year', '')
                if year and filters.get('date_range'):
                    try:
                        if int(year) < filters['date_range'][0].year or int(year) > filters['date_range'][1].year:
                            continue
                    except:
                        pass
                        
                results.append({
                    'id': hashlib.md5(paper.get('bib', {}).get('title', '').encode()).hexdigest()[:10],
                    'title': paper.get('bib', {}).get('title', ''),
                    'authors': paper.get('bib', {}).get('author', '').split(' and '),
                    'year': year,
                    'venue': paper.get('bib', {}).get('venue', ''),
                    'abstract': paper.get('bib', {}).get('abstract', ''),
                    'citations': paper.get('num_citations', 0),
                    'url': paper.get('pub_url', ''),
                    'pdf_url': paper.get('eprint_url', ''),
                    'source': 'Google Scholar'
                })
                
        except Exception as e:
            st.error(f"Google Scholar ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
            
        return results
        
    def _search_arxiv(self, query: str, filters: Dict) -> List[Dict]:
        """arXiv ê²€ìƒ‰"""
        results = []
        
        try:
            # arXiv ê²€ìƒ‰
            search = arxiv.Search(
                query=query,
                max_results=20,
                sort_by=arxiv.SortCriterion.Relevance
            )
            
            for paper in search.results():
                # í•„í„° ì ìš©
                if filters.get('date_range'):
                    if paper.published.date() < filters['date_range'][0] or paper.published.date() > filters['date_range'][1]:
                        continue
                        
                results.append({
                    'id': paper.entry_id.split('/')[-1],
                    'title': paper.title,
                    'authors': [author.name for author in paper.authors],
                    'year': paper.published.year,
                    'abstract': paper.summary,
                    'categories': paper.categories,
                    'url': paper.entry_id,
                    'pdf_url': paper.pdf_url,
                    'source': 'arXiv'
                })
                
        except Exception as e:
            st.error(f"arXiv ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
            
        return results
        
    def _search_github(self, query: str, filters: Dict) -> List[Dict]:
        """GitHub ê²€ìƒ‰"""
        results = []
        
        if not self.github_client:
            return results
            
        try:
            # ì €ì¥ì†Œ ê²€ìƒ‰
            search_query = f"{query} polymer in:readme"
            repos = self.github_client.search_repositories(
                query=search_query,
                sort='stars',
                order='desc'
            )
            
            for repo in repos[:10]:
                # ê´€ë ¨ íŒŒì¼ ì°¾ê¸°
                data_files = []
                code_files = []
                
                try:
                    contents = repo.get_contents("")
                    while contents:
                        file_content = contents.pop(0)
                        if file_content.type == "dir":
                            contents.extend(repo.get_contents(file_content.path))
                        else:
                            # ë°ì´í„° íŒŒì¼
                            if any(file_content.name.endswith(ext) for ext in ['.csv', '.xlsx', '.json', '.hdf5']):
                                data_files.append({
                                    'name': file_content.name,
                                    'path': file_content.path,
                                    'size': file_content.size
                                })
                            # ì½”ë“œ íŒŒì¼
                            elif any(file_content.name.endswith(ext) for ext in ['.py', '.ipynb', '.m', '.R']):
                                code_files.append({
                                    'name': file_content.name,
                                    'path': file_content.path
                                })
                except:
                    pass
                    
                # ë°ì´í„°ì…‹ìœ¼ë¡œ ë¶„ë¥˜
                if data_files:
                    results.append({
                        'id': f"github_data_{repo.id}",
                        'type': 'dataset',
                        'title': f"{repo.name} - Dataset",
                        'description': repo.description or '',
                        'source': 'GitHub',
                        'url': repo.html_url,
                        'size': sum(f['size'] for f in data_files),
                        'formats': list(set(f['name'].split('.')[-1] for f in data_files)),
                        'files': data_files,
                        'stars': repo.stargazers_count,
                        'updated': repo.updated_at.strftime('%Y-%m-%d')
                    })
                    
                # ì½”ë“œë¡œ ë¶„ë¥˜
                if code_files:
                    results.append({
                        'id': f"github_code_{repo.id}",
                        'type': 'code',
                        'title': repo.name,
                        'description': repo.description or '',
                        'source': 'GitHub',
                        'url': repo.html_url,
                        'language': repo.language,
                        'files': code_files,
                        'stars': repo.stargazers_count,
                        'topics': repo.get_topics(),
                        'updated': repo.updated_at.strftime('%Y-%m-%d')
                    })
                    
        except Exception as e:
            st.error(f"GitHub ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
            
        return results
        
    def _extract_protocol_from_paper(self, paper: Dict):
        """ë…¼ë¬¸ì—ì„œ í”„ë¡œí† ì½œ ì¶”ì¶œ"""
        if not paper.get('pdf_url'):
            show_error("PDF URLì´ ì—†ìŠµë‹ˆë‹¤.")
            return
            
        with st.spinner("PDFì—ì„œ ì‹¤í—˜ í”„ë¡œí† ì½œì„ ì¶”ì¶œí•˜ëŠ” ì¤‘..."):
            try:
                # PDF ë‹¤ìš´ë¡œë“œ
                response = requests.get(paper['pdf_url'])
                pdf_file = BytesIO(response.content)
                
                # í…ìŠ¤íŠ¸ ì¶”ì¶œ
                text = self._extract_text_from_pdf(pdf_file)
                
                # Methods ì„¹ì…˜ ì°¾ê¸°
                methods_text = self._extract_methods_section(text)
                
                if not methods_text:
                    show_warning("Methods ì„¹ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    return
                    
                # AIë¡œ êµ¬ì¡°í™”
                protocol = self._structure_protocol_with_ai(methods_text, paper['title'])
                
                # í”„ë¡œí† ì½œ í‘œì‹œ
                self._show_extracted_protocol(protocol)
                
            except Exception as e:
                show_error(f"í”„ë¡œí† ì½œ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
                
    def _extract_text_from_pdf(self, pdf_file: BytesIO) -> str:
        """PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        text = ""
        
        if fitz:
            # PyMuPDF ì‚¬ìš©
            with fitz.open(stream=pdf_file, filetype="pdf") as pdf:
                for page in pdf:
                    text += page.get_text()
        elif PyPDF2:
            # PyPDF2 ì‚¬ìš©
            reader = PyPDF2.PdfReader(pdf_file)
            for page in reader.pages:
                text += page.extract_text()
                
        return text
        
    def _extract_methods_section(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ì—ì„œ Methods ì„¹ì…˜ ì¶”ì¶œ"""
        # ë‹¤ì–‘í•œ ì„¹ì…˜ ì´ë¦„ íŒ¨í„´
        patterns = [
            r'(?i)(experimental|methods|methodology|materials and methods|experimental section)(.*?)(?=references|acknowledgment|conclusion|results)',
            r'(?i)(2\.\s*experimental|3\.\s*experimental|2\.\s*methods|3\.\s*methods)(.*?)(?=\d\.\s*\w+|references)',
            r'(?i)(experimental procedures?|experimental details?)(.*?)(?=references|acknowledgment|conclusion)'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text, re.DOTALL)
            if match:
                return match.group(2).strip()
                
        # í‚¤ì›Œë“œ ê¸°ë°˜ ì¶”ì¶œ
        keywords = ['prepared', 'synthesized', 'mixed', 'dissolved', 'heated', 'cooled', 'stirred']
        paragraphs = text.split('\n\n')
        
        methods_paragraphs = []
        for para in paragraphs:
            if any(keyword in para.lower() for keyword in keywords):
                methods_paragraphs.append(para)
                
        return '\n\n'.join(methods_paragraphs[:10])
        
    def _structure_protocol_with_ai(self, methods_text: str, paper_title: str) -> Dict:
        """AIë¥¼ ì‚¬ìš©í•˜ì—¬ í”„ë¡œí† ì½œ êµ¬ì¡°í™”"""
        prompt = f"""
        ë‹¤ìŒ ë…¼ë¬¸ì˜ ì‹¤í—˜ ë°©ë²• í…ìŠ¤íŠ¸ì—ì„œ êµ¬ì¡°í™”ëœ í”„ë¡œí† ì½œì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”:
        
        ë…¼ë¬¸ ì œëª©: {paper_title}
        
        Methods í…ìŠ¤íŠ¸:
        {methods_text[:3000]}
        
        ë‹¤ìŒ ì •ë³´ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œí•˜ì„¸ìš”:
        1. materials: ì¬ë£Œ ëª©ë¡ (ì´ë¦„, ìˆœë„, ê³µê¸‰ì‚¬, ì‚¬ìš©ëŸ‰)
        2. equipment: ì¥ë¹„ ëª©ë¡ (ì¢…ë¥˜, ëª¨ë¸, ì„¤ì •ê°’)
        3. procedure: ì‹¤í—˜ ì ˆì°¨ (ë‹¨ê³„ë³„ë¡œ)
        4. conditions: ê³µì • ì¡°ê±´ (ì˜¨ë„, ì••ë ¥, ì‹œê°„ ë“±)
        5. characterization: ë¶„ì„ ë°©ë²•
        """
        
        try:
            response = self.api.generate_response(prompt, json_mode=True)
            protocol = json.loads(response)
            
            # ì¶”ê°€ ì •ë³´
            protocol['title'] = f"Protocol from: {paper_title}"
            protocol['source'] = 'Extracted from paper'
            protocol['paper_id'] = paper_title[:50]
            
            return protocol
            
        except Exception as e:
            st.error(f"AI í”„ë¡œí† ì½œ êµ¬ì¡°í™” ì‹¤íŒ¨: {str(e)}")
            return {
                'title': f"Protocol from: {paper_title}",
                'raw_text': methods_text,
                'error': str(e)
            }
            
    def _analyze_and_connect_results(self, results: Dict, query: str) -> Dict:
        """AIë¥¼ ì‚¬ìš©í•œ ê²°ê³¼ ë¶„ì„ ë° ì—°ê²°"""
        # ë¦¬ì†ŒìŠ¤ ê°„ ì—°ê²° ì°¾ê¸°
        resource_groups = self._find_resource_connections(results)
        
        # AI ì¸ì‚¬ì´íŠ¸ ìƒì„±
        insights = self._generate_integrated_insights(results, query)
        
        # ì¶”ì²œ ì„ ì •
        recommendations = self._select_best_resources(results, query)
        insights['recommendations'] = recommendations
        
        # ì£¼ì˜ì‚¬í•­ ì‹ë³„
        warnings = self._identify_warnings(results)
        insights['warnings'] = warnings
        
        results['resource_groups'] = resource_groups
        results['ai_insights'] = insights
        
        return results
        
    def _find_resource_connections(self, results: Dict) -> List[Dict]:
        """ë¦¬ì†ŒìŠ¤ ê°„ ì—°ê²° ê´€ê³„ ì°¾ê¸°"""
        connections = []
        
        papers = results.get('papers', [])
        protocols = results.get('protocols', [])
        datasets = results.get('datasets', [])
        
        # ë…¼ë¬¸-í”„ë¡œí† ì½œ-ë°ì´í„° ë§¤ì¹­
        for paper in papers:
            paper_authors = set(author.lower() for author in paper.get('authors', []))
            paper_keywords = self._extract_keywords(paper.get('title', '') + ' ' + paper.get('abstract', ''))
            
            matching_protocols = []
            matching_datasets = []
            
            # í”„ë¡œí† ì½œ ë§¤ì¹­
            for protocol in protocols:
                # ì €ì ë§¤ì¹­
                if protocol.get('authors'):
                    protocol_authors = set(author.lower() for author in protocol['authors'])
                    if paper_authors & protocol_authors:
                        matching_protocols.append(protocol)
                        continue
                        
                # í‚¤ì›Œë“œ ë§¤ì¹­
                protocol_keywords = self._extract_keywords(protocol.get('title', '') + ' ' + protocol.get('description', ''))
                if len(paper_keywords & protocol_keywords) >= 3:
                    matching_protocols.append(protocol)
                    
            # ë°ì´í„°ì…‹ ë§¤ì¹­
            for dataset in datasets:
                dataset_keywords = self._extract_keywords(dataset.get('title', '') + ' ' + dataset.get('description', ''))
                if len(paper_keywords & dataset_keywords) >= 3:
                    matching_datasets.append(dataset)
                    
            # ì—°ê²° ê·¸ë£¹ ìƒì„±
            if matching_protocols or matching_datasets:
                connections.append({
                    'title': paper['title'][:80] + "...",
                    'relevance_score': 0.8 if matching_protocols and matching_datasets else 0.6,
                    'description': f"ë…¼ë¬¸ + {len(matching_protocols)}ê°œ í”„ë¡œí† ì½œ + {len(matching_datasets)}ê°œ ë°ì´í„°ì…‹",
                    'paper': paper,
                    'protocol': matching_protocols[0] if matching_protocols else None,
                    'dataset': matching_datasets[0] if matching_datasets else None,
                    'explanation': self._generate_connection_explanation(paper, matching_protocols, matching_datasets)
                })
                
        return sorted(connections, key=lambda x: x['relevance_score'], reverse=True)[:10]
        
    def _generate_connection_explanation(self, paper: Dict, protocols: List[Dict], datasets: List[Dict]) -> str:
        """ì—°ê²° ê´€ê³„ ì„¤ëª… ìƒì„±"""
        explanation = f"ì´ ë…¼ë¬¸ '{paper['title']}'ì€"
        
        if protocols:
            explanation += f" {len(protocols)}ê°œì˜ ê´€ë ¨ í”„ë¡œí† ì½œê³¼ ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤."
            if protocols[0].get('reproducibility'):
                explanation += f" ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ í”„ë¡œí† ì½œì˜ ì¬í˜„ì„±ì€ {protocols[0]['reproducibility']:.0%}ì…ë‹ˆë‹¤."
                
        if datasets:
            explanation += f" {len(datasets)}ê°œì˜ ë°ì´í„°ì…‹ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤."
            total_size = sum(d.get('size', 0) for d in datasets)
            if total_size > 0:
                explanation += f" ì´ {self._format_file_size(total_size)}ì˜ ë°ì´í„°ë¥¼ ì œê³µí•©ë‹ˆë‹¤."
                
        return explanation
        
    def _extract_keywords(self, text: str) -> set:
        """í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ NLP í•„ìš”)
        words = re.findall(r'\b[a-z]+\b', text.lower())
        
        # ë¶ˆìš©ì–´ ì œê±°
        stopwords = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'from', 'as', 'is', 'was', 'are', 'been'}
        
        keywords = set()
        for word in words:
            if len(word) > 3 and word not in stopwords:
                keywords.add(word)
                
        return keywords
        
    def _format_file_size(self, size: int) -> str:
        """íŒŒì¼ í¬ê¸° í¬ë§·íŒ…"""
        for unit in ['B', 'KB', 'MB', 'GB']:
            if size < 1024.0:
                return f"{size:.1f} {unit}"
            size /= 1024.0
        return f"{size:.1f} TB"
        
    def _deduplicate_results(self, results: Dict) -> Dict:
        """ì¤‘ë³µ ê²°ê³¼ ì œê±°"""
        for key in ['papers', 'protocols', 'datasets', 'materials', 'code']:
            if key in results and isinstance(results[key], list):
                # ì œëª© ê¸°ë°˜ ì¤‘ë³µ ì œê±°
                seen_titles = set()
                unique_items = []
                
                for item in results[key]:
                    title = item.get('title', '').lower()
                    if title and title not in seen_titles:
                        seen_titles.add(title)
                        unique_items.append(item)
                        
                results[key] = unique_items
                
        return results
        
    def _save_search_history(self, query: str, results: Dict):
        """ê²€ìƒ‰ ê¸°ë¡ ì €ì¥"""
        history_entry = {
            'query': query,
            'timestamp': datetime.now(),
            'result_count': results['total_count'],
            'user_id': self.current_user['id']
        }
        
        st.session_state.search_history.append(history_entry)
        
        # ë°ì´í„°ë² ì´ìŠ¤ì—ë„ ì €ì¥
        try:
            self.sheets.create_data('SearchHistory', history_entry)
        except:
            pass


def render():
    """í˜ì´ì§€ ë Œë”ë§ í•¨ìˆ˜"""
    # ì¸ì¦ í™•ì¸
    if not check_authentication():
        st.error("ë¡œê·¸ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        st.stop()
        
    # ì—°êµ¬ ìì› ê´€ë¦¬ì ì´ˆê¸°í™” ë° ë Œë”ë§
    manager = IntegratedResearchManager()
    manager.render_page()


if __name__ == "__main__":
    render()
