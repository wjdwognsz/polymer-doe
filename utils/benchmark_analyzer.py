"""
ë²¤ì¹˜ë§ˆí¬ ë¶„ì„ê¸° - ì‹¤í—˜ ê²°ê³¼ì™€ ë¬¸í—Œ ë°ì´í„° ë¹„êµ ë¶„ì„
"""

import logging
import asyncio
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime, timedelta
import pandas as pd
import numpy as np
from dataclasses import dataclass, field
import json
from collections import defaultdict
import hashlib

# Plotly
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ
from config.app_config import get_config
from utils.error_handler import handle_errors
from utils.api_manager import get_api_manager
from utils.data_processor import DataProcessor

logger = logging.getLogger(__name__)

# ===========================================================================
# ğŸ¯ ë°ì´í„° ëª¨ë¸
# ===========================================================================

@dataclass
class BenchmarkMetrics:
    """ì„±ëŠ¥ í‰ê°€ ì§€í‘œ"""
    percentile_rank: float  # ë°±ë¶„ìœ„ ìˆœìœ„ (0-100)
    z_score: float  # í‘œì¤€í™” ì ìˆ˜
    relative_performance: Dict[str, float]  # í•­ëª©ë³„ ìƒëŒ€ ì„±ëŠ¥
    improvement_suggestions: List[str]  # ê°œì„  ì œì•ˆ
    similar_works: List[Dict[str, Any]]  # ìœ ì‚¬ ì—°êµ¬
    confidence_score: float = 0.0  # ì‹ ë¢°ë„ ì ìˆ˜
    sample_size: int = 0  # ë¹„êµ ë°ì´í„° ìˆ˜

@dataclass
class LiteratureData:
    """ë¬¸í—Œ ë°ì´í„°"""
    source: str  # ì¶œì²˜ (OpenAlex, Crossref ë“±)
    title: str
    authors: List[str]
    year: int
    doi: Optional[str] = None
    url: Optional[str] = None
    abstract: Optional[str] = None
    metrics: Dict[str, Any] = field(default_factory=dict)
    materials: List[str] = field(default_factory=list)
    conditions: Dict[str, Any] = field(default_factory=dict)
    relevance_score: float = 0.0

@dataclass
class ComparisonResult:
    """ë¹„êµ ë¶„ì„ ê²°ê³¼"""
    experiment_id: str
    timestamp: datetime
    metrics: BenchmarkMetrics
    literature_data: List[LiteratureData]
    visualizations: Dict[str, Any]
    report: Dict[str, Any]

# ===========================================================================
# ğŸ” ë²¤ì¹˜ë§ˆí¬ ë¶„ì„ê¸°
# ===========================================================================

class BenchmarkAnalyzer:
    """ì‹¤í—˜ ê²°ê³¼ì™€ ë¬¸í—Œ ë°ì´í„° ë¹„êµ ë¶„ì„"""
    
    def __init__(self):
        self.config = get_config()
        self.api_manager = get_api_manager()
        self.data_processor = DataProcessor()
        self.cache = {}
        self.cache_ttl = timedelta(hours=24)
        
    # ===== ë©”ì¸ ë¶„ì„ í•¨ìˆ˜ =====
    
    @handle_errors
    async def analyze_benchmark(
        self,
        experiment_data: Dict[str, Any],
        search_params: Optional[Dict[str, Any]] = None,
        analysis_type: str = 'comprehensive'
    ) -> ComparisonResult:
        """ì‹¤í—˜ ê²°ê³¼ì˜ ë²¤ì¹˜ë§ˆí¬ ë¶„ì„ ìˆ˜í–‰"""
        
        logger.info(f"Starting benchmark analysis for experiment: {experiment_data.get('name', 'Unknown')}")
        
        # 1. ê²€ìƒ‰ íŒŒë¼ë¯¸í„° ì¤€ë¹„
        if not search_params:
            search_params = self._prepare_search_params(experiment_data)
            
        # 2. ë¬¸í—Œ ë°ì´í„° ìˆ˜ì§‘ (ë¹„ë™ê¸°)
        literature_data = await self._collect_literature_data(search_params)
        
        # 3. ë°ì´í„° ë§¤ì¹­ ë° í•„í„°ë§
        matched_data = self._match_and_filter(experiment_data, literature_data)
        
        # 4. ë©”íŠ¸ë¦­ ê³„ì‚°
        metrics = self._calculate_metrics(experiment_data, matched_data)
        
        # 5. ì‹œê°í™” ìƒì„±
        visualizations = self._create_visualizations(experiment_data, matched_data, metrics)
        
        # 6. ë¦¬í¬íŠ¸ ìƒì„±
        report = self._generate_report(experiment_data, matched_data, metrics)
        
        # ê²°ê³¼ ë°˜í™˜
        result = ComparisonResult(
            experiment_id=experiment_data.get('id', 'unknown'),
            timestamp=datetime.now(),
            metrics=metrics,
            literature_data=matched_data,
            visualizations=visualizations,
            report=report
        )
        
        logger.info(f"Benchmark analysis completed. Found {len(matched_data)} comparable studies.")
        
        return result
        
    # ===== ë°ì´í„° ìˆ˜ì§‘ =====
    
    async def _collect_literature_data(
        self, 
        search_params: Dict[str, Any]
    ) -> List[LiteratureData]:
        """ì—¬ëŸ¬ ì†ŒìŠ¤ì—ì„œ ë¬¸í—Œ ë°ì´í„° ìˆ˜ì§‘"""
        
        # ìºì‹œ í™•ì¸
        cache_key = self._get_cache_key(search_params)
        if cache_key in self.cache:
            cached_data, timestamp = self.cache[cache_key]
            if datetime.now() - timestamp < self.cache_ttl:
                logger.info("Using cached literature data")
                return cached_data
                
        # ë¹„ë™ê¸° ì‘ì—… ëª©ë¡
        tasks = []
        
        # OpenAlex ê²€ìƒ‰
        if self.config.get('apis', {}).get('openalex', {}).get('enabled', True):
            tasks.append(self._search_openalex(search_params))
            
        # Crossref ê²€ìƒ‰
        if self.config.get('apis', {}).get('crossref', {}).get('enabled', True):
            tasks.append(self._search_crossref(search_params))
            
        # PubMed ê²€ìƒ‰
        if self.config.get('apis', {}).get('pubmed', {}).get('enabled', True):
            tasks.append(self._search_pubmed(search_params))
            
        # Materials Project ê²€ìƒ‰
        if self.config.get('apis', {}).get('materials_project', {}).get('enabled', True):
            tasks.append(self._search_materials_project(search_params))
            
        # ë³‘ë ¬ ì‹¤í–‰
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ê²°ê³¼ í†µí•©
        all_literature = []
        for result in results:
            if isinstance(result, Exception):
                logger.error(f"Error in data collection: {result}")
            else:
                all_literature.extend(result)
                
        # ì¤‘ë³µ ì œê±°
        unique_literature = self._deduplicate_literature(all_literature)
        
        # ìºì‹œ ì €ì¥
        self.cache[cache_key] = (unique_literature, datetime.now())
        
        return unique_literature
        
    async def _search_openalex(self, params: Dict[str, Any]) -> List[LiteratureData]:
        """OpenAlex API ê²€ìƒ‰"""
        try:
            query = params.get('keywords', [])
            if isinstance(query, list):
                query = ' '.join(query)
                
            # API í˜¸ì¶œ
            response = await self.api_manager.search_literature(
                query=query,
                source='openalex',
                filters=params.get('filters', {}),
                limit=params.get('limit', 50)
            )
            
            # ê²°ê³¼ íŒŒì‹±
            literature = []
            for work in response.get('results', []):
                lit_data = LiteratureData(
                    source='OpenAlex',
                    title=work.get('title', ''),
                    authors=[author.get('author', {}).get('display_name', '') 
                            for author in work.get('authorships', [])],
                    year=work.get('publication_year', 0),
                    doi=work.get('doi', ''),
                    abstract=work.get('abstract', ''),
                    metrics=self._extract_metrics_from_text(
                        work.get('title', '') + ' ' + work.get('abstract', '')
                    )
                )
                literature.append(lit_data)
                
            return literature
            
        except Exception as e:
            logger.error(f"OpenAlex search error: {e}")
            return []
            
    async def _search_crossref(self, params: Dict[str, Any]) -> List[LiteratureData]:
        """Crossref API ê²€ìƒ‰"""
        try:
            query = params.get('keywords', [])
            if isinstance(query, list):
                query = ' '.join(query)
                
            # API í˜¸ì¶œ
            response = await self.api_manager.search_literature(
                query=query,
                source='crossref',
                filters=params.get('filters', {}),
                limit=params.get('limit', 50)
            )
            
            # ê²°ê³¼ íŒŒì‹±
            literature = []
            for item in response.get('message', {}).get('items', []):
                lit_data = LiteratureData(
                    source='Crossref',
                    title=item.get('title', [''])[0],
                    authors=[f"{author.get('given', '')} {author.get('family', '')}"
                            for author in item.get('author', [])],
                    year=item.get('published-print', {}).get('date-parts', [[0]])[0][0],
                    doi=item.get('DOI', ''),
                    url=item.get('URL', '')
                )
                literature.append(lit_data)
                
            return literature
            
        except Exception as e:
            logger.error(f"Crossref search error: {e}")
            return []
            
    async def _search_pubmed(self, params: Dict[str, Any]) -> List[LiteratureData]:
        """PubMed API ê²€ìƒ‰"""
        try:
            query = params.get('keywords', [])
            if isinstance(query, list):
                query = ' '.join(query)
                
            # API í˜¸ì¶œ
            response = await self.api_manager.search_literature(
                query=query,
                source='pubmed',
                filters=params.get('filters', {}),
                limit=params.get('limit', 50)
            )
            
            # ê²°ê³¼ íŒŒì‹±
            literature = []
            for article in response.get('articles', []):
                lit_data = LiteratureData(
                    source='PubMed',
                    title=article.get('title', ''),
                    authors=article.get('authors', []),
                    year=article.get('year', 0),
                    doi=article.get('doi', ''),
                    abstract=article.get('abstract', ''),
                    metrics=self._extract_metrics_from_text(article.get('abstract', ''))
                )
                literature.append(lit_data)
                
            return literature
            
        except Exception as e:
            logger.error(f"PubMed search error: {e}")
            return []
            
    async def _search_materials_project(self, params: Dict[str, Any]) -> List[LiteratureData]:
        """Materials Project ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰"""
        try:
            # ì¬ë£Œ ê´€ë ¨ ê²€ìƒ‰
            materials = params.get('materials', [])
            if not materials:
                return []
                
            # API í˜¸ì¶œ
            response = await self.api_manager.search_materials_data(
                materials=materials,
                properties=params.get('properties', [])
            )
            
            # ê²°ê³¼ íŒŒì‹±
            literature = []
            for material in response.get('data', []):
                lit_data = LiteratureData(
                    source='Materials Project',
                    title=f"{material.get('formula', '')} - {material.get('material_id', '')}",
                    authors=['Materials Project'],
                    year=datetime.now().year,
                    url=f"https://materialsproject.org/materials/{material.get('material_id', '')}",
                    metrics=material.get('properties', {}),
                    materials=[material.get('formula', '')]
                )
                literature.append(lit_data)
                
            return literature
            
        except Exception as e:
            logger.error(f"Materials Project search error: {e}")
            return []
            
    # ===== ë°ì´í„° ë§¤ì¹­ =====
    
    def _match_and_filter(
        self,
        experiment_data: Dict[str, Any],
        literature_data: List[LiteratureData]
    ) -> List[LiteratureData]:
        """ì‹¤í—˜ê³¼ ê´€ë ¨ëœ ë¬¸í—Œ ë§¤ì¹­ ë° í•„í„°ë§"""
        
        matched = []
        
        for lit in literature_data:
            # ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
            relevance_score = self._calculate_relevance(experiment_data, lit)
            lit.relevance_score = relevance_score
            
            # ì„ê³„ê°’ ì´ìƒë§Œ í¬í•¨
            if relevance_score >= 0.5:
                matched.append(lit)
                
        # ê´€ë ¨ì„± ìˆœìœ¼ë¡œ ì •ë ¬
        matched.sort(key=lambda x: x.relevance_score, reverse=True)
        
        # ìƒìœ„ Nê°œë§Œ ë°˜í™˜
        max_results = self.config.get('benchmark', {}).get('max_comparison_items', 100)
        return matched[:max_results]
        
    def _calculate_relevance(
        self,
        experiment: Dict[str, Any],
        literature: LiteratureData
    ) -> float:
        """ì‹¤í—˜ê³¼ ë¬¸í—Œì˜ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°"""
        
        score = 0.0
        weights = {
            'materials': 0.4,
            'conditions': 0.3,
            'keywords': 0.2,
            'year': 0.1
        }
        
        # ì¬ë£Œ ìœ ì‚¬ë„
        exp_materials = set(experiment.get('materials', []))
        lit_materials = set(literature.materials)
        if exp_materials and lit_materials:
            material_similarity = len(exp_materials & lit_materials) / len(exp_materials | lit_materials)
            score += material_similarity * weights['materials']
            
        # ì¡°ê±´ ìœ ì‚¬ë„
        exp_conditions = experiment.get('conditions', {})
        lit_conditions = literature.conditions
        if exp_conditions and lit_conditions:
            condition_similarity = self._calculate_condition_similarity(exp_conditions, lit_conditions)
            score += condition_similarity * weights['conditions']
            
        # í‚¤ì›Œë“œ ìœ ì‚¬ë„
        exp_keywords = set(experiment.get('keywords', []))
        lit_keywords = set(self._extract_keywords(literature.title + ' ' + (literature.abstract or '')))
        if exp_keywords and lit_keywords:
            keyword_similarity = len(exp_keywords & lit_keywords) / len(exp_keywords | lit_keywords)
            score += keyword_similarity * weights['keywords']
            
        # ì—°ë„ ê°€ì¤‘ì¹˜ (ìµœì‹  ë…¼ë¬¸ ì„ í˜¸)
        if literature.year:
            year_diff = datetime.now().year - literature.year
            year_score = max(0, 1 - (year_diff / 10))  # 10ë…„ ì´ë‚´ ì„ í˜¸
            score += year_score * weights['year']
            
        return score
        
    def _calculate_condition_similarity(
        self,
        cond1: Dict[str, Any],
        cond2: Dict[str, Any]
    ) -> float:
        """ì‹¤í—˜ ì¡°ê±´ ìœ ì‚¬ë„ ê³„ì‚°"""
        
        if not cond1 or not cond2:
            return 0.0
            
        common_params = set(cond1.keys()) & set(cond2.keys())
        if not common_params:
            return 0.0
            
        similarities = []
        for param in common_params:
            val1 = cond1[param]
            val2 = cond2[param]
            
            if isinstance(val1, (int, float)) and isinstance(val2, (int, float)):
                # ìˆ˜ì¹˜ ë¹„êµ
                if val1 == 0 and val2 == 0:
                    sim = 1.0
                else:
                    diff = abs(val1 - val2)
                    avg = (abs(val1) + abs(val2)) / 2
                    sim = max(0, 1 - (diff / avg))
            else:
                # ë²”ì£¼í˜• ë¹„êµ
                sim = 1.0 if val1 == val2 else 0.0
                
            similarities.append(sim)
            
        return sum(similarities) / len(similarities)
        
    # ===== ë©”íŠ¸ë¦­ ê³„ì‚° =====
    
    def _calculate_metrics(
        self,
        experiment_data: Dict[str, Any],
        matched_literature: List[LiteratureData]
    ) -> BenchmarkMetrics:
        """ë²¤ì¹˜ë§ˆí¬ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        
        if not matched_literature:
            return BenchmarkMetrics(
                percentile_rank=50.0,
                z_score=0.0,
                relative_performance={},
                improvement_suggestions=['ë¬¸í—Œ ë°ì´í„°ê°€ ë¶€ì¡±í•˜ì—¬ ë¹„êµê°€ ì œí•œì ì…ë‹ˆë‹¤.'],
                similar_works=[],
                confidence_score=0.0,
                sample_size=0
            )
            
        # ì£¼ìš” ë©”íŠ¸ë¦­ ì¶”ì¶œ
        metric_values = defaultdict(list)
        for lit in matched_literature:
            for metric, value in lit.metrics.items():
                if isinstance(value, (int, float)):
                    metric_values[metric].append(value)
                    
        # ê° ë©”íŠ¸ë¦­ë³„ ë¶„ì„
        relative_performance = {}
        percentiles = []
        z_scores = []
        
        for metric, values in metric_values.items():
            if metric in experiment_data.get('results', {}):
                exp_value = experiment_data['results'][metric]
                if isinstance(exp_value, (int, float)) and len(values) > 1:
                    # ë°±ë¶„ìœ„ ê³„ì‚°
                    percentile = self._calculate_percentile(exp_value, values)
                    percentiles.append(percentile)
                    
                    # Z-score ê³„ì‚°
                    z_score = self._calculate_z_score(exp_value, values)
                    z_scores.append(z_score)
                    
                    # ìƒëŒ€ ì„±ëŠ¥
                    relative_performance[metric] = {
                        'value': exp_value,
                        'mean': np.mean(values),
                        'std': np.std(values),
                        'percentile': percentile,
                        'z_score': z_score,
                        'sample_size': len(values)
                    }
                    
        # ì „ì²´ ë©”íŠ¸ë¦­
        overall_percentile = np.mean(percentiles) if percentiles else 50.0
        overall_z_score = np.mean(z_scores) if z_scores else 0.0
        
        # ìœ ì‚¬ ì—°êµ¬ ì„ ì •
        similar_works = self._find_similar_works(experiment_data, matched_literature)
        
        # ê°œì„  ì œì•ˆ ìƒì„±
        suggestions = self._generate_improvement_suggestions(
            experiment_data,
            relative_performance,
            similar_works
        )
        
        # ì‹ ë¢°ë„ ê³„ì‚°
        confidence = self._calculate_confidence_score(matched_literature, relative_performance)
        
        return BenchmarkMetrics(
            percentile_rank=overall_percentile,
            z_score=overall_z_score,
            relative_performance=relative_performance,
            improvement_suggestions=suggestions,
            similar_works=similar_works,
            confidence_score=confidence,
            sample_size=len(matched_literature)
        )
        
    def _calculate_percentile(self, value: float, values: List[float]) -> float:
        """ë°±ë¶„ìœ„ ê³„ì‚°"""
        return (sum(1 for v in values if v <= value) / len(values)) * 100
        
    def _calculate_z_score(self, value: float, values: List[float]) -> float:
        """Z-score ê³„ì‚°"""
        mean = np.mean(values)
        std = np.std(values)
        return (value - mean) / std if std > 0 else 0.0
        
    def _find_similar_works(
        self,
        experiment: Dict[str, Any],
        literature: List[LiteratureData]
    ) -> List[Dict[str, Any]]:
        """ê°€ì¥ ìœ ì‚¬í•œ ì—°êµ¬ ì°¾ê¸°"""
        
        # ê´€ë ¨ì„± ë†’ì€ ìƒìœ„ 5ê°œ
        top_similar = sorted(literature, key=lambda x: x.relevance_score, reverse=True)[:5]
        
        similar_works = []
        for lit in top_similar:
            work = {
                'title': lit.title,
                'authors': lit.authors[:3],  # ì²« 3ëª…ë§Œ
                'year': lit.year,
                'doi': lit.doi,
                'relevance_score': lit.relevance_score,
                'key_metrics': lit.metrics,
                'differences': self._identify_differences(experiment, lit)
            }
            similar_works.append(work)
            
        return similar_works
        
    def _identify_differences(
        self,
        experiment: Dict[str, Any],
        literature: LiteratureData
    ) -> Dict[str, Any]:
        """ì£¼ìš” ì°¨ì´ì  ì‹ë³„"""
        
        differences = {
            'materials': [],
            'conditions': [],
            'performance': []
        }
        
        # ì¬ë£Œ ì°¨ì´
        exp_materials = set(experiment.get('materials', []))
        lit_materials = set(literature.materials)
        differences['materials'] = {
            'unique_to_experiment': list(exp_materials - lit_materials),
            'unique_to_literature': list(lit_materials - exp_materials)
        }
        
        # ì¡°ê±´ ì°¨ì´
        exp_conditions = experiment.get('conditions', {})
        lit_conditions = literature.conditions
        for param in set(exp_conditions.keys()) | set(lit_conditions.keys()):
            if param in exp_conditions and param in lit_conditions:
                exp_val = exp_conditions[param]
                lit_val = lit_conditions[param]
                if exp_val != lit_val:
                    differences['conditions'].append({
                        'parameter': param,
                        'experiment': exp_val,
                        'literature': lit_val
                    })
                    
        return differences
        
    # ===== ê°œì„  ì œì•ˆ =====
    
    def _generate_improvement_suggestions(
        self,
        experiment: Dict[str, Any],
        performance: Dict[str, Any],
        similar_works: List[Dict[str, Any]]
    ) -> List[str]:
        """ê°œì„  ì œì•ˆ ìƒì„±"""
        
        suggestions = []
        
        # ì„±ëŠ¥ ê¸°ë°˜ ì œì•ˆ
        for metric, perf in performance.items():
            if perf['percentile'] < 50:
                suggestions.append(
                    f"{metric}ì´(ê°€) í‰ê· ë³´ë‹¤ ë‚®ìŠµë‹ˆë‹¤ (í•˜ìœ„ {perf['percentile']:.0f}%). "
                    f"ìƒìœ„ ì—°êµ¬ë“¤ì˜ í‰ê· ê°’ì€ {perf['mean']:.2f}ì…ë‹ˆë‹¤."
                )
                
        # ìœ ì‚¬ ì—°êµ¬ ê¸°ë°˜ ì œì•ˆ
        if similar_works:
            top_work = similar_works[0]
            if top_work['differences']['materials']['unique_to_literature']:
                materials = top_work['differences']['materials']['unique_to_literature'][:3]
                suggestions.append(
                    f"ìƒìœ„ ì—°êµ¬ì—ì„œ ì‚¬ìš©ëœ ì¶”ê°€ ì¬ë£Œë¥¼ ê³ ë ¤í•´ë³´ì„¸ìš”: {', '.join(materials)}"
                )
                
        # ì¡°ê±´ ìµœì í™” ì œì•ˆ
        condition_suggestions = self._suggest_condition_optimization(experiment, performance, similar_works)
        suggestions.extend(condition_suggestions)
        
        # AI ê¸°ë°˜ ì œì•ˆ (ì„ íƒì )
        if self.config.get('benchmark', {}).get('use_ai_suggestions', True):
            ai_suggestions = self._get_ai_suggestions(experiment, performance, similar_works)
            suggestions.extend(ai_suggestions[:2])  # ìµœëŒ€ 2ê°œ
            
        return suggestions[:5]  # ìµœëŒ€ 5ê°œ ì œì•ˆ
        
    def _suggest_condition_optimization(
        self,
        experiment: Dict[str, Any],
        performance: Dict[str, Any],
        similar_works: List[Dict[str, Any]]
    ) -> List[str]:
        """ì¡°ê±´ ìµœì í™” ì œì•ˆ"""
        
        suggestions = []
        
        # ìƒìœ„ ì—°êµ¬ë“¤ì˜ ì¡°ê±´ ë¶„ì„
        if similar_works:
            condition_stats = defaultdict(list)
            
            for work in similar_works[:3]:  # ìƒìœ„ 3ê°œ
                for diff in work['differences']['conditions']:
                    param = diff['parameter']
                    lit_val = diff['literature']
                    if isinstance(lit_val, (int, float)):
                        condition_stats[param].append(lit_val)
                        
            # í‰ê· ê°’ê³¼ ë¹„êµ
            exp_conditions = experiment.get('conditions', {})
            for param, values in condition_stats.items():
                if param in exp_conditions:
                    exp_val = exp_conditions[param]
                    avg_val = np.mean(values)
                    if isinstance(exp_val, (int, float)):
                        diff_percent = abs(exp_val - avg_val) / avg_val * 100
                        if diff_percent > 20:  # 20% ì´ìƒ ì°¨ì´
                            suggestions.append(
                                f"{param}ì„(ë¥¼) {avg_val:.1f}(ìœ¼)ë¡œ ì¡°ì •í•´ë³´ì„¸ìš” "
                                f"(í˜„ì¬: {exp_val:.1f}, ì°¨ì´: {diff_percent:.0f}%)"
                            )
                            
        return suggestions
        
    def _get_ai_suggestions(
        self,
        experiment: Dict[str, Any],
        performance: Dict[str, Any],
        similar_works: List[Dict[str, Any]]
    ) -> List[str]:
        """AI ê¸°ë°˜ ê°œì„  ì œì•ˆ"""
        
        try:
            # AI í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = f"""
            ì‹¤í—˜ ê²°ê³¼: {json.dumps(experiment.get('results', {}), indent=2)}
            ì„±ëŠ¥ ë¶„ì„: {json.dumps(performance, indent=2)}
            ìœ ì‚¬ ì—°êµ¬: {json.dumps(similar_works[:2], indent=2)}
            
            ì´ ì‹¤í—˜ì„ ê°œì„ í•˜ê¸° ìœ„í•œ êµ¬ì²´ì ì¸ ì œì•ˆ 2ê°œë¥¼ ì œì‹œí•´ì£¼ì„¸ìš”.
            ê° ì œì•ˆì€ í•œ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.
            """
            
            # API í˜¸ì¶œ (ë™ê¸°)
            response = self.api_manager.generate_text('gemini', prompt)
            
            # ì œì•ˆ ì¶”ì¶œ
            suggestions = []
            if response:
                lines = response.strip().split('\n')
                for line in lines:
                    if line.strip() and not line.startswith('#'):
                        suggestions.append(line.strip())
                        
            return suggestions[:2]
            
        except Exception as e:
            logger.error(f"AI suggestion error: {e}")
            return []
            
    # ===== ì‹œê°í™” =====
    
    def _create_visualizations(
        self,
        experiment: Dict[str, Any],
        literature: List[LiteratureData],
        metrics: BenchmarkMetrics
    ) -> Dict[str, Any]:
        """ë²¤ì¹˜ë§ˆí¬ ì‹œê°í™” ìƒì„±"""
        
        visualizations = {}
        
        # 1. ë°±ë¶„ìœ„ ê²Œì´ì§€ ì°¨íŠ¸
        visualizations['percentile_gauge'] = self._create_percentile_gauge(metrics.percentile_rank)
        
        # 2. ì„±ëŠ¥ ë¹„êµ ë ˆì´ë” ì°¨íŠ¸
        visualizations['performance_radar'] = self._create_performance_radar(
            experiment,
            metrics.relative_performance
        )
        
        # 3. íˆìŠ¤í† ê·¸ë¨ (ë¶„í¬ ë¹„êµ)
        visualizations['distribution_histogram'] = self._create_distribution_histogram(
            experiment,
            literature,
            metrics.relative_performance
        )
        
        # 4. ì‹œê³„ì—´ íŠ¸ë Œë“œ
        visualizations['timeline_trend'] = self._create_timeline_trend(literature)
        
        # 5. íˆíŠ¸ë§µ (ìƒê´€ê´€ê³„)
        visualizations['correlation_heatmap'] = self._create_correlation_heatmap(
            experiment,
            literature
        )
        
        return visualizations
        
    def _create_percentile_gauge(self, percentile: float) -> Dict[str, Any]:
        """ë°±ë¶„ìœ„ ê²Œì´ì§€ ì°¨íŠ¸"""
        
        fig = go.Figure(go.Indicator(
            mode="gauge+number+delta",
            value=percentile,
            domain={'x': [0, 1], 'y': [0, 1]},
            title={'text': "ì „ì²´ ì„±ëŠ¥ ë°±ë¶„ìœ„"},
            delta={'reference': 50, 'increasing': {'color': "green"}},
            gauge={
                'axis': {'range': [None, 100], 'tickwidth': 1, 'tickcolor': "darkblue"},
                'bar': {'color': "darkblue"},
                'bgcolor': "white",
                'borderwidth': 2,
                'bordercolor': "gray",
                'steps': [
                    {'range': [0, 25], 'color': '#ff4444'},
                    {'range': [25, 50], 'color': '#ffaa00'},
                    {'range': [50, 75], 'color': '#44ff44'},
                    {'range': [75, 100], 'color': '#00aa00'}
                ],
                'threshold': {
                    'line': {'color': "red", 'width': 4},
                    'thickness': 0.75,
                    'value': 90
                }
            }
        ))
        
        fig.update_layout(
            paper_bgcolor="white",
            font={'color': "darkblue", 'family': "Arial"},
            height=300
        )
        
        return fig.to_dict()
        
    def _create_performance_radar(
        self,
        experiment: Dict[str, Any],
        performance: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë ˆì´ë” ì°¨íŠ¸"""
        
        if not performance:
            return {}
            
        # ë©”íŠ¸ë¦­ ì¶”ì¶œ
        metrics = list(performance.keys())[:8]  # ìµœëŒ€ 8ê°œ
        exp_values = []
        lit_values = []
        
        for metric in metrics:
            perf = performance[metric]
            exp_values.append(perf['value'])
            lit_values.append(perf['mean'])
            
        # ì •ê·œí™” (0-100)
        max_values = [max(exp_values[i], lit_values[i]) for i in range(len(metrics))]
        exp_normalized = [(v / m * 100) if m > 0 else 0 for v, m in zip(exp_values, max_values)]
        lit_normalized = [(v / m * 100) if m > 0 else 0 for v, m in zip(lit_values, max_values)]
        
        fig = go.Figure()
        
        # ì‹¤í—˜ ë°ì´í„°
        fig.add_trace(go.Scatterpolar(
            r=exp_normalized,
            theta=metrics,
            fill='toself',
            name='ë‚´ ì‹¤í—˜',
            line_color='blue'
        ))
        
        # ë¬¸í—Œ í‰ê· 
        fig.add_trace(go.Scatterpolar(
            r=lit_normalized,
            theta=metrics,
            fill='toself',
            name='ë¬¸í—Œ í‰ê· ',
            line_color='red',
            opacity=0.6
        ))
        
        fig.update_layout(
            polar=dict(
                radialaxis=dict(
                    visible=True,
                    range=[0, 100]
                )
            ),
            showlegend=True,
            title="ì„±ëŠ¥ ë¹„êµ (ì •ê·œí™”)",
            height=400
        )
        
        return fig.to_dict()
        
    def _create_distribution_histogram(
        self,
        experiment: Dict[str, Any],
        literature: List[LiteratureData],
        performance: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ë¶„í¬ íˆìŠ¤í† ê·¸ë¨"""
        
        if not performance:
            return {}
            
        # ê°€ì¥ ì¤‘ìš”í•œ ë©”íŠ¸ë¦­ ì„ íƒ
        metric_name = list(performance.keys())[0]
        metric_data = performance[metric_name]
        
        # ë¬¸í—Œ ê°’ ìˆ˜ì§‘
        lit_values = []
        for lit in literature:
            if metric_name in lit.metrics:
                value = lit.metrics[metric_name]
                if isinstance(value, (int, float)):
                    lit_values.append(value)
                    
        if not lit_values:
            return {}
            
        fig = go.Figure()
        
        # íˆìŠ¤í† ê·¸ë¨
        fig.add_trace(go.Histogram(
            x=lit_values,
            name='ë¬¸í—Œ ë¶„í¬',
            nbinsx=20,
            opacity=0.7
        ))
        
        # ë‚´ ì‹¤í—˜ ê°’ í‘œì‹œ
        exp_value = metric_data['value']
        fig.add_vline(
            x=exp_value,
            line_dash="dash",
            line_color="red",
            annotation_text=f"ë‚´ ì‹¤í—˜: {exp_value:.2f}"
        )
        
        # í‰ê·  í‘œì‹œ
        mean_value = np.mean(lit_values)
        fig.add_vline(
            x=mean_value,
            line_dash="dot",
            line_color="green",
            annotation_text=f"í‰ê· : {mean_value:.2f}"
        )
        
        fig.update_layout(
            title=f"{metric_name} ë¶„í¬",
            xaxis_title=metric_name,
            yaxis_title="ë¹ˆë„",
            showlegend=True,
            height=300
        )
        
        return fig.to_dict()
        
    def _create_timeline_trend(self, literature: List[LiteratureData]) -> Dict[str, Any]:
        """ì‹œê³„ì—´ íŠ¸ë Œë“œ"""
        
        # ì—°ë„ë³„ ë°ì´í„° ìˆ˜ì§‘
        year_data = defaultdict(list)
        
        for lit in literature:
            if lit.year and lit.metrics:
                for metric, value in lit.metrics.items():
                    if isinstance(value, (int, float)):
                        year_data[lit.year].append(value)
                        
        if not year_data:
            return {}
            
        # ì—°ë„ë³„ í‰ê·  ê³„ì‚°
        years = sorted(year_data.keys())
        averages = [np.mean(year_data[year]) for year in years]
        
        fig = go.Figure()
        
        fig.add_trace(go.Scatter(
            x=years,
            y=averages,
            mode='lines+markers',
            name='ì—°ë„ë³„ í‰ê·  ì„±ëŠ¥',
            line=dict(color='blue', width=2),
            marker=dict(size=8)
        ))
        
        # ì¶”ì„¸ì„ 
        if len(years) > 2:
            z = np.polyfit(years, averages, 1)
            p = np.poly1d(z)
            fig.add_trace(go.Scatter(
                x=years,
                y=p(years),
                mode='lines',
                name='ì¶”ì„¸ì„ ',
                line=dict(color='red', dash='dash')
            ))
            
        fig.update_layout(
            title="ì—°êµ¬ ë™í–¥",
            xaxis_title="ì—°ë„",
            yaxis_title="í‰ê·  ì„±ëŠ¥",
            showlegend=True,
            height=300
        )
        
        return fig.to_dict()
        
    def _create_correlation_heatmap(
        self,
        experiment: Dict[str, Any],
        literature: List[LiteratureData]
    ) -> Dict[str, Any]:
        """ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ"""
        
        # ë°ì´í„° ìˆ˜ì§‘
        data_matrix = []
        metric_names = set()
        
        for lit in literature[:20]:  # ìµœëŒ€ 20ê°œ
            if lit.metrics:
                row = {}
                for metric, value in lit.metrics.items():
                    if isinstance(value, (int, float)):
                        row[metric] = value
                        metric_names.add(metric)
                if row:
                    data_matrix.append(row)
                    
        if len(data_matrix) < 3 or len(metric_names) < 2:
            return {}
            
        # DataFrame ìƒì„±
        df = pd.DataFrame(data_matrix)
        df = df[list(metric_names)]
        df = df.dropna(axis=1, how='all')
        
        # ìƒê´€ê´€ê³„ ê³„ì‚°
        corr = df.corr()
        
        fig = go.Figure(data=go.Heatmap(
            z=corr.values,
            x=corr.columns,
            y=corr.columns,
            colorscale='RdBu',
            zmid=0,
            text=np.round(corr.values, 2),
            texttemplate='%{text}',
            textfont={"size": 10}
        ))
        
        fig.update_layout(
            title="ë©”íŠ¸ë¦­ ê°„ ìƒê´€ê´€ê³„",
            height=400,
            width=400
        )
        
        return fig.to_dict()
        
    # ===== ë¦¬í¬íŠ¸ ìƒì„± =====
    
    def _generate_report(
        self,
        experiment: Dict[str, Any],
        literature: List[LiteratureData],
        metrics: BenchmarkMetrics
    ) -> Dict[str, Any]:
        """ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±"""
        
        report = {
            'summary': self._generate_summary(experiment, metrics),
            'detailed_analysis': self._generate_detailed_analysis(experiment, literature, metrics),
            'recommendations': metrics.improvement_suggestions,
            'similar_works': metrics.similar_works,
            'methodology': self._generate_methodology_section(),
            'limitations': self._identify_limitations(literature, metrics),
            'next_steps': self._suggest_next_steps(experiment, metrics)
        }
        
        return report
        
    def _generate_summary(
        self,
        experiment: Dict[str, Any],
        metrics: BenchmarkMetrics
    ) -> Dict[str, Any]:
        """ìš”ì•½ ìƒì„±"""
        
        # ì„±ëŠ¥ ìˆ˜ì¤€ íŒì •
        if metrics.percentile_rank >= 90:
            level = "ìµœìƒìœ„"
            emoji = "ğŸ†"
        elif metrics.percentile_rank >= 75:
            level = "ìƒìœ„"
            emoji = "ğŸ¥‡"
        elif metrics.percentile_rank >= 50:
            level = "í‰ê·  ì´ìƒ"
            emoji = "ğŸ¥ˆ"
        elif metrics.percentile_rank >= 25:
            level = "í‰ê·  ì´í•˜"
            emoji = "ğŸ¥‰"
        else:
            level = "í•˜ìœ„"
            emoji = "ğŸ“Š"
            
        summary = {
            'overall_rating': f"{emoji} {level}",
            'percentile': f"ìƒìœ„ {100 - metrics.percentile_rank:.1f}%",
            'confidence': f"{metrics.confidence_score:.0%}",
            'sample_size': metrics.sample_size,
            'key_findings': self._extract_key_findings(metrics)
        }
        
        return summary
        
    def _generate_detailed_analysis(
        self,
        experiment: Dict[str, Any],
        literature: List[LiteratureData],
        metrics: BenchmarkMetrics
    ) -> Dict[str, Any]:
        """ìƒì„¸ ë¶„ì„"""
        
        analysis = {
            'performance_breakdown': {},
            'strength_areas': [],
            'weakness_areas': [],
            'unique_aspects': [],
            'trend_position': ""
        }
        
        # ì„±ëŠ¥ ë¶„ì„
        for metric, perf in metrics.relative_performance.items():
            analysis['performance_breakdown'][metric] = {
                'percentile': perf['percentile'],
                'vs_average': f"{((perf['value'] / perf['mean']) - 1) * 100:+.1f}%",
                'interpretation': self._interpret_performance(perf)
            }
            
            # ê°•ì /ì•½ì  ë¶„ë¥˜
            if perf['percentile'] >= 75:
                analysis['strength_areas'].append(metric)
            elif perf['percentile'] <= 25:
                analysis['weakness_areas'].append(metric)
                
        # ë…íŠ¹í•œ ì¸¡ë©´ ì‹ë³„
        analysis['unique_aspects'] = self._identify_unique_aspects(experiment, literature)
        
        # íŠ¸ë Œë“œ í¬ì§€ì…˜
        analysis['trend_position'] = self._analyze_trend_position(experiment, literature)
        
        return analysis
        
    def _interpret_performance(self, perf: Dict[str, Any]) -> str:
        """ì„±ëŠ¥ í•´ì„"""
        
        if perf['z_score'] > 2:
            return "ë§¤ìš° ìš°ìˆ˜ (2Ïƒ ì´ìƒ)"
        elif perf['z_score'] > 1:
            return "ìš°ìˆ˜ (1-2Ïƒ)"
        elif perf['z_score'] > -1:
            return "í‰ê·  ìˆ˜ì¤€ (-1Ïƒ ~ +1Ïƒ)"
        elif perf['z_score'] > -2:
            return "í‰ê·  ì´í•˜ (-2Ïƒ ~ -1Ïƒ)"
        else:
            return "ë§¤ìš° ë‚®ìŒ (-2Ïƒ ì´í•˜)"
            
    def _extract_key_findings(self, metrics: BenchmarkMetrics) -> List[str]:
        """í•µì‹¬ ë°œê²¬ì‚¬í•­ ì¶”ì¶œ"""
        
        findings = []
        
        # ì „ì²´ ì„±ëŠ¥
        findings.append(
            f"ì „ì²´ ì„±ëŠ¥ì€ {metrics.percentile_rank:.0f} ë°±ë¶„ìœ„ë¡œ, "
            f"ë¹„êµ ëŒ€ìƒ {metrics.sample_size}ê°œ ì—°êµ¬ ì¤‘ ìƒìœ„ {100 - metrics.percentile_rank:.0f}%ì— í•´ë‹¹í•©ë‹ˆë‹¤."
        )
        
        # ê°•ì  ë¶„ì•¼
        strong_metrics = [
            m for m, p in metrics.relative_performance.items()
            if p['percentile'] >= 75
        ]
        if strong_metrics:
            findings.append(
                f"íŠ¹íˆ {', '.join(strong_metrics[:3])} ë¶„ì•¼ì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤."
            )
            
        # ê°œì„  í•„ìš” ë¶„ì•¼
        weak_metrics = [
            m for m, p in metrics.relative_performance.items()
            if p['percentile'] <= 25
        ]
        if weak_metrics:
            findings.append(
                f"{', '.join(weak_metrics[:3])} ë¶„ì•¼ëŠ” ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤."
            )
            
        return findings
        
    def _identify_unique_aspects(
        self,
        experiment: Dict[str, Any],
        literature: List[LiteratureData]
    ) -> List[str]:
        """ë…íŠ¹í•œ ì¸¡ë©´ ì‹ë³„"""
        
        unique_aspects = []
        
        # ë…íŠ¹í•œ ì¬ë£Œ
        all_materials = set()
        for lit in literature:
            all_materials.update(lit.materials)
            
        exp_materials = set(experiment.get('materials', []))
        unique_materials = exp_materials - all_materials
        
        if unique_materials:
            unique_aspects.append(
                f"ë…íŠ¹í•œ ì¬ë£Œ ì‚¬ìš©: {', '.join(list(unique_materials)[:3])}"
            )
            
        # ë…íŠ¹í•œ ì¡°ê±´
        # ... (êµ¬í˜„ ìƒëµ)
        
        return unique_aspects
        
    def _analyze_trend_position(
        self,
        experiment: Dict[str, Any],
        literature: List[LiteratureData]
    ) -> str:
        """íŠ¸ë Œë“œ ìœ„ì¹˜ ë¶„ì„"""
        
        # ìµœê·¼ ì—°êµ¬ ë¹„ì¤‘
        recent_years = [lit.year for lit in literature if lit.year >= datetime.now().year - 2]
        recent_ratio = len(recent_years) / len(literature) if literature else 0
        
        if recent_ratio > 0.5:
            return "í™œë°œíˆ ì—°êµ¬ë˜ëŠ” ìµœì‹  ë¶„ì•¼"
        elif recent_ratio > 0.2:
            return "ì§€ì†ì ìœ¼ë¡œ ì—°êµ¬ë˜ëŠ” ë¶„ì•¼"
        else:
            return "ì„±ìˆ™í•œ ì—°êµ¬ ë¶„ì•¼"
            
    def _generate_methodology_section(self) -> Dict[str, Any]:
        """ë°©ë²•ë¡  ì„¹ì…˜"""
        
        return {
            'data_sources': [
                "OpenAlex - í•™ìˆ  ë…¼ë¬¸ ë©”íƒ€ë°ì´í„°",
                "Crossref - DOI ê¸°ë°˜ ì„œì§€ì •ë³´",
                "PubMed - ìƒì˜í•™ ë¬¸í—Œ",
                "Materials Project - ì¬ë£Œ ë¬¼ì„± ë°ì´í„°"
            ],
            'matching_algorithm': "ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ ë‹¤ì°¨ì› ë§¤ì¹­",
            'statistical_methods': [
                "ë°±ë¶„ìœ„ ìˆœìœ„ ê³„ì‚°",
                "Z-score í‘œì¤€í™”",
                "í”¼ì–´ìŠ¨ ìƒê´€ê´€ê³„ ë¶„ì„"
            ],
            'confidence_calculation': "ìƒ˜í”Œ í¬ê¸°, ê´€ë ¨ì„± ì ìˆ˜, ë°ì´í„° ì™„ì „ì„± ê¸°ë°˜"
        }
        
    def _identify_limitations(
        self,
        literature: List[LiteratureData],
        metrics: BenchmarkMetrics
    ) -> List[str]:
        """ë¶„ì„ í•œê³„ì  ì‹ë³„"""
        
        limitations = []
        
        if metrics.sample_size < 10:
            limitations.append("ë¹„êµ ê°€ëŠ¥í•œ ë¬¸í—Œì´ ì ì–´ í†µê³„ì  ì‹ ë¢°ë„ê°€ ì œí•œì ì…ë‹ˆë‹¤.")
            
        if metrics.confidence_score < 0.7:
            limitations.append("ë°ì´í„° ë§¤ì¹­ ì‹ ë¢°ë„ê°€ ë‚®ì•„ í•´ì„ì— ì£¼ì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            
        # ì‹œê°„ì  í¸í–¥
        years = [lit.year for lit in literature if lit.year]
        if years:
            avg_year = np.mean(years)
            if datetime.now().year - avg_year > 5:
                limitations.append("ë¹„êµ ë¬¸í—Œì´ ë‹¤ì†Œ ì˜¤ë˜ë˜ì–´ ìµœì‹  ë™í–¥ì„ ë°˜ì˜í•˜ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                
        return limitations
        
    def _suggest_next_steps(
        self,
        experiment: Dict[str, Any],
        metrics: BenchmarkMetrics
    ) -> List[str]:
        """ë‹¤ìŒ ë‹¨ê³„ ì œì•ˆ"""
        
        next_steps = []
        
        # ì„±ëŠ¥ ìˆ˜ì¤€ì— ë”°ë¥¸ ì œì•ˆ
        if metrics.percentile_rank >= 80:
            next_steps.append("ìš°ìˆ˜í•œ ê²°ê³¼ë¥¼ ë…¼ë¬¸ìœ¼ë¡œ ë°œí‘œí•˜ëŠ” ê²ƒì„ ê³ ë ¤í•˜ì„¸ìš”.")
            next_steps.append("íŠ¹í—ˆ ì¶œì› ê°€ëŠ¥ì„±ì„ ê²€í† í•˜ì„¸ìš”.")
        elif metrics.percentile_rank >= 50:
            next_steps.append("ìƒìœ„ ì—°êµ¬ì˜ ë°©ë²•ë¡ ì„ ì°¸ê³ í•˜ì—¬ ì¶”ê°€ ì‹¤í—˜ì„ ê³„íší•˜ì„¸ìš”.")
            next_steps.append("ê°œì„ ëœ ì¡°ê±´ìœ¼ë¡œ ì¬í˜„ ì‹¤í—˜ì„ ìˆ˜í–‰í•˜ì„¸ìš”.")
        else:
            next_steps.append("ê·¼ë³¸ì ì¸ ì ‘ê·¼ ë°©ë²• ì¬ê²€í† ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            next_steps.append("ì „ë¬¸ê°€ ìë¬¸ì„ êµ¬í•˜ëŠ” ê²ƒì„ ê³ ë ¤í•˜ì„¸ìš”.")
            
        # ì•½ì  ê¸°ë°˜ ì œì•ˆ
        if metrics.improvement_suggestions:
            next_steps.append("ì œì•ˆëœ ê°œì„ ì‚¬í•­ì„ ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ì ìš©í•˜ì„¸ìš”.")
            
        return next_steps
        
    # ===== ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ =====
    
    def _prepare_search_params(self, experiment: Dict[str, Any]) -> Dict[str, Any]:
        """ê²€ìƒ‰ íŒŒë¼ë¯¸í„° ì¤€ë¹„"""
        
        params = {
            'keywords': [],
            'materials': experiment.get('materials', []),
            'filters': {},
            'limit': 100
        }
        
        # í‚¤ì›Œë“œ ì¶”ì¶œ
        if 'name' in experiment:
            params['keywords'].extend(self._extract_keywords(experiment['name']))
        if 'description' in experiment:
            params['keywords'].extend(self._extract_keywords(experiment['description']))
            
        # ì‹¤í—˜ ìœ í˜•
        if 'type' in experiment:
            params['filters']['type'] = experiment['type']
            
        # ë‚ ì§œ ë²”ìœ„ (ìµœê·¼ 10ë…„)
        params['filters']['from_year'] = datetime.now().year - 10
        
        return params
        
    def _extract_keywords(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (ì‹¤ì œë¡œëŠ” NLP ì‚¬ìš© ê¶Œì¥)
        stopwords = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for'}
        words = text.lower().split()
        keywords = [w for w in words if len(w) > 3 and w not in stopwords]
        
        return keywords[:10]  # ìµœëŒ€ 10ê°œ
        
    def _extract_metrics_from_text(self, text: str) -> Dict[str, Any]:
        """í…ìŠ¤íŠ¸ì—ì„œ ë©”íŠ¸ë¦­ ì¶”ì¶œ"""
        
        metrics = {}
        
        # ìˆ«ì íŒ¨í„´ ì°¾ê¸° (ê°„ë‹¨í•œ ì˜ˆì‹œ)
        import re
        
        # ë°±ë¶„ìœ¨
        percent_pattern = r'(\d+\.?\d*)\s*%'
        percents = re.findall(percent_pattern, text)
        if percents:
            metrics['percentage'] = float(percents[0])
            
        # ì˜¨ë„
        temp_pattern = r'(\d+\.?\d*)\s*Â°C'
        temps = re.findall(temp_pattern, text)
        if temps:
            metrics['temperature'] = float(temps[0])
            
        # ìˆ˜ìœ¨
        yield_pattern = r'yield[:\s]+(\d+\.?\d*)'
        yields = re.findall(yield_pattern, text, re.IGNORECASE)
        if yields:
            metrics['yield'] = float(yields[0])
            
        return metrics
        
    def _deduplicate_literature(self, literature: List[LiteratureData]) -> List[LiteratureData]:
        """ì¤‘ë³µ ë¬¸í—Œ ì œê±°"""
        
        seen = set()
        unique = []
        
        for lit in literature:
            # DOI ê¸°ë°˜ ì¤‘ë³µ ì²´í¬
            if lit.doi and lit.doi in seen:
                continue
                
            # ì œëª© ê¸°ë°˜ ì¤‘ë³µ ì²´í¬
            title_key = lit.title.lower().strip()
            if title_key in seen:
                continue
                
            seen.add(lit.doi if lit.doi else title_key)
            unique.append(lit)
            
        return unique
        
    def _get_cache_key(self, params: Dict[str, Any]) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        
        # íŒŒë¼ë¯¸í„°ë¥¼ ì •ë ¬í•˜ì—¬ ì¼ê´€ëœ í‚¤ ìƒì„±
        sorted_params = json.dumps(params, sort_keys=True)
        return hashlib.md5(sorted_params.encode()).hexdigest()
        
    def _calculate_confidence_score(
        self,
        literature: List[LiteratureData],
        performance: Dict[str, Any]
    ) -> float:
        """ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°"""
        
        score = 0.0
        weights = {
            'sample_size': 0.3,
            'relevance': 0.3,
            'data_completeness': 0.2,
            'recency': 0.2
        }
        
        # ìƒ˜í”Œ í¬ê¸°
        if len(literature) >= 50:
            score += weights['sample_size']
        elif len(literature) >= 20:
            score += weights['sample_size'] * 0.7
        elif len(literature) >= 10:
            score += weights['sample_size'] * 0.5
        else:
            score += weights['sample_size'] * 0.3
            
        # ê´€ë ¨ì„±
        avg_relevance = np.mean([lit.relevance_score for lit in literature]) if literature else 0
        score += weights['relevance'] * avg_relevance
        
        # ë°ì´í„° ì™„ì „ì„±
        if performance:
            completeness = len([p for p in performance.values() if p['sample_size'] > 5]) / len(performance)
            score += weights['data_completeness'] * completeness
            
        # ìµœì‹ ì„±
        if literature:
            recent_count = len([lit for lit in literature if lit.year >= datetime.now().year - 3])
            recency_ratio = recent_count / len(literature)
            score += weights['recency'] * recency_ratio
            
        return min(score, 1.0)

# ===========================================================================
# ğŸ”§ í—¬í¼ í•¨ìˆ˜
# ===========================================================================

async def quick_benchmark(
    experiment_data: Dict[str, Any],
    metric_name: str = None
) -> Dict[str, Any]:
    """ë¹ ë¥¸ ë²¤ì¹˜ë§ˆí¬ ë¶„ì„"""
    
    analyzer = BenchmarkAnalyzer()
    
    # íŠ¹ì • ë©”íŠ¸ë¦­ë§Œ ë¶„ì„
    if metric_name:
        experiment_data = {
            'name': experiment_data.get('name', 'Quick Analysis'),
            'results': {metric_name: experiment_data['results'].get(metric_name)}
        }
        
    result = await analyzer.analyze_benchmark(
        experiment_data,
        analysis_type='quick'
    )
    
    return {
        'percentile': result.metrics.percentile_rank,
        'sample_size': result.metrics.sample_size,
        'suggestions': result.metrics.improvement_suggestions[:3]
    }
