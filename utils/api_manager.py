"""
ğŸ¤– Universal DOE Platform - API í†µí•© ê´€ë¦¬ì
================================================================================
6ê°œ AI ì—”ì§„ê³¼ ê³¼í•™ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ í†µí•© ê´€ë¦¬í•˜ëŠ” í•µì‹¬ ëª¨ë“ˆ
ì˜¤í”„ë¼ì¸ ìš°ì„  ì„¤ê³„, ìºì‹±, í´ë°± ë©”ì»¤ë‹ˆì¦˜ ì œê³µ
================================================================================
"""

import os
import json
import logging
import asyncio
import time
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple, Union, Callable
from dataclasses import dataclass, field, asdict
from collections import defaultdict, deque
from functools import wraps, lru_cache
from enum import Enum
import hashlib
import threading
from pathlib import Path
import base64
from cryptography.fernet import Fernet

# ì„œë“œíŒŒí‹° ë¼ì´ë¸ŒëŸ¬ë¦¬
import streamlit as st
import pandas as pd
import numpy as np
from tenacity import retry, stop_after_attempt, wait_exponential
import aiohttp
import httpx

# AI ë¼ì´ë¸ŒëŸ¬ë¦¬ - ì„ íƒì  import
try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False
    logging.warning("Google Gemini API ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

try:
    from openai import AsyncOpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    logging.warning("OpenAI API ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

try:
    from groq import AsyncGroq
    GROQ_AVAILABLE = True
except ImportError:
    GROQ_AVAILABLE = False
    logging.warning("Groq API ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

try:
    from transformers import pipeline
    import torch
    HUGGINGFACE_AVAILABLE = True
except ImportError:
    HUGGINGFACE_AVAILABLE = False
    logging.warning("HuggingFace ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

# ê³¼í•™ ë°ì´í„°ë² ì´ìŠ¤ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    from mp_api.client import MPRester
    MATERIALS_PROJECT_AVAILABLE = True
except ImportError:
    MATERIALS_PROJECT_AVAILABLE = False
    logging.warning("Materials Project API ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

try:
    import pubchempy as pcp
    PUBCHEM_AVAILABLE = True
except ImportError:
    PUBCHEM_AVAILABLE = False
    logging.warning("PubChemPy ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

try:
    from github import Github
    GITHUB_AVAILABLE = True
except ImportError:
    GITHUB_AVAILABLE = False
    logging.warning("PyGithub ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

# ë¡œì»¬ ëª¨ë“ˆ ì„í¬íŠ¸
try:
    from config.app_config import (
        API_CONFIG, FILE_PROCESSING, PROTOCOL_EXTRACTION,
        get_config
    )
    from config.secrets_config import API_KEY_STRUCTURE
except ImportError:
    # ê¸°ë³¸ê°’ ì„¤ì •
    API_CONFIG = {
        'google_gemini': {
            'name': 'Google Gemini 2.0 Flash',
            'model': 'gemini-2.0-flash-exp',
            'required': True,
            'free_tier': True,
            'rate_limit': 60,
            'max_tokens': 1048576
        }
    }
    FILE_PROCESSING = {
        'allowed_extensions': {
            'document': ['.pdf', '.docx', '.doc', '.txt', '.rtf'],
            'markup': ['.html', '.htm', '.md', '.xml'],
            'data': ['.json', '.csv']
        }
    }

# ===========================================================================
# ğŸ”§ ë¡œê¹… ì„¤ì •
# ===========================================================================

logger = logging.getLogger(__name__)

# ===========================================================================
# ğŸ“Œ ìƒìˆ˜ ë° Enum ì •ì˜
# ===========================================================================

class AIEngineType(Enum):
    """AI ì—”ì§„ íƒ€ì…"""
    GEMINI = "gemini"
    GROK = "grok"
    GROQ = "groq"
    SAMBANOVA = "sambanova"
    DEEPSEEK = "deepseek"
    HUGGINGFACE = "huggingface"

class ResponseStatus(Enum):
    """API ì‘ë‹µ ìƒíƒœ"""
    SUCCESS = "success"
    ERROR = "error"
    RATE_LIMITED = "rate_limited"
    CACHED = "cached"
    OFFLINE = "offline"

# Rate Limit ì„¤ì •
RATE_LIMITS = {
    AIEngineType.GEMINI: {'calls': 60, 'period': 60},  # 60 calls/min
    AIEngineType.GROQ: {'calls': 100, 'period': 60},   # 100 calls/min
    AIEngineType.HUGGINGFACE: {'calls': 1000, 'period': 3600},  # 1000 calls/hour
    'default': {'calls': 30, 'period': 60}
}

# ìºì‹œ TTL ì„¤ì • (ì´ˆ)
CACHE_TTL = {
    'ai_response': 3600,      # 1ì‹œê°„
    'material_data': 86400,   # 24ì‹œê°„
    'compound_data': 86400,   # 24ì‹œê°„
    'protocol': 7200,         # 2ì‹œê°„
    'default': 1800           # 30ë¶„
}

# API ë¹„ìš© ì¶”ì • (1K í† í°ë‹¹ USD)
API_COSTS = {
    AIEngineType.GEMINI: {'input': 0.0, 'output': 0.0},  # ë¬´ë£Œ
    AIEngineType.GROQ: {'input': 0.0, 'output': 0.0},    # ë¬´ë£Œ
    AIEngineType.DEEPSEEK: {'input': 0.001, 'output': 0.002},
    AIEngineType.SAMBANOVA: {'input': 0.0, 'output': 0.0},  # ë¬´ë£Œ í‹°ì–´
    'default': {'input': 0.001, 'output': 0.002}
}

# ===========================================================================
# ğŸ”§ í”„ë¡œí† ì½œ ì¶”ì¶œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
# ===========================================================================

EXTRACTION_PROMPTS = {
    'pdf_academic': """
í•™ìˆ  ë…¼ë¬¸ PDFì—ì„œ ì‹¤í—˜ í”„ë¡œí† ì½œì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
Methods/Experimental/Materials and Methods ì„¹ì…˜ì„ ì¤‘ì ì ìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”.

ì¶”ì¶œí•´ì•¼ í•  ì •ë³´:
1. ì¬ë£Œ ë° ì‹œì•½ (ì´ë¦„, ìˆœë„, ê³µê¸‰ì—…ì²´)
2. ì¥ë¹„ ë° ë„êµ¬ (ëª¨ë¸ëª…, ì œì¡°ì‚¬)
3. ì‹¤í—˜ ì¡°ê±´ (ì˜¨ë„, ì••ë ¥, ì‹œê°„, pH ë“±)
4. ì‹¤í—˜ ì ˆì°¨ (ë‹¨ê³„ë³„ ìƒì„¸ ì„¤ëª…)
5. ì£¼ì˜ì‚¬í•­ ë° ì•ˆì „ ì •ë³´

JSON í˜•ì‹ìœ¼ë¡œ êµ¬ì¡°í™”í•˜ì—¬ ë°˜í™˜í•˜ì„¸ìš”.
""",
    
    'text_protocol': """
ì¼ë°˜ í…ìŠ¤íŠ¸ì—ì„œ ì‹¤í—˜ ì ˆì°¨ë¥¼ ì‹ë³„í•©ë‹ˆë‹¤.
ë²ˆí˜¸ë‚˜ ë¶ˆë¦¿ìœ¼ë¡œ êµ¬ë¶„ëœ ë‹¨ê³„ë¥¼ ì°¾ê³ , ì¬ë£Œì™€ ì¡°ê±´ì„ êµ¬ë¶„í•˜ì„¸ìš”.

ì¶”ì¶œ í¬ë§·:
{
  "materials": [...],
  "equipment": [...],
  "conditions": {...},
  "procedure": [...],
  "safety": [...]
}
""",
    
    'html_webpage': """
ì›¹í˜ì´ì§€ì—ì„œ í”„ë¡œí† ì½œ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
êµ¬ì¡°í™”ëœ ë¦¬ìŠ¤íŠ¸, í…Œì´ë¸”, ë˜ëŠ” ë‹¨ê³„ë³„ ì„¤ëª…ì„ ì°¾ìœ¼ì„¸ìš”.
ë„¤ë¹„ê²Œì´ì…˜ì´ë‚˜ ê´‘ê³  ê°™ì€ ë¬´ê´€í•œ ë‚´ìš©ì€ ì œì™¸í•˜ì„¸ìš”.
""",
    
    'mixed_format': """
ë‹¤ì–‘í•œ í˜•ì‹ì´ í˜¼ì¬ëœ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
ì¬ë£Œ, ì¡°ê±´, ì ˆì°¨ë¥¼ êµ¬ë¶„í•˜ì—¬ ì¶”ì¶œí•˜ê³ , ë…¼ë¦¬ì  ìˆœì„œë¡œ ì •ë¦¬í•˜ì„¸ìš”.
ìˆ˜ëŸ‰, ë‹¨ìœ„, ì‹œê°„ ì •ë³´ë¥¼ ì •í™•íˆ íŒŒì•…í•˜ì„¸ìš”.
"""
}

# ===========================================================================
# ğŸ“Š ë°ì´í„° í´ë˜ìŠ¤
# ===========================================================================

@dataclass
class APIResponse:
    """API ì‘ë‹µ ë°ì´í„° í´ë˜ìŠ¤"""
    status: ResponseStatus
    data: Optional[Any] = None
    error: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)
    cached: bool = False
    timestamp: datetime = field(default_factory=datetime.now)

@dataclass
class UsageRecord:
    """API ì‚¬ìš©ëŸ‰ ê¸°ë¡"""
    user_id: str
    api_type: str
    timestamp: datetime
    tokens_used: Optional[int] = None
    cost: Optional[float] = None
    success: bool = True
    error: Optional[str] = None

# ===========================================================================
# ğŸ” ì•”í˜¸í™” ê´€ë¦¬ì
# ===========================================================================

class EncryptionManager:
    """API í‚¤ ì•”í˜¸í™” ê´€ë¦¬"""
    
    def __init__(self):
        self.key = self._get_or_create_key()
        self.cipher = Fernet(self.key)
    
    def _get_or_create_key(self) -> bytes:
        """ì•”í˜¸í™” í‚¤ ê°€ì ¸ì˜¤ê¸° ë˜ëŠ” ìƒì„±"""
        key_file = Path.home() / '.universaldoe' / 'api_key.key'
        key_file.parent.mkdir(parents=True, exist_ok=True)
        
        if key_file.exists():
            return key_file.read_bytes()
        else:
            key = Fernet.generate_key()
            key_file.write_bytes(key)
            return key
    
    def encrypt(self, value: str) -> str:
        """ë¬¸ìì—´ ì•”í˜¸í™”"""
        return self.cipher.encrypt(value.encode()).decode()
    
    def decrypt(self, encrypted: str) -> str:
        """ë¬¸ìì—´ ë³µí˜¸í™”"""
        return self.cipher.decrypt(encrypted.encode()).decode()

# ===========================================================================
# â±ï¸ Rate Limiter
# ===========================================================================

class RateLimiter:
    """API Rate Limiting"""
    
    def __init__(self, max_calls: int, period: int):
        self.max_calls = max_calls
        self.period = period  # ì´ˆ
        self.calls = deque()
        self.lock = threading.Lock()
    
    def check(self) -> bool:
        """í˜¸ì¶œ ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        with self.lock:
            now = time.time()
            # ê¸°ê°„ì´ ì§€ë‚œ í˜¸ì¶œ ì œê±°
            while self.calls and self.calls[0] < now - self.period:
                self.calls.popleft()
            
            return len(self.calls) < self.max_calls
    
    def record(self):
        """í˜¸ì¶œ ê¸°ë¡"""
        with self.lock:
            self.calls.append(time.time())
    
    def get_remaining(self) -> Dict[str, Any]:
        """ë‚¨ì€ í˜¸ì¶œ ìˆ˜ ë°˜í™˜"""
        with self.lock:
            now = time.time()
            while self.calls and self.calls[0] < now - self.period:
                self.calls.popleft()
            
            return {
                'remaining': self.max_calls - len(self.calls),
                'reset_in': int(self.period - (now - self.calls[0])) if self.calls else 0
            }

# ===========================================================================
# ğŸ’¾ ìºì‹œ ì‹œìŠ¤í…œ
# ===========================================================================

class APICache:
    """API ì‘ë‹µ ìºì‹±"""
    
    def __init__(self, ttl: int = 3600):
        self.ttl = ttl
        self.cache: Dict[str, Tuple[Any, float]] = {}
        self.lock = threading.Lock()
    
    def _make_key(self, prefix: str, params: Dict) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        param_str = json.dumps(params, sort_keys=True)
        return f"{prefix}:{hashlib.md5(param_str.encode()).hexdigest()}"
    
    def get(self, prefix: str, params: Dict) -> Optional[Any]:
        """ìºì‹œì—ì„œ ê°€ì ¸ì˜¤ê¸°"""
        with self.lock:
            key = self._make_key(prefix, params)
            if key in self.cache:
                data, timestamp = self.cache[key]
                if time.time() - timestamp < self.ttl:
                    return data
                else:
                    del self.cache[key]
            return None
    
    def set(self, prefix: str, params: Dict, data: Any):
        """ìºì‹œì— ì €ì¥"""
        with self.lock:
            key = self._make_key(prefix, params)
            self.cache[key] = (data, time.time())
    
    def clear(self, prefix: Optional[str] = None):
        """ìºì‹œ ì´ˆê¸°í™”"""
        with self.lock:
            if prefix:
                keys_to_remove = [k for k in self.cache.keys() if k.startswith(prefix)]
                for key in keys_to_remove:
                    del self.cache[key]
            else:
                self.cache.clear()

# ===========================================================================
# ğŸ“Š ì‚¬ìš©ëŸ‰ ì¶”ì 
# ===========================================================================

class UsageTracker:
    """API ì‚¬ìš©ëŸ‰ ì¶”ì """
    
    def __init__(self):
        self.records: List[UsageRecord] = []
        self.lock = threading.Lock()
    
    def record(self, record: UsageRecord):
        """ì‚¬ìš©ëŸ‰ ê¸°ë¡"""
        with self.lock:
            self.records.append(record)
            # 7ì¼ ì´ìƒ ëœ ê¸°ë¡ ì œê±°
            cutoff = datetime.now() - timedelta(days=7)
            self.records = [r for r in self.records if r.timestamp > cutoff]
    
    def get_summary(self, user_id: Optional[str] = None, 
                   period: str = 'day') -> Dict[str, Any]:
        """ì‚¬ìš©ëŸ‰ ìš”ì•½"""
        with self.lock:
            # ê¸°ê°„ ì„¤ì •
            if period == 'day':
                start = datetime.now() - timedelta(days=1)
            elif period == 'week':
                start = datetime.now() - timedelta(days=7)
            elif period == 'month':
                start = datetime.now() - timedelta(days=30)
            else:
                start = datetime.min
            
            # í•„í„°ë§
            filtered = [r for r in self.records if r.timestamp >= start]
            if user_id:
                filtered = [r for r in filtered if r.user_id == user_id]
            
            # ì§‘ê³„
            summary = defaultdict(lambda: {
                'calls': 0,
                'tokens': 0,
                'cost': 0.0,
                'errors': 0
            })
            
            for record in filtered:
                api_summary = summary[record.api_type]
                api_summary['calls'] += 1
                api_summary['tokens'] += record.tokens_used or 0
                api_summary['cost'] += record.cost or 0
                if not record.success:
                    api_summary['errors'] += 1
            
            return dict(summary)

# ===========================================================================
# ğŸ¤– AI ì—”ì§„ ê¸°ë³¸ í´ë˜ìŠ¤
# ===========================================================================

class BaseAIEngine:
    """AI ì—”ì§„ ì¶”ìƒ í´ë˜ìŠ¤"""
    
    def __init__(self, engine_type: AIEngineType, model_name: str):
        self.engine_type = engine_type
        self.model_name = model_name
        self.api_key = self._get_api_key()
        self.is_available = self._check_availability()
        
        # Rate Limiter
        limits = RATE_LIMITS.get(engine_type, RATE_LIMITS['default'])
        self.rate_limiter = RateLimiter(limits['calls'], limits['period'])
        
        # Cache
        ttl = CACHE_TTL.get('ai_response', CACHE_TTL['default'])
        self.cache = APICache(ttl)
        
        # ë¹„ìš© ì •ë³´
        self.costs = API_COSTS.get(engine_type, API_COSTS['default'])
    
    def _get_api_key(self) -> Optional[str]:
        """API í‚¤ ê°€ì ¸ì˜¤ê¸°"""
        # 1. ì„¸ì…˜ ìƒíƒœ í™•ì¸
        if hasattr(st, 'session_state') and 'api_keys' in st.session_state:
            key = st.session_state.api_keys.get(self.engine_type.value)
            if key:
                return key
        
        # 2. Streamlit secrets í™•ì¸
        if hasattr(st, 'secrets'):
            try:
                key = st.secrets.get(f"{self.engine_type.value}_api_key")
                if key:
                    return key
            except:
                pass
        
        # 3. í™˜ê²½ ë³€ìˆ˜ í™•ì¸
        env_key = f"{self.engine_type.value.upper()}_API_KEY"
        return os.getenv(env_key)
    
    def _check_availability(self) -> bool:
        """ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        return bool(self.api_key)
    
    async def generate(self, prompt: str, user_id: str = "anonymous", 
                      **kwargs) -> APIResponse:
        """í…ìŠ¤íŠ¸ ìƒì„± (êµ¬í˜„ í•„ìš”)"""
        raise NotImplementedError
    
    def estimate_cost(self, input_tokens: int, output_tokens: int) -> float:
        """ë¹„ìš© ì¶”ì •"""
        input_cost = (input_tokens / 1000) * self.costs['input']
        output_cost = (output_tokens / 1000) * self.costs['output']
        return input_cost + output_cost

# ===========================================================================
# ğŸŒŸ Google Gemini ì—”ì§„
# ===========================================================================

class GeminiEngine(BaseAIEngine):
    """Google Gemini AI ì—”ì§„"""
    
    def __init__(self):
        super().__init__(AIEngineType.GEMINI, "gemini-2.0-flash-exp")
        if self.is_available and GEMINI_AVAILABLE:
            genai.configure(api_key=self.api_key)
            self.model = genai.GenerativeModel(self.model_name)
            self.chat = None
    
    async def generate(self, prompt: str, user_id: str = "anonymous", 
                      **kwargs) -> APIResponse:
        """Geminië¡œ í…ìŠ¤íŠ¸ ìƒì„±"""
        # ìºì‹œ í™•ì¸
        cache_params = {'prompt': prompt, **kwargs}
        cached = self.cache.get('gemini', cache_params)
        if cached:
            return APIResponse(
                status=ResponseStatus.CACHED,
                data=cached,
                cached=True
            )
        
        # Rate limit í™•ì¸
        if not self.rate_limiter.check():
            return APIResponse(
                status=ResponseStatus.RATE_LIMITED,
                error="Rate limit exceeded"
            )
        
        try:
            # ë™ê¸° í˜¸ì¶œì„ ë¹„ë™ê¸°ë¡œ ë³€í™˜
            response = await asyncio.to_thread(
                self.model.generate_content, prompt
            )
            
            # Rate limit ê¸°ë¡
            self.rate_limiter.record()
            
            # ì‘ë‹µ ì²˜ë¦¬
            result = response.text
            
            # ìºì‹œ ì €ì¥
            self.cache.set('gemini', cache_params, result)
            
            # ì‚¬ìš©ëŸ‰ ê¸°ë¡ (ì¶”í›„ í† í° ê³„ì‚° ì¶”ê°€)
            # tokens = response.usage_metadata...
            
            return APIResponse(
                status=ResponseStatus.SUCCESS,
                data=result,
                metadata={'model': self.model_name}
            )
            
        except Exception as e:
            logger.error(f"Gemini ì—ëŸ¬: {str(e)}")
            return APIResponse(
                status=ResponseStatus.ERROR,
                error=str(e)
            )
    
    async def extract_protocol(self, text: str, file_type: str = "mixed_format",
                             user_id: str = "anonymous") -> APIResponse:
        """í”„ë¡œí† ì½œ ì¶”ì¶œ íŠ¹í™” ê¸°ëŠ¥"""
        # íŒŒì¼ íƒ€ì…ë³„ í”„ë¡¬í”„íŠ¸ ì„ íƒ
        base_prompt = EXTRACTION_PROMPTS.get(file_type, EXTRACTION_PROMPTS['mixed_format'])
        
        # ì „ì²´ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        full_prompt = f"""
{base_prompt}

í…ìŠ¤íŠ¸:
{text[:10000]}  # ê¸¸ì´ ì œí•œ

ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:
{{
  "materials": [
    {{"name": "ì¬ë£Œëª…", "amount": "ì–‘", "purity": "ìˆœë„", "supplier": "ê³µê¸‰ì—…ì²´"}}
  ],
  "equipment": [
    {{"name": "ì¥ë¹„ëª…", "model": "ëª¨ë¸", "manufacturer": "ì œì¡°ì‚¬"}}
  ],
  "conditions": {{
    "temperature": "ì˜¨ë„",
    "pressure": "ì••ë ¥",
    "time": "ì‹œê°„",
    "ph": "pH",
    "other": {{}}
  }},
  "procedure": [
    {{"step": 1, "action": "ë™ì‘", "details": "ìƒì„¸ ì„¤ëª…", "duration": "ì†Œìš” ì‹œê°„"}}
  ],
  "safety": ["ì£¼ì˜ì‚¬í•­1", "ì£¼ì˜ì‚¬í•­2"]
}}
"""
        
        response = await self.generate(full_prompt, user_id)
        
        if response.status == ResponseStatus.SUCCESS:
            try:
                # JSON íŒŒì‹±
                protocol_data = json.loads(response.data)
                response.data = protocol_data
            except json.JSONDecodeError:
                # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ í…ìŠ¤íŠ¸ ê·¸ëŒ€ë¡œ ë°˜í™˜
                logger.warning("í”„ë¡œí† ì½œ JSON íŒŒì‹± ì‹¤íŒ¨")
        
        return response

# ===========================================================================
# âš¡ Groq ì—”ì§„
# ===========================================================================

class GroqEngine(BaseAIEngine):
    """Groq ì´ˆê³ ì† ì¶”ë¡  ì—”ì§„"""
    
    def __init__(self):
        super().__init__(AIEngineType.GROQ, "llama-3.3-70b-versatile")
        if self.is_available and GROQ_AVAILABLE:
            self.client = AsyncGroq(api_key=self.api_key)
    
    async def generate(self, prompt: str, user_id: str = "anonymous", 
                      **kwargs) -> APIResponse:
        """Groqë¡œ í…ìŠ¤íŠ¸ ìƒì„±"""
        # ìºì‹œ í™•ì¸
        cache_params = {'prompt': prompt, **kwargs}
        cached = self.cache.get('groq', cache_params)
        if cached:
            return APIResponse(
                status=ResponseStatus.CACHED,
                data=cached,
                cached=True
            )
        
        # Rate limit í™•ì¸
        if not self.rate_limiter.check():
            return APIResponse(
                status=ResponseStatus.RATE_LIMITED,
                error="Rate limit exceeded"
            )
        
        try:
            # API í˜¸ì¶œ
            response = await self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": "You are an expert in materials science and chemistry."},
                    {"role": "user", "content": prompt}
                ],
                temperature=kwargs.get('temperature', 0.7),
                max_tokens=kwargs.get('max_tokens', 4096)
            )
            
            # Rate limit ê¸°ë¡
            self.rate_limiter.record()
            
            # ì‘ë‹µ ì²˜ë¦¬
            result = response.choices[0].message.content
            
            # ìºì‹œ ì €ì¥
            self.cache.set('groq', cache_params, result)
            
            # ì‚¬ìš©ëŸ‰ ê¸°ë¡
            if hasattr(response, 'usage'):
                tokens = response.usage.total_tokens
                # í† í° ê¸°ë¡...
            
            return APIResponse(
                status=ResponseStatus.SUCCESS,
                data=result,
                metadata={'model': self.model_name}
            )
            
        except Exception as e:
            logger.error(f"Groq ì—ëŸ¬: {str(e)}")
            return APIResponse(
                status=ResponseStatus.ERROR,
                error=str(e)
            )

# ===========================================================================
# ğŸ¤— HuggingFace ì—”ì§„
# ===========================================================================

class HuggingFaceEngine(BaseAIEngine):
    """HuggingFace íŠ¹ìˆ˜ ëª¨ë¸ ì—”ì§„"""
    
    def __init__(self):
        super().__init__(AIEngineType.HUGGINGFACE, "microsoft/BioGPT-Large")
        if self.is_available and HUGGINGFACE_AVAILABLE:
            # ë¡œì»¬ ëª¨ë¸ ë˜ëŠ” API ì‚¬ìš©
            self.use_local = kwargs.get('use_local', False)
            if self.use_local:
                self._init_local_model()
            else:
                self._init_api_client()
    
    def _init_local_model(self):
        """ë¡œì»¬ ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            self.pipeline = pipeline(
                "text-generation",
                model=self.model_name,
                device=0 if torch.cuda.is_available() else -1
            )
        except Exception as e:
            logger.error(f"ë¡œì»¬ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.is_available = False
    
    def _init_api_client(self):
        """HuggingFace API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        self.api_url = f"https://api-inference.huggingface.co/models/{self.model_name}"
        self.headers = {"Authorization": f"Bearer {self.api_key}"}
    
    async def generate(self, prompt: str, user_id: str = "anonymous", 
                      **kwargs) -> APIResponse:
        """HuggingFaceë¡œ í…ìŠ¤íŠ¸ ìƒì„±"""
        # ìºì‹œ í™•ì¸
        cache_params = {'prompt': prompt, **kwargs}
        cached = self.cache.get('huggingface', cache_params)
        if cached:
            return APIResponse(
                status=ResponseStatus.CACHED,
                data=cached,
                cached=True
            )
        
        try:
            if self.use_local and hasattr(self, 'pipeline'):
                # ë¡œì»¬ ì¶”ë¡ 
                result = await asyncio.to_thread(
                    self.pipeline,
                    prompt,
                    max_length=kwargs.get('max_length', 512),
                    temperature=kwargs.get('temperature', 0.7)
                )
                text = result[0]['generated_text']
            else:
                # API í˜¸ì¶œ
                async with aiohttp.ClientSession() as session:
                    payload = {
                        "inputs": prompt,
                        "parameters": {
                            "max_length": kwargs.get('max_length', 512),
                            "temperature": kwargs.get('temperature', 0.7)
                        }
                    }
                    async with session.post(
                        self.api_url,
                        headers=self.headers,
                        json=payload
                    ) as response:
                        result = await response.json()
                        text = result[0]['generated_text']
            
            # ìºì‹œ ì €ì¥
            self.cache.set('huggingface', cache_params, text)
            
            return APIResponse(
                status=ResponseStatus.SUCCESS,
                data=text,
                metadata={'model': self.model_name}
            )
            
        except Exception as e:
            logger.error(f"HuggingFace ì—ëŸ¬: {str(e)}")
            return APIResponse(
                status=ResponseStatus.ERROR,
                error=str(e)
            )

# ===========================================================================
# ğŸ”„ OpenAI í˜¸í™˜ ì—”ì§„ (Grok, DeepSeek, SambaNova)
# ===========================================================================

class OpenAICompatibleEngine(BaseAIEngine):
    """OpenAI API í˜¸í™˜ ì—”ì§„"""
    
    def __init__(self, engine_type: AIEngineType, model_name: str, 
                 base_url: str):
        super().__init__(engine_type, model_name)
        self.base_url = base_url
        if self.is_available and OPENAI_AVAILABLE:
            self.client = AsyncOpenAI(
                api_key=self.api_key,
                base_url=base_url
            )
    
    async def generate(self, prompt: str, user_id: str = "anonymous", 
                      **kwargs) -> APIResponse:
        """OpenAI í˜¸í™˜ APIë¡œ í…ìŠ¤íŠ¸ ìƒì„±"""
        # ìºì‹œ í™•ì¸
        cache_params = {'prompt': prompt, **kwargs}
        cached = self.cache.get(self.engine_type.value, cache_params)
        if cached:
            return APIResponse(
                status=ResponseStatus.CACHED,
                data=cached,
                cached=True
            )
        
        # Rate limit í™•ì¸
        if not self.rate_limiter.check():
            return APIResponse(
                status=ResponseStatus.RATE_LIMITED,
                error="Rate limit exceeded"
            )
        
        try:
            # API í˜¸ì¶œ
            response = await self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": "You are an expert assistant."},
                    {"role": "user", "content": prompt}
                ],
                temperature=kwargs.get('temperature', 0.7),
                max_tokens=kwargs.get('max_tokens', 4096)
            )
            
            # Rate limit ê¸°ë¡
            self.rate_limiter.record()
            
            # ì‘ë‹µ ì²˜ë¦¬
            result = response.choices[0].message.content
            
            # ìºì‹œ ì €ì¥
            self.cache.set(self.engine_type.value, cache_params, result)
            
            # ì‚¬ìš©ëŸ‰ ê¸°ë¡
            if hasattr(response, 'usage'):
                tokens = response.usage.total_tokens
                cost = self.estimate_cost(
                    response.usage.prompt_tokens,
                    response.usage.completion_tokens
                )
                # ê¸°ë¡...
            
            return APIResponse(
                status=ResponseStatus.SUCCESS,
                data=result,
                metadata={'model': self.model_name}
            )
            
        except Exception as e:
            logger.error(f"{self.engine_type.value} ì—ëŸ¬: {str(e)}")
            return APIResponse(
                status=ResponseStatus.ERROR,
                error=str(e)
            )

# ===========================================================================
# ğŸ”¬ ê³¼í•™ ë°ì´í„°ë² ì´ìŠ¤ í´ë¼ì´ì–¸íŠ¸
# ===========================================================================

class ScienceDBClient:
    """ê³¼í•™ ë°ì´í„°ë² ì´ìŠ¤ í†µí•© í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self):
        self.clients = {}
        self.cache = APICache(CACHE_TTL.get('material_data', 86400))
        self._init_clients()
    
    def _init_clients(self):
        """í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        # Materials Project
        if MATERIALS_PROJECT_AVAILABLE:
            mp_key = self._get_api_key('materials_project')
            if mp_key:
                self.clients['materials_project'] = MPRester(mp_key)
        
        # PubChem (API í‚¤ ë¶ˆí•„ìš”)
        if PUBCHEM_AVAILABLE:
            self.clients['pubchem'] = pcp
        
        # GitHub
        if GITHUB_AVAILABLE:
            github_token = self._get_api_key('github')
            if github_token:
                self.clients['github'] = Github(github_token)
    
    def _get_api_key(self, service: str) -> Optional[str]:
        """API í‚¤ ê°€ì ¸ì˜¤ê¸°"""
        # ì„¸ì…˜, secrets, í™˜ê²½ë³€ìˆ˜ ìˆœìœ¼ë¡œ í™•ì¸
        if hasattr(st, 'session_state') and 'api_keys' in st.session_state:
            key = st.session_state.api_keys.get(service)
            if key:
                return key
        
        if hasattr(st, 'secrets'):
            try:
                key = st.secrets.get(f"{service}_api_key")
                if key:
                    return key
            except:
                pass
        
        return os.getenv(f"{service.upper()}_API_KEY")
    
    async def search_materials(self, formula: Optional[str] = None,
                             **criteria) -> List[Dict]:
        """ì¬ë£Œ ê²€ìƒ‰"""
        if 'materials_project' not in self.clients:
            return []
        
        # ìºì‹œ í™•ì¸
        cache_params = {'formula': formula, **criteria}
        cached = self.cache.get('materials', cache_params)
        if cached:
            return cached
        
        try:
            mp = self.clients['materials_project']
            # ë™ê¸° í˜¸ì¶œì„ ë¹„ë™ê¸°ë¡œ ë³€í™˜
            results = await asyncio.to_thread(
                mp.materials.search,
                formula=formula,
                **criteria
            )
            
            # ê²°ê³¼ ë³€í™˜
            materials = []
            for material in results:
                materials.append({
                    'material_id': material.material_id,
                    'formula': material.formula,
                    'energy': material.energy_per_atom,
                    'band_gap': material.band_gap,
                    'density': material.density,
                    'crystal_system': material.symmetry.crystal_system
                })
            
            # ìºì‹œ ì €ì¥
            self.cache.set('materials', cache_params, materials)
            
            return materials
            
        except Exception as e:
            logger.error(f"Materials Project ê²€ìƒ‰ ì—ëŸ¬: {str(e)}")
            return []
    
    async def search_compounds(self, name: Optional[str] = None,
                             formula: Optional[str] = None) -> List[Dict]:
        """í™”í•©ë¬¼ ê²€ìƒ‰"""
        if 'pubchem' not in self.clients:
            return []
        
        # ìºì‹œ í™•ì¸
        cache_params = {'name': name, 'formula': formula}
        cached = self.cache.get('compounds', cache_params)
        if cached:
            return cached
        
        try:
            compounds = []
            
            if name:
                # ì´ë¦„ìœ¼ë¡œ ê²€ìƒ‰
                results = await asyncio.to_thread(
                    pcp.get_compounds, name, 'name'
                )
            elif formula:
                # ë¶„ìì‹ìœ¼ë¡œ ê²€ìƒ‰
                results = await asyncio.to_thread(
                    pcp.get_compounds, formula, 'formula'
                )
            else:
                return []
            
            # ê²°ê³¼ ë³€í™˜
            for compound in results[:10]:  # ìµœëŒ€ 10ê°œ
                compounds.append({
                    'cid': compound.cid,
                    'iupac_name': compound.iupac_name,
                    'molecular_formula': compound.molecular_formula,
                    'molecular_weight': compound.molecular_weight,
                    'smiles': compound.canonical_smiles,
                    'synonyms': compound.synonyms[:5] if compound.synonyms else []
                })
            
            # ìºì‹œ ì €ì¥
            self.cache.set('compounds', cache_params, compounds)
            
            return compounds
            
        except Exception as e:
            logger.error(f"PubChem ê²€ìƒ‰ ì—ëŸ¬: {str(e)}")
            return []

# ===========================================================================
# ğŸ¯ ë©”ì¸ API ê´€ë¦¬ì
# ===========================================================================

class APIManager:
    """í†µí•© API ê´€ë¦¬ì"""
    
    def __init__(self):
        self.encryption_manager = EncryptionManager()
        self.ai_engines: Dict[str, BaseAIEngine] = {}
        self.db_client = ScienceDBClient()
        self.usage_tracker = UsageTracker()
        self._init_engines()
        
        logger.info("API Manager ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _init_engines(self):
        """AI ì—”ì§„ ì´ˆê¸°í™”"""
        # Gemini
        if GEMINI_AVAILABLE:
            try:
                engine = GeminiEngine()
                if engine.is_available:
                    self.ai_engines['gemini'] = engine
                    logger.info("Gemini ì—”ì§„ ì´ˆê¸°í™” ì„±ê³µ")
            except Exception as e:
                logger.error(f"Gemini ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        # Groq
        if GROQ_AVAILABLE:
            try:
                engine = GroqEngine()
                if engine.is_available:
                    self.ai_engines['groq'] = engine
                    logger.info("Groq ì—”ì§„ ì´ˆê¸°í™” ì„±ê³µ")
            except Exception as e:
                logger.error(f"Groq ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        # HuggingFace
        if HUGGINGFACE_AVAILABLE:
            try:
                engine = HuggingFaceEngine()
                if engine.is_available:
                    self.ai_engines['huggingface'] = engine
                    logger.info("HuggingFace ì—”ì§„ ì´ˆê¸°í™” ì„±ê³µ")
            except Exception as e:
                logger.error(f"HuggingFace ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        # OpenAI í˜¸í™˜ ì—”ì§„ë“¤ (ë‚˜ì¤‘ì— API ì œê³µì‹œ í™œì„±í™”)
        # self.ai_engines['grok'] = OpenAICompatibleEngine(
        #     AIEngineType.GROK, "grok-beta", "https://api.x.ai/v1"
        # )
    
    def get_available_engines(self) -> List[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ AI ì—”ì§„ ëª©ë¡"""
        return list(self.ai_engines.keys())
    
    def set_api_key(self, service: str, api_key: str) -> bool:
        """API í‚¤ ì„¤ì •"""
        try:
            # ì„¸ì…˜ ìƒíƒœì— ì €ì¥
            if 'api_keys' not in st.session_state:
                st.session_state.api_keys = {}
            
            st.session_state.api_keys[service] = api_key
            
            # ì—”ì§„ ì¬ì´ˆê¸°í™”
            self._init_engines()
            
            return True
        except Exception as e:
            logger.error(f"API í‚¤ ì„¤ì • ì‹¤íŒ¨: {e}")
            return False
    
    async def generate_text(self, engine_id: str, prompt: str,
                          user_id: str = "anonymous", **kwargs) -> APIResponse:
        """AI í…ìŠ¤íŠ¸ ìƒì„±"""
        # ì˜¤í”„ë¼ì¸ ì²´í¬
        if not self._check_connection():
            # ìºì‹œ í™•ì¸
            for engine in self.ai_engines.values():
                cache_params = {'prompt': prompt, **kwargs}
                cached = engine.cache.get(engine_id, cache_params)
                if cached:
                    return APIResponse(
                        status=ResponseStatus.OFFLINE,
                        data=cached,
                        cached=True,
                        metadata={'offline_mode': True}
                    )
            
            return APIResponse(
                status=ResponseStatus.OFFLINE,
                error="ì˜¤í”„ë¼ì¸ ëª¨ë“œ: ìºì‹œëœ ì‘ë‹µì´ ì—†ìŠµë‹ˆë‹¤"
            )
        
        # ì—”ì§„ í™•ì¸
        if engine_id not in self.ai_engines:
            # í´ë°±: ì‚¬ìš© ê°€ëŠ¥í•œ ì²« ë²ˆì§¸ ì—”ì§„ ì‚¬ìš©
            if self.ai_engines:
                engine_id = list(self.ai_engines.keys())[0]
                logger.warning(f"ìš”ì²­ëœ ì—”ì§„ ì—†ìŒ, {engine_id}ë¡œ í´ë°±")
            else:
                return APIResponse(
                    status=ResponseStatus.ERROR,
                    error="ì‚¬ìš© ê°€ëŠ¥í•œ AI ì—”ì§„ì´ ì—†ìŠµë‹ˆë‹¤"
                )
        
        engine = self.ai_engines[engine_id]
        
        # ìƒì„± ìš”ì²­
        response = await engine.generate(prompt, user_id, **kwargs)
        
        # ì‚¬ìš©ëŸ‰ ê¸°ë¡
        if response.status == ResponseStatus.SUCCESS:
            record = UsageRecord(
                user_id=user_id,
                api_type=engine_id,
                timestamp=datetime.now(),
                tokens_used=kwargs.get('tokens', 0),
                cost=0.0,  # ì¶”í›„ ê³„ì‚°
                success=True
            )
            self.usage_tracker.record(record)
        
        return response
    
    async def extract_protocol(self, text: str, file_type: str = "mixed_format",
                             user_id: str = "anonymous") -> APIResponse:
        """í”„ë¡œí† ì½œ ì¶”ì¶œ"""
        # Gemini ìš°ì„  ì‚¬ìš© (í”„ë¡œí† ì½œ ì¶”ì¶œì— ìµœì í™”)
        if 'gemini' in self.ai_engines:
            engine = self.ai_engines['gemini']
            if hasattr(engine, 'extract_protocol'):
                return await engine.extract_protocol(text, file_type, user_id)
        
        # ë‹¤ë¥¸ ì—”ì§„ìœ¼ë¡œ í´ë°±
        prompt = f"""
{EXTRACTION_PROMPTS.get(file_type, EXTRACTION_PROMPTS['mixed_format'])}

í…ìŠ¤íŠ¸:
{text[:10000]}

JSON í˜•ì‹ìœ¼ë¡œ í”„ë¡œí† ì½œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.
"""
        
        return await self.generate_text(
            list(self.ai_engines.keys())[0] if self.ai_engines else 'gemini',
            prompt,
            user_id
        )
    
    async def analyze_experiment(self, engine_id: str, experiment_data: Dict,
                               user_id: str = "anonymous") -> APIResponse:
        """ì‹¤í—˜ ë°ì´í„° ë¶„ì„"""
        prompt = f"""
ë‹¤ìŒ ì‹¤í—˜ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•˜ì„¸ìš”:

ì‹¤í—˜ ì •ë³´:
{json.dumps(experiment_data, indent=2, ensure_ascii=False)}

ë‹¤ìŒ í•­ëª©ë“¤ì„ í¬í•¨í•˜ì—¬ ë¶„ì„í•˜ì„¸ìš”:
1. ì£¼ìš” ë°œê²¬ì‚¬í•­
2. í†µê³„ì  ìœ ì˜ì„±
3. ê°œì„  ì œì•ˆ
4. ë‹¤ìŒ ì‹¤í—˜ ì¶”ì²œ
"""
        
        return await self.generate_text(engine_id, prompt, user_id)
    
    def get_usage_summary(self, user_id: Optional[str] = None,
                         period: str = 'day') -> Dict[str, Any]:
        """ì‚¬ìš©ëŸ‰ ìš”ì•½"""
        return self.usage_tracker.get_summary(user_id, period)
    
    def get_api_status(self) -> Dict[str, Any]:
        """API ìƒíƒœ í™•ì¸"""
        status = {
            'ai_engines': {},
            'databases': {},
            'total_available': 0
        }
        
        # AI ì—”ì§„ ìƒíƒœ
        for engine_id, engine in self.ai_engines.items():
            status['ai_engines'][engine_id] = {
                'available': engine.is_available,
                'model': engine.model_name,
                'rate_limit': engine.rate_limiter.get_remaining()
            }
            if engine.is_available:
                status['total_available'] += 1
        
        # ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ
        for db_name, client in self.db_client.clients.items():
            status['databases'][db_name] = {
                'available': client is not None
            }
        
        return status
    
    def _check_connection(self) -> bool:
        """ì¸í„°ë„· ì—°ê²° í™•ì¸"""
        try:
            import requests
            response = requests.get('https://www.google.com', timeout=3)
            return response.status_code == 200
        except:
            return False
    
    def clear_cache(self, cache_type: Optional[str] = None):
        """ìºì‹œ ì´ˆê¸°í™”"""
        for engine in self.ai_engines.values():
            engine.cache.clear(cache_type)
        
        self.db_client.cache.clear(cache_type)
        
        logger.info(f"ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ: {cache_type or 'ì „ì²´'}")

# ===========================================================================
# ğŸ”§ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
# ===========================================================================

_api_manager: Optional[APIManager] = None

def get_api_manager() -> APIManager:
    """API Manager ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _api_manager
    
    if _api_manager is None:
        _api_manager = APIManager()
    
    return _api_manager

# ===========================================================================
# ğŸ¯ í—¬í¼ í•¨ìˆ˜
# ===========================================================================

async def ask_ai(prompt: str, engine: str = "gemini",
                user_id: str = "anonymous", **kwargs) -> str:
    """ê°„í¸ AI ì§ˆë¬¸ í•¨ìˆ˜"""
    manager = get_api_manager()
    response = await manager.generate_text(engine, prompt, user_id, **kwargs)
    
    if response.status in [ResponseStatus.SUCCESS, ResponseStatus.CACHED]:
        return response.data
    else:
        return f"ì˜¤ë¥˜: {response.error}"

def get_available_ai_engines() -> List[str]:
    """ì‚¬ìš© ê°€ëŠ¥í•œ AI ì—”ì§„ ëª©ë¡"""
    manager = get_api_manager()
    return manager.get_available_engines()

# ===========================================================================
# ğŸ§ª í…ŒìŠ¤íŠ¸ ì½”ë“œ
# ===========================================================================

if __name__ == "__main__":
    import asyncio
    
    async def test_api_manager():
        """API Manager í…ŒìŠ¤íŠ¸"""
        manager = get_api_manager()
        
        # ìƒíƒœ í™•ì¸
        print("API ìƒíƒœ:", manager.get_api_status())
        
        # í…ìŠ¤íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸
        if manager.get_available_engines():
            engine = manager.get_available_engines()[0]
            response = await manager.generate_text(
                engine,
                "ê³ ë¶„ì í•©ì„±ì˜ ê¸°ë³¸ ì›ë¦¬ë¥¼ ì„¤ëª…í•˜ì„¸ìš”.",
                "test_user"
            )
            print(f"\n{engine} ì‘ë‹µ:", response.data[:200] if response.data else response.error)
        
        # í”„ë¡œí† ì½œ ì¶”ì¶œ í…ŒìŠ¤íŠ¸
        test_text = """
        Materials: Polymer A (98% purity, Sigma-Aldrich), Solvent B
        
        Procedure:
        1. Dissolve 5g of Polymer A in 100mL of Solvent B
        2. Heat to 80Â°C for 2 hours
        3. Cool to room temperature
        """
        
        protocol_response = await manager.extract_protocol(test_text)
        print("\ní”„ë¡œí† ì½œ ì¶”ì¶œ:", protocol_response.data)
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    asyncio.run(test_api_manager())
