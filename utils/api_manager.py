"""
ğŸ¤– Universal DOE Platform - API í†µí•© ê´€ë¦¬ì
================================================================================
6ê°œ AI ì—”ì§„ê³¼ ê³¼í•™ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ í†µí•© ê´€ë¦¬í•˜ëŠ” í•µì‹¬ ëª¨ë“ˆ
ì˜¤í”„ë¼ì¸ ìš°ì„  ì„¤ê³„, ìºì‹±, í´ë°± ë©”ì»¤ë‹ˆì¦˜ ì œê³µ
================================================================================
"""

import os
import json
import logging
import asyncio
import time
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple, Union, Callable
from dataclasses import dataclass, field, asdict
from collections import defaultdict, deque
from functools import wraps, lru_cache
from enum import Enum
import hashlib
import threading
from pathlib import Path
import base64
from cryptography.fernet import Fernet

# ì„œë“œíŒŒí‹° ë¼ì´ë¸ŒëŸ¬ë¦¬
import streamlit as st
import pandas as pd
import numpy as np
from tenacity import retry, stop_after_attempt, wait_exponential
import aiohttp
import httpx
import requests

# AI ë¼ì´ë¸ŒëŸ¬ë¦¬ - ì„ íƒì  import
try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False
    logging.warning("Google Gemini API ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

try:
    from openai import AsyncOpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    logging.warning("OpenAI API ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

try:
    from groq import AsyncGroq
    GROQ_AVAILABLE = True
except ImportError:
    GROQ_AVAILABLE = False
    logging.warning("Groq API ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

try:
    from transformers import pipeline
    import torch
    HUGGINGFACE_AVAILABLE = True
except ImportError:
    HUGGINGFACE_AVAILABLE = False
    logging.warning("HuggingFace ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

try:
    import anthropic
    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False
    logging.warning("Anthropic API ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

# ê³¼í•™ ë°ì´í„°ë² ì´ìŠ¤ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    from mp_api.client import MPRester
    MATERIALS_PROJECT_AVAILABLE = True
except ImportError:
    MATERIALS_PROJECT_AVAILABLE = False
    logging.warning("Materials Project API ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

try:
    import pubchempy as pcp
    PUBCHEM_AVAILABLE = True
except ImportError:
    PUBCHEM_AVAILABLE = False
    logging.warning("PubChemPy ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

try:
    from github import Github
    GITHUB_AVAILABLE = True
except ImportError:
    GITHUB_AVAILABLE = False
    logging.warning("PyGithub ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

try:
    from pyalex import Works, Authors, Institutions
    OPENALEX_AVAILABLE = True
except ImportError:
    OPENALEX_AVAILABLE = False
    logging.warning("PyAlex ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

try:
    from crossref.restful import Works as CrossrefWorks
    CROSSREF_AVAILABLE = True
except ImportError:
    CROSSREF_AVAILABLE = False
    logging.warning("Crossref ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

try:
    import arxiv
    ARXIV_AVAILABLE = True
except ImportError:
    ARXIV_AVAILABLE = False
    logging.warning("arXiv ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

try:
    from Bio import Entrez
    PUBMED_AVAILABLE = True
except ImportError:
    PUBMED_AVAILABLE = False
    logging.warning("Biopython ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

# ë¡œì»¬ ëª¨ë“ˆ ì„í¬íŠ¸
try:
    from config.app_config import (
        API_CONFIG, FILE_PROCESSING, PROTOCOL_EXTRACTION,
        get_config, LITERATURE_APIS, DATABASE_APIS
    )
    from config.secrets_config import API_KEY_STRUCTURE
    from config.error_config import ERROR_CODES, USER_FRIENDLY_MESSAGES
    from utils.error_handler import handle_api_error
except ImportError:
    # ê¸°ë³¸ê°’ ì„¤ì •
    API_CONFIG = {
        'google_gemini': {
            'name': 'Google Gemini 2.0 Flash',
            'model': 'gemini-2.0-flash-exp',
            'required': True,
            'free_tier': True,
            'rate_limit': 60,
            'max_tokens': 1048576
        },
        'xai_grok': {
            'name': 'xAI Grok',
            'model': 'grok-beta',
            'required': False,
            'free_tier': False,
            'rate_limit': 60
        },
        'groq': {
            'name': 'Groq',
            'model': 'llama-3.3-70b-versatile',
            'required': False,
            'free_tier': True,
            'rate_limit': 100
        },
        'sambanova': {
            'name': 'SambaNova',
            'model': 'llama-3.1-405b',
            'required': False,
            'free_tier': True,
            'rate_limit': 60
        },
        'deepseek': {
            'name': 'DeepSeek',
            'model': 'deepseek-chat',
            'required': False,
            'free_tier': False,
            'rate_limit': 60
        },
        'huggingface': {
            'name': 'HuggingFace',
            'model': 'microsoft/BioGPT-Large',
            'required': False,
            'free_tier': True,
            'rate_limit': 1000
        }
    }
    ERROR_CODES = {}
    USER_FRIENDLY_MESSAGES = {}

# ===========================================================================
# ğŸ”§ ë¡œê¹… ì„¤ì •
# ===========================================================================

logger = logging.getLogger(__name__)

# ===========================================================================
# ğŸ“Œ ìƒìˆ˜ ë° Enum ì •ì˜
# ===========================================================================

class AIEngineType(Enum):
    """AI ì—”ì§„ íƒ€ì…"""
    GEMINI = "gemini"
    GROK = "grok"
    GROQ = "groq"
    SAMBANOVA = "sambanova"
    DEEPSEEK = "deepseek"
    HUGGINGFACE = "huggingface"

class ResponseStatus(Enum):
    """API ì‘ë‹µ ìƒíƒœ"""
    SUCCESS = "success"
    ERROR = "error"
    RATE_LIMITED = "rate_limited"
    CACHED = "cached"
    OFFLINE = "offline"
    FALLBACK = "fallback"

class DatabaseType(Enum):
    """ë°ì´í„°ë² ì´ìŠ¤ íƒ€ì…"""
    MATERIALS_PROJECT = "materials_project"
    PUBCHEM = "pubchem"
    GITHUB = "github"
    OPENALEX = "openalex"
    CROSSREF = "crossref"
    ARXIV = "arxiv"
    PUBMED = "pubmed"
    ZENODO = "zenodo"
    PROTOCOLS_IO = "protocols_io"
    FIGSHARE = "figshare"

# Rate Limit ì„¤ì •
RATE_LIMITS = {
    AIEngineType.GEMINI: {'calls': 60, 'period': 60},  # 60 calls/min
    AIEngineType.GROK: {'calls': 60, 'period': 60},
    AIEngineType.GROQ: {'calls': 100, 'period': 60},   # 100 calls/min
    AIEngineType.SAMBANOVA: {'calls': 60, 'period': 60},
    AIEngineType.DEEPSEEK: {'calls': 60, 'period': 60},
    AIEngineType.HUGGINGFACE: {'calls': 1000, 'period': 3600},  # 1000 calls/hour
    # ë°ì´í„°ë² ì´ìŠ¤
    DatabaseType.OPENALEX: {'calls': 100, 'period': 60},  # Polite pool
    DatabaseType.CROSSREF: {'calls': 50, 'period': 60},
    DatabaseType.ARXIV: {'calls': 3, 'period': 10},  # 3ì´ˆ ê°„ê²©
    DatabaseType.PUBMED: {'calls': 10, 'period': 1},  # 10/sec
    'default': {'calls': 30, 'period': 60}
}

# ìºì‹œ TTL ì„¤ì • (ì´ˆ)
CACHE_TTL = {
    'ai_response': 3600,      # 1ì‹œê°„
    'material_data': 86400,   # 24ì‹œê°„
    'compound_data': 86400,   # 24ì‹œê°„
    'protocol': 7200,         # 2ì‹œê°„
    'literature': 21600,      # 6ì‹œê°„
    'benchmark': 43200,       # 12ì‹œê°„
    'default': 1800           # 30ë¶„
}

# API ë¹„ìš© ì¶”ì • (1K í† í°ë‹¹ USD)
API_COSTS = {
    AIEngineType.GEMINI: {'input': 0.0, 'output': 0.0},  # ë¬´ë£Œ
    AIEngineType.GROK: {'input': 0.005, 'output': 0.015},
    AIEngineType.GROQ: {'input': 0.0, 'output': 0.0},    # ë¬´ë£Œ
    AIEngineType.DEEPSEEK: {'input': 0.001, 'output': 0.002},
    AIEngineType.SAMBANOVA: {'input': 0.0, 'output': 0.0},  # ë¬´ë£Œ í‹°ì–´
    AIEngineType.HUGGINGFACE: {'input': 0.0, 'output': 0.0},  # ë¬´ë£Œ
    'default': {'input': 0.001, 'output': 0.002}
}

# ===========================================================================
# ğŸ”§ í”„ë¡œí† ì½œ ì¶”ì¶œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
# ===========================================================================

EXTRACTION_PROMPTS = {
    'pdf_academic': """
í•™ìˆ  ë…¼ë¬¸ PDFì—ì„œ ì‹¤í—˜ í”„ë¡œí† ì½œì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
Methods/Experimental/Materials and Methods ì„¹ì…˜ì„ ì¤‘ì ì ìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”.

ì¶”ì¶œí•´ì•¼ í•  ì •ë³´:
1. ì¬ë£Œ ë° ì‹œì•½ (ì´ë¦„, ìˆœë„, ê³µê¸‰ì—…ì²´)
2. ì¥ë¹„ ë° ë„êµ¬ (ëª¨ë¸ëª…, ì œì¡°ì‚¬)
3. ì‹¤í—˜ ì¡°ê±´ (ì˜¨ë„, ì••ë ¥, ì‹œê°„, pH ë“±)
4. ì‹¤í—˜ ì ˆì°¨ (ë‹¨ê³„ë³„ ìƒì„¸ ì„¤ëª…)
5. ì£¼ì˜ì‚¬í•­ ë° ì•ˆì „ ì •ë³´

JSON í˜•ì‹ìœ¼ë¡œ êµ¬ì¡°í™”í•˜ì—¬ ë°˜í™˜í•˜ì„¸ìš”.
""",
    
    'text_protocol': """
ì¼ë°˜ í…ìŠ¤íŠ¸ì—ì„œ ì‹¤í—˜ ì ˆì°¨ë¥¼ ì‹ë³„í•©ë‹ˆë‹¤.
ë²ˆí˜¸ë‚˜ ë¶ˆë¦¿ìœ¼ë¡œ êµ¬ë¶„ëœ ë‹¨ê³„ë¥¼ ì°¾ê³ , ì¬ë£Œì™€ ì¡°ê±´ì„ êµ¬ë¶„í•˜ì„¸ìš”.

ì¶”ì¶œ í¬ë§·:
{
  "materials": [...],
  "equipment": [...],
  "conditions": {...},
  "procedure": [...],
  "safety": [...]
}
""",
    
    'html_webpage': """
ì›¹í˜ì´ì§€ì—ì„œ í”„ë¡œí† ì½œ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
êµ¬ì¡°í™”ëœ ë¦¬ìŠ¤íŠ¸, í…Œì´ë¸”, ë˜ëŠ” ë‹¨ê³„ë³„ ì„¤ëª…ì„ ì°¾ìœ¼ì„¸ìš”.
ë„¤ë¹„ê²Œì´ì…˜ì´ë‚˜ ê´‘ê³  ê°™ì€ ë¬´ê´€í•œ ë‚´ìš©ì€ ì œì™¸í•˜ì„¸ìš”.
""",
    
    'polymer_specific': """
ê³ ë¶„ì ì‹¤í—˜ í”„ë¡œí† ì½œì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
íŠ¹íˆ ë‹¤ìŒ ì‚¬í•­ì— ì£¼ì˜í•˜ì„¸ìš”:
- ìš©ë§¤ ì‹œìŠ¤í…œ (ë‹¨ì¼/ì´ì„±ë¶„/ì‚¼ì„±ë¶„)
- ê³ ë¶„ì ë†ë„ ë° ë¶„ìëŸ‰
- ê°€ê³µ ì¡°ê±´ (ì˜¨ë„, ì••ë ¥, ì‹œê°„)
- íŠ¹ìˆ˜ ì¥ë¹„ (ì „ê¸°ë°©ì‚¬, ì••ì¶œê¸° ë“±)
""",
    
    'mixed_format': """
ë‹¤ì–‘í•œ í˜•ì‹ì´ í˜¼ì¬ëœ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
ì¬ë£Œ, ì¡°ê±´, ì ˆì°¨ë¥¼ êµ¬ë¶„í•˜ì—¬ ì¶”ì¶œí•˜ê³ , ë…¼ë¦¬ì  ìˆœì„œë¡œ ì •ë¦¬í•˜ì„¸ìš”.
ìˆ˜ëŸ‰, ë‹¨ìœ„, ì‹œê°„ ì •ë³´ë¥¼ ì •í™•íˆ íŒŒì•…í•˜ì„¸ìš”.
"""
}

# ===========================================================================
# ğŸ“Š ë°ì´í„° í´ë˜ìŠ¤
# ===========================================================================

@dataclass
class APIResponse:
    """API ì‘ë‹µ ë°ì´í„° í´ë˜ìŠ¤"""
    status: ResponseStatus
    data: Optional[Any] = None
    error: Optional[str] = None
    error_code: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)
    cached: bool = False
    timestamp: datetime = field(default_factory=datetime.now)
    fallback_used: Optional[str] = None

@dataclass
class UsageRecord:
    """API ì‚¬ìš©ëŸ‰ ê¸°ë¡"""
    user_id: str
    api_type: str
    timestamp: datetime
    tokens_used: Optional[int] = None
    cost: Optional[float] = None
    success: bool = True
    error: Optional[str] = None
    response_time: Optional[float] = None

@dataclass
class ProtocolData:
    """ì¶”ì¶œëœ í”„ë¡œí† ì½œ ë°ì´í„°"""
    materials: List[Dict[str, Any]]
    equipment: List[Dict[str, Any]]
    conditions: Dict[str, Any]
    procedure: List[Dict[str, Any]]
    safety: List[str]
    metadata: Dict[str, Any] = field(default_factory=dict)
    confidence_score: float = 0.0
    source: Optional[str] = None

# ===========================================================================
# ğŸ” ì•”í˜¸í™” ê´€ë¦¬ì
# ===========================================================================

class EncryptionManager:
    """API í‚¤ ì•”í˜¸í™” ê´€ë¦¬"""
    
    def __init__(self):
        self.key = self._get_or_create_key()
        self.cipher = Fernet(self.key)
    
    def _get_or_create_key(self) -> bytes:
        """ì•”í˜¸í™” í‚¤ ê°€ì ¸ì˜¤ê¸° ë˜ëŠ” ìƒì„±"""
        key_file = Path.home() / '.universaldoe' / 'api_key.key'
        key_file.parent.mkdir(parents=True, exist_ok=True)
        
        if key_file.exists():
            return key_file.read_bytes()
        else:
            key = Fernet.generate_key()
            key_file.write_bytes(key)
            return key
    
    def encrypt(self, value: str) -> str:
        """ë¬¸ìì—´ ì•”í˜¸í™”"""
        return self.cipher.encrypt(value.encode()).decode()
    
    def decrypt(self, encrypted: str) -> str:
        """ë¬¸ìì—´ ë³µí˜¸í™”"""
        return self.cipher.decrypt(encrypted.encode()).decode()

# ===========================================================================
# â±ï¸ Rate Limiter
# ===========================================================================

class RateLimiter:
    """API Rate Limiting"""
    
    def __init__(self, max_calls: int, period: int):
        self.max_calls = max_calls
        self.period = period  # ì´ˆ
        self.calls = deque()
        self.lock = threading.Lock()
    
    def check(self) -> bool:
        """í˜¸ì¶œ ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        with self.lock:
            now = time.time()
            # ê¸°ê°„ì´ ì§€ë‚œ í˜¸ì¶œ ì œê±°
            while self.calls and self.calls[0] < now - self.period:
                self.calls.popleft()
            
            return len(self.calls) < self.max_calls
    
    def record(self):
        """í˜¸ì¶œ ê¸°ë¡"""
        with self.lock:
            self.calls.append(time.time())
    
    def get_remaining(self) -> Dict[str, Any]:
        """ë‚¨ì€ í˜¸ì¶œ ìˆ˜ ë°˜í™˜"""
        with self.lock:
            now = time.time()
            while self.calls and self.calls[0] < now - self.period:
                self.calls.popleft()
            
            return {
                'remaining': self.max_calls - len(self.calls),
                'reset_in': int(self.period - (now - self.calls[0])) if self.calls else 0,
                'total': self.max_calls
            }
    
    def wait_if_needed(self) -> float:
        """í•„ìš”ì‹œ ëŒ€ê¸° ì‹œê°„ ë°˜í™˜"""
        with self.lock:
            if not self.check():
                now = time.time()
                oldest_call = self.calls[0]
                wait_time = self.period - (now - oldest_call) + 0.1
                return wait_time
            return 0

# ===========================================================================
# ğŸ’¾ ìºì‹œ ì‹œìŠ¤í…œ
# ===========================================================================

class APICache:
    """API ì‘ë‹µ ìºì‹±"""
    
    def __init__(self, ttl: int = 3600):
        self.ttl = ttl
        self.cache: Dict[str, Tuple[Any, float]] = {}
        self.lock = threading.Lock()
        self.stats = {'hits': 0, 'misses': 0, 'evictions': 0}
    
    def _make_key(self, prefix: str, params: Dict) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        param_str = json.dumps(params, sort_keys=True)
        return f"{prefix}:{hashlib.md5(param_str.encode()).hexdigest()}"
    
    def get(self, prefix: str, params: Dict) -> Optional[Any]:
        """ìºì‹œì—ì„œ ê°€ì ¸ì˜¤ê¸°"""
        with self.lock:
            key = self._make_key(prefix, params)
            if key in self.cache:
                data, timestamp = self.cache[key]
                if time.time() - timestamp < self.ttl:
                    self.stats['hits'] += 1
                    return data
                else:
                    del self.cache[key]
                    self.stats['evictions'] += 1
            self.stats['misses'] += 1
            return None
    
    def set(self, prefix: str, params: Dict, data: Any):
        """ìºì‹œì— ì €ì¥"""
        with self.lock:
            key = self._make_key(prefix, params)
            self.cache[key] = (data, time.time())
    
    def clear(self, prefix: Optional[str] = None):
        """ìºì‹œ ì´ˆê¸°í™”"""
        with self.lock:
            if prefix:
                keys_to_remove = [k for k in self.cache.keys() if k.startswith(prefix)]
                for key in keys_to_remove:
                    del self.cache[key]
            else:
                self.cache.clear()
    
    def get_stats(self) -> Dict[str, int]:
        """ìºì‹œ í†µê³„"""
        with self.lock:
            return self.stats.copy()

# ===========================================================================
# ğŸ“Š ì‚¬ìš©ëŸ‰ ì¶”ì 
# ===========================================================================

class UsageTracker:
    """API ì‚¬ìš©ëŸ‰ ì¶”ì """
    
    def __init__(self):
        self.records: List[UsageRecord] = []
        self.lock = threading.Lock()
    
    def record(self, record: UsageRecord):
        """ì‚¬ìš©ëŸ‰ ê¸°ë¡"""
        with self.lock:
            self.records.append(record)
            # 7ì¼ ì´ìƒ ëœ ê¸°ë¡ ì œê±°
            cutoff = datetime.now() - timedelta(days=7)
            self.records = [r for r in self.records if r.timestamp > cutoff]
    
    def get_summary(self, user_id: Optional[str] = None, 
                   period: str = 'day') -> Dict[str, Any]:
        """ì‚¬ìš©ëŸ‰ ìš”ì•½"""
        with self.lock:
            # ê¸°ê°„ ì„¤ì •
            if period == 'day':
                start = datetime.now() - timedelta(days=1)
            elif period == 'week':
                start = datetime.now() - timedelta(days=7)
            elif period == 'month':
                start = datetime.now() - timedelta(days=30)
            else:
                start = datetime.min
            
            # í•„í„°ë§
            filtered = [r for r in self.records if r.timestamp >= start]
            if user_id:
                filtered = [r for r in filtered if r.user_id == user_id]
            
            # ì§‘ê³„
            summary = defaultdict(lambda: {
                'calls': 0,
                'tokens': 0,
                'cost': 0.0,
                'errors': 0,
                'avg_response_time': 0.0
            })
            
            response_times = defaultdict(list)
            
            for record in filtered:
                api_summary = summary[record.api_type]
                api_summary['calls'] += 1
                api_summary['tokens'] += record.tokens_used or 0
                api_summary['cost'] += record.cost or 0
                if not record.success:
                    api_summary['errors'] += 1
                if record.response_time:
                    response_times[record.api_type].append(record.response_time)
            
            # í‰ê·  ì‘ë‹µ ì‹œê°„ ê³„ì‚°
            for api_type, times in response_times.items():
                if times:
                    summary[api_type]['avg_response_time'] = sum(times) / len(times)
            
            return dict(summary)

# ===========================================================================
# ğŸ¤– AI ì—”ì§„ ê¸°ë³¸ í´ë˜ìŠ¤
# ===========================================================================

class BaseAIEngine:
    """AI ì—”ì§„ ì¶”ìƒ í´ë˜ìŠ¤"""
    
    def __init__(self, engine_type: AIEngineType, model_name: str):
        self.engine_type = engine_type
        self.model_name = model_name
        self.api_key = self._get_api_key()
        self.is_available = self._check_availability()
        
        # Rate Limiter
        limits = RATE_LIMITS.get(engine_type, RATE_LIMITS['default'])
        self.rate_limiter = RateLimiter(limits['calls'], limits['period'])
        
        # Cache
        ttl = CACHE_TTL.get('ai_response', CACHE_TTL['default'])
        self.cache = APICache(ttl)
        
        # ë¹„ìš© ì •ë³´
        self.costs = API_COSTS.get(engine_type, API_COSTS['default'])
    
    def _get_api_key(self) -> Optional[str]:
        """API í‚¤ ê°€ì ¸ì˜¤ê¸°"""
        # 1. ì„¸ì…˜ ìƒíƒœ í™•ì¸
        if hasattr(st, 'session_state') and 'api_keys' in st.session_state:
            key = st.session_state.api_keys.get(self.engine_type.value)
            if key:
                return key
        
        # 2. Streamlit secrets í™•ì¸
        if hasattr(st, 'secrets'):
            try:
                key = st.secrets.get(f"{self.engine_type.value}_api_key")
                if key:
                    return key
            except:
                pass
        
        # 3. í™˜ê²½ ë³€ìˆ˜ í™•ì¸
        env_key = f"{self.engine_type.value.upper()}_API_KEY"
        return os.getenv(env_key)
    
    def _check_availability(self) -> bool:
        """ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        return bool(self.api_key)
    
    async def generate(self, prompt: str, user_id: str = "anonymous", 
                      **kwargs) -> APIResponse:
        """í…ìŠ¤íŠ¸ ìƒì„± (êµ¬í˜„ í•„ìš”)"""
        raise NotImplementedError
    
    def estimate_cost(self, input_tokens: int, output_tokens: int) -> float:
        """ë¹„ìš© ì¶”ì •"""
        input_cost = (input_tokens / 1000) * self.costs['input']
        output_cost = (output_tokens / 1000) * self.costs['output']
        return input_cost + output_cost
    
    def estimate_tokens(self, text: str) -> int:
        """í† í° ìˆ˜ ì¶”ì • (ëŒ€ëµì )"""
        # í•œê¸€ì€ í‰ê·  2-3í† í°, ì˜ì–´ëŠ” í‰ê·  1.3í† í° per word
        korean_chars = len([c for c in text if ord(c) >= 0xAC00 and ord(c) <= 0xD7AF])
        english_words = len(text.split()) - korean_chars // 3
        return int(korean_chars * 2.5 + english_words * 1.3)

# ===========================================================================
# ğŸŒŸ Google Gemini ì—”ì§„
# ===========================================================================

class GeminiEngine(BaseAIEngine):
    """Google Gemini AI ì—”ì§„"""
    
    def __init__(self):
        super().__init__(AIEngineType.GEMINI, "gemini-2.0-flash-exp")
        if self.is_available and GEMINI_AVAILABLE:
            genai.configure(api_key=self.api_key)
            self.model = genai.GenerativeModel(self.model_name)
            self.chat = None
    
    async def generate(self, prompt: str, user_id: str = "anonymous", 
                      **kwargs) -> APIResponse:
        """Geminië¡œ í…ìŠ¤íŠ¸ ìƒì„±"""
        start_time = time.time()
        
        # ìºì‹œ í™•ì¸
        cache_params = {'prompt': prompt, **kwargs}
        cached = self.cache.get('gemini', cache_params)
        if cached:
            return APIResponse(
                status=ResponseStatus.CACHED,
                data=cached,
                cached=True,
                metadata={'response_time': 0.0}
            )
        
        # Rate limit í™•ì¸
        wait_time = self.rate_limiter.wait_if_needed()
        if wait_time > 0:
            await asyncio.sleep(wait_time)
        
        try:
            # ë™ê¸° í˜¸ì¶œì„ ë¹„ë™ê¸°ë¡œ ë³€í™˜
            response = await asyncio.to_thread(
                self.model.generate_content, prompt
            )
            
            # Rate limit ê¸°ë¡
            self.rate_limiter.record()
            
            # ì‘ë‹µ ì²˜ë¦¬
            result = response.text
            response_time = time.time() - start_time
            
            # ìºì‹œ ì €ì¥
            self.cache.set('gemini', cache_params, result)
            
            # ì‚¬ìš©ëŸ‰ ê¸°ë¡
            estimated_tokens = self.estimate_tokens(prompt + result)
            record = UsageRecord(
                user_id=user_id,
                api_type='gemini',
                timestamp=datetime.now(),
                tokens_used=estimated_tokens,
                cost=0.0,  # ë¬´ë£Œ
                success=True,
                response_time=response_time
            )
            
            return APIResponse(
                status=ResponseStatus.SUCCESS,
                data=result,
                metadata={
                    'model': self.model_name,
                    'response_time': response_time,
                    'tokens': estimated_tokens
                }
            )
            
        except Exception as e:
            logger.error(f"Gemini ì—ëŸ¬: {str(e)}")
            return APIResponse(
                status=ResponseStatus.ERROR,
                error=str(e),
                error_code='3001'  # AI API ì—ëŸ¬
            )
    
    async def extract_protocol(self, text: str, file_type: str = "mixed_format",
                             user_id: str = "anonymous") -> APIResponse:
        """í”„ë¡œí† ì½œ ì¶”ì¶œ íŠ¹í™” ê¸°ëŠ¥"""
        # íŒŒì¼ íƒ€ì…ë³„ í”„ë¡¬í”„íŠ¸ ì„ íƒ
        base_prompt = EXTRACTION_PROMPTS.get(file_type, EXTRACTION_PROMPTS['mixed_format'])
        
        # ì „ì²´ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        full_prompt = f"""
{base_prompt}

í…ìŠ¤íŠ¸:
{text[:10000]}  # ê¸¸ì´ ì œí•œ

ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:
{{
  "materials": [
    {{"name": "ì¬ë£Œëª…", "amount": "ì–‘", "purity": "ìˆœë„", "supplier": "ê³µê¸‰ì—…ì²´"}}
  ],
  "equipment": [
    {{"name": "ì¥ë¹„ëª…", "model": "ëª¨ë¸", "manufacturer": "ì œì¡°ì‚¬"}}
  ],
  "conditions": {{
    "temperature": "ì˜¨ë„",
    "pressure": "ì••ë ¥",
    "time": "ì‹œê°„",
    "ph": "pH",
    "other": {{}}
  }},
  "procedure": [
    {{"step": 1, "action": "ë™ì‘", "details": "ìƒì„¸ ì„¤ëª…", "duration": "ì†Œìš” ì‹œê°„"}}
  ],
  "safety": ["ì£¼ì˜ì‚¬í•­1", "ì£¼ì˜ì‚¬í•­2"]
}}
"""
        
        response = await self.generate(full_prompt, user_id)
        
        if response.status == ResponseStatus.SUCCESS:
            try:
                # JSON íŒŒì‹±
                protocol_data = json.loads(response.data)
                response.data = ProtocolData(
                    materials=protocol_data.get('materials', []),
                    equipment=protocol_data.get('equipment', []),
                    conditions=protocol_data.get('conditions', {}),
                    procedure=protocol_data.get('procedure', []),
                    safety=protocol_data.get('safety', []),
                    confidence_score=0.9,
                    source='gemini'
                )
            except json.JSONDecodeError:
                # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ í…ìŠ¤íŠ¸ ê·¸ëŒ€ë¡œ ë°˜í™˜
                logger.warning("í”„ë¡œí† ì½œ JSON íŒŒì‹± ì‹¤íŒ¨")
                response.error_code = '4202'  # í”„ë¡œí† ì½œ ì¶”ì¶œ ì‹¤íŒ¨
        
        return response

# ===========================================================================
# âš¡ Groq ì—”ì§„
# ===========================================================================

class GroqEngine(BaseAIEngine):
    """Groq ì´ˆê³ ì† ì¶”ë¡  ì—”ì§„"""
    
    def __init__(self):
        super().__init__(AIEngineType.GROQ, "llama-3.3-70b-versatile")
        if self.is_available and GROQ_AVAILABLE:
            self.client = AsyncGroq(api_key=self.api_key)
    
    async def generate(self, prompt: str, user_id: str = "anonymous", 
                      **kwargs) -> APIResponse:
        """Groqë¡œ í…ìŠ¤íŠ¸ ìƒì„±"""
        start_time = time.time()
        
        # ìºì‹œ í™•ì¸
        cache_params = {'prompt': prompt, **kwargs}
        cached = self.cache.get('groq', cache_params)
        if cached:
            return APIResponse(
                status=ResponseStatus.CACHED,
                data=cached,
                cached=True
            )
        
        # Rate limit í™•ì¸
        wait_time = self.rate_limiter.wait_if_needed()
        if wait_time > 0:
            await asyncio.sleep(wait_time)
        
        try:
            # API í˜¸ì¶œ
            response = await self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": "You are an expert in materials science and chemistry."},
                    {"role": "user", "content": prompt}
                ],
                temperature=kwargs.get('temperature', 0.7),
                max_tokens=kwargs.get('max_tokens', 4096)
            )
            
            # Rate limit ê¸°ë¡
            self.rate_limiter.record()
            
            # ì‘ë‹µ ì²˜ë¦¬
            result = response.choices[0].message.content
            response_time = time.time() - start_time
            
            # ìºì‹œ ì €ì¥
            self.cache.set('groq', cache_params, result)
            
            # ì‚¬ìš©ëŸ‰ ê¸°ë¡
            tokens = response.usage.total_tokens if hasattr(response, 'usage') else 0
            
            return APIResponse(
                status=ResponseStatus.SUCCESS,
                data=result,
                metadata={
                    'model': self.model_name,
                    'response_time': response_time,
                    'tokens': tokens
                }
            )
            
        except Exception as e:
            logger.error(f"Groq ì—ëŸ¬: {str(e)}")
            return APIResponse(
                status=ResponseStatus.ERROR,
                error=str(e),
                error_code='3002'
            )

# ===========================================================================
# ğŸ¤— HuggingFace ì—”ì§„
# ===========================================================================

class HuggingFaceEngine(BaseAIEngine):
    """HuggingFace íŠ¹ìˆ˜ ëª¨ë¸ ì—”ì§„"""
    
    def __init__(self, use_local: bool = False):
        super().__init__(AIEngineType.HUGGINGFACE, "microsoft/BioGPT-Large")
        self.use_local = use_local
        if self.is_available and HUGGINGFACE_AVAILABLE:
            if self.use_local:
                self._init_local_model()
            else:
                self._init_api_client()
    
    def _init_local_model(self):
        """ë¡œì»¬ ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            self.pipeline = pipeline(
                "text-generation",
                model=self.model_name,
                device=0 if torch.cuda.is_available() else -1
            )
        except Exception as e:
            logger.error(f"ë¡œì»¬ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.is_available = False
    
    def _init_api_client(self):
        """HuggingFace API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        self.api_url = f"https://api-inference.huggingface.co/models/{self.model_name}"
        self.headers = {"Authorization": f"Bearer {self.api_key}"}
    
    async def generate(self, prompt: str, user_id: str = "anonymous", 
                      **kwargs) -> APIResponse:
        """HuggingFaceë¡œ í…ìŠ¤íŠ¸ ìƒì„±"""
        start_time = time.time()
        
        # ìºì‹œ í™•ì¸
        cache_params = {'prompt': prompt, **kwargs}
        cached = self.cache.get('huggingface', cache_params)
        if cached:
            return APIResponse(
                status=ResponseStatus.CACHED,
                data=cached,
                cached=True
            )
        
        try:
            if self.use_local and hasattr(self, 'pipeline'):
                # ë¡œì»¬ ì¶”ë¡ 
                result = await asyncio.to_thread(
                    self.pipeline,
                    prompt,
                    max_length=kwargs.get('max_length', 512),
                    temperature=kwargs.get('temperature', 0.7)
                )
                text = result[0]['generated_text']
            else:
                # API í˜¸ì¶œ
                async with aiohttp.ClientSession() as session:
                    payload = {
                        "inputs": prompt,
                        "parameters": {
                            "max_length": kwargs.get('max_length', 512),
                            "temperature": kwargs.get('temperature', 0.7)
                        }
                    }
                    async with session.post(
                        self.api_url,
                        headers=self.headers,
                        json=payload
                    ) as response:
                        result = await response.json()
                        text = result[0]['generated_text']
            
            response_time = time.time() - start_time
            
            # ìºì‹œ ì €ì¥
            self.cache.set('huggingface', cache_params, text)
            
            return APIResponse(
                status=ResponseStatus.SUCCESS,
                data=text,
                metadata={
                    'model': self.model_name,
                    'response_time': response_time
                }
            )
            
        except Exception as e:
            logger.error(f"HuggingFace ì—ëŸ¬: {str(e)}")
            return APIResponse(
                status=ResponseStatus.ERROR,
                error=str(e),
                error_code='3006'
            )

# ===========================================================================
# ğŸ”„ OpenAI í˜¸í™˜ ì—”ì§„ (Grok, DeepSeek, SambaNova)
# ===========================================================================

class OpenAICompatibleEngine(BaseAIEngine):
    """OpenAI API í˜¸í™˜ ì—”ì§„"""
    
    def __init__(self, engine_type: AIEngineType, model_name: str, 
                 base_url: str):
        super().__init__(engine_type, model_name)
        self.base_url = base_url
        if self.is_available and OPENAI_AVAILABLE:
            self.client = AsyncOpenAI(
                api_key=self.api_key,
                base_url=base_url
            )
    
    async def generate(self, prompt: str, user_id: str = "anonymous", 
                      **kwargs) -> APIResponse:
        """OpenAI í˜¸í™˜ APIë¡œ í…ìŠ¤íŠ¸ ìƒì„±"""
        start_time = time.time()
        
        # ìºì‹œ í™•ì¸
        cache_params = {'prompt': prompt, **kwargs}
        cached = self.cache.get(self.engine_type.value, cache_params)
        if cached:
            return APIResponse(
                status=ResponseStatus.CACHED,
                data=cached,
                cached=True
            )
        
        # Rate limit í™•ì¸
        wait_time = self.rate_limiter.wait_if_needed()
        if wait_time > 0:
            await asyncio.sleep(wait_time)
        
        try:
            # API í˜¸ì¶œ
            response = await self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": "You are an expert assistant."},
                    {"role": "user", "content": prompt}
                ],
                temperature=kwargs.get('temperature', 0.7),
                max_tokens=kwargs.get('max_tokens', 4096)
            )
            
            # Rate limit ê¸°ë¡
            self.rate_limiter.record()
            
            # ì‘ë‹µ ì²˜ë¦¬
            result = response.choices[0].message.content
            response_time = time.time() - start_time
            
            # ìºì‹œ ì €ì¥
            self.cache.set(self.engine_type.value, cache_params, result)
            
            # ì‚¬ìš©ëŸ‰ ê¸°ë¡
            tokens = 0
            cost = 0.0
            if hasattr(response, 'usage'):
                tokens = response.usage.total_tokens
                cost = self.estimate_cost(
                    response.usage.prompt_tokens,
                    response.usage.completion_tokens
                )
            
            return APIResponse(
                status=ResponseStatus.SUCCESS,
                data=result,
                metadata={
                    'model': self.model_name,
                    'response_time': response_time,
                    'tokens': tokens,
                    'cost': cost
                }
            )
            
        except Exception as e:
            logger.error(f"{self.engine_type.value} ì—ëŸ¬: {str(e)}")
            error_code = {
                AIEngineType.GROK: '3003',
                AIEngineType.DEEPSEEK: '3004',
                AIEngineType.SAMBANOVA: '3005'
            }.get(self.engine_type, '3000')
            
            return APIResponse(
                status=ResponseStatus.ERROR,
                error=str(e),
                error_code=error_code
            )

# ===========================================================================
# ğŸ§¬ SambaNova ì—”ì§„ (Claude API)
# ===========================================================================

class SambaNovaEngine(BaseAIEngine):
    """SambaNova Claude í˜¸í™˜ ì—”ì§„"""
    
    def __init__(self):
        super().__init__(AIEngineType.SAMBANOVA, "llama-3.1-405b")
        if self.is_available and ANTHROPIC_AVAILABLE:
            self.client = anthropic.AsyncAnthropic(
                api_key=self.api_key,
                base_url="https://api.sambanova.ai/v1"
            )
    
    async def generate(self, prompt: str, user_id: str = "anonymous", 
                      **kwargs) -> APIResponse:
        """SambaNovaë¡œ í…ìŠ¤íŠ¸ ìƒì„±"""
        start_time = time.time()
        
        # ìºì‹œ í™•ì¸
        cache_params = {'prompt': prompt, **kwargs}
        cached = self.cache.get('sambanova', cache_params)
        if cached:
            return APIResponse(
                status=ResponseStatus.CACHED,
                data=cached,
                cached=True
            )
        
        try:
            # API í˜¸ì¶œ
            response = await self.client.messages.create(
                model=self.model_name,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=kwargs.get('max_tokens', 4096),
                temperature=kwargs.get('temperature', 0.7)
            )
            
            # ì‘ë‹µ ì²˜ë¦¬
            result = response.content[0].text
            response_time = time.time() - start_time
            
            # ìºì‹œ ì €ì¥
            self.cache.set('sambanova', cache_params, result)
            
            return APIResponse(
                status=ResponseStatus.SUCCESS,
                data=result,
                metadata={
                    'model': self.model_name,
                    'response_time': response_time
                }
            )
            
        except Exception as e:
            logger.error(f"SambaNova ì—ëŸ¬: {str(e)}")
            return APIResponse(
                status=ResponseStatus.ERROR,
                error=str(e),
                error_code='3005'
            )

# ===========================================================================
# ğŸ”¬ ê³¼í•™ ë°ì´í„°ë² ì´ìŠ¤ í´ë¼ì´ì–¸íŠ¸
# ===========================================================================

class ScienceDBClient:
    """ê³¼í•™ ë°ì´í„°ë² ì´ìŠ¤ í†µí•© í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self):
        self.clients = {}
        self.cache = APICache(CACHE_TTL.get('material_data', 86400))
        self.rate_limiters = {}
        self._init_clients()
    
    def _init_clients(self):
        """í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        # Materials Project
        if MATERIALS_PROJECT_AVAILABLE:
            mp_key = self._get_api_key('materials_project')
            if mp_key:
                self.clients['materials_project'] = MPRester(mp_key)
        
        # PubChem (API í‚¤ ë¶ˆí•„ìš”)
        if PUBCHEM_AVAILABLE:
            self.clients['pubchem'] = pcp
        
        # GitHub
        if GITHUB_AVAILABLE:
            github_token = self._get_api_key('github')
            if github_token:
                self.clients['github'] = Github(github_token)
        
        # OpenAlex (Polite pool)
        if OPENALEX_AVAILABLE:
            email = os.getenv('OPENALEX_EMAIL', 'your-email@example.com')
            Works().config.email = email
            self.clients['openalex'] = Works()
            limits = RATE_LIMITS.get(DatabaseType.OPENALEX, RATE_LIMITS['default'])
            self.rate_limiters['openalex'] = RateLimiter(limits['calls'], limits['period'])
        
        # Crossref
        if CROSSREF_AVAILABLE:
            self.clients['crossref'] = CrossrefWorks()
            limits = RATE_LIMITS.get(DatabaseType.CROSSREF, RATE_LIMITS['default'])
            self.rate_limiters['crossref'] = RateLimiter(limits['calls'], limits['period'])
        
        # arXiv
        if ARXIV_AVAILABLE:
            self.clients['arxiv'] = arxiv
            limits = RATE_LIMITS.get(DatabaseType.ARXIV, RATE_LIMITS['default'])
            self.rate_limiters['arxiv'] = RateLimiter(limits['calls'], limits['period'])
        
        # PubMed
        if PUBMED_AVAILABLE:
            Entrez.email = os.getenv('PUBMED_EMAIL', 'your-email@example.com')
            Entrez.tool = os.getenv('PUBMED_TOOL', 'UniversalDOE')
            self.clients['pubmed'] = Entrez
            limits = RATE_LIMITS.get(DatabaseType.PUBMED, RATE_LIMITS['default'])
            self.rate_limiters['pubmed'] = RateLimiter(limits['calls'], limits['period'])
    
    def _get_api_key(self, service: str) -> Optional[str]:
        """API í‚¤ ê°€ì ¸ì˜¤ê¸°"""
        # ì„¸ì…˜, secrets, í™˜ê²½ë³€ìˆ˜ ìˆœìœ¼ë¡œ í™•ì¸
        if hasattr(st, 'session_state') and 'api_keys' in st.session_state:
            key = st.session_state.api_keys.get(service)
            if key:
                return key
        
        if hasattr(st, 'secrets'):
            try:
                key = st.secrets.get(f"{service}_api_key")
                if key:
                    return key
            except:
                pass
        
        return os.getenv(f"{service.upper()}_API_KEY")
    
    async def search_materials(self, formula: Optional[str] = None,
                             **criteria) -> List[Dict]:
        """ì¬ë£Œ ê²€ìƒ‰"""
        if 'materials_project' not in self.clients:
            return []
        
        # ìºì‹œ í™•ì¸
        cache_params = {'formula': formula, **criteria}
        cached = self.cache.get('materials', cache_params)
        if cached:
            return cached
        
        try:
            mp = self.clients['materials_project']
            # ë™ê¸° í˜¸ì¶œì„ ë¹„ë™ê¸°ë¡œ ë³€í™˜
            results = await asyncio.to_thread(
                mp.materials.search,
                formula=formula,
                **criteria
            )
            
            # ê²°ê³¼ ë³€í™˜
            materials = []
            for material in results:
                materials.append({
                    'material_id': material.material_id,
                    'formula': material.formula,
                    'energy': material.energy_per_atom,
                    'band_gap': material.band_gap,
                    'density': material.density,
                    'crystal_system': material.symmetry.crystal_system
                })
            
            # ìºì‹œ ì €ì¥
            self.cache.set('materials', cache_params, materials)
            
            return materials
            
        except Exception as e:
            logger.error(f"Materials Project ê²€ìƒ‰ ì—ëŸ¬: {str(e)}")
            return []
    
    async def search_compounds(self, name: Optional[str] = None,
                             formula: Optional[str] = None) -> List[Dict]:
        """í™”í•©ë¬¼ ê²€ìƒ‰"""
        if 'pubchem' not in self.clients:
            return []
        
        # ìºì‹œ í™•ì¸
        cache_params = {'name': name, 'formula': formula}
        cached = self.cache.get('compounds', cache_params)
        if cached:
            return cached
        
        try:
            compounds = []
            
            if name:
                # ì´ë¦„ìœ¼ë¡œ ê²€ìƒ‰
                results = await asyncio.to_thread(
                    pcp.get_compounds, name, 'name'
                )
            elif formula:
                # ë¶„ìì‹ìœ¼ë¡œ ê²€ìƒ‰
                results = await asyncio.to_thread(
                    pcp.get_compounds, formula, 'formula'
                )
            else:
                return []
            
            # ê²°ê³¼ ë³€í™˜
            for compound in results[:10]:  # ìµœëŒ€ 10ê°œ
                compounds.append({
                    'cid': compound.cid,
                    'iupac_name': compound.iupac_name,
                    'molecular_formula': compound.molecular_formula,
                    'molecular_weight': compound.molecular_weight,
                    'smiles': compound.canonical_smiles,
                    'synonyms': compound.synonyms[:5] if compound.synonyms else []
                })
            
            # ìºì‹œ ì €ì¥
            self.cache.set('compounds', cache_params, compounds)
            
            return compounds
            
        except Exception as e:
            logger.error(f"PubChem ê²€ìƒ‰ ì—ëŸ¬: {str(e)}")
            return []
    
    async def search_literature(self, query: str, source: str = 'openalex',
                              limit: int = 10) -> List[Dict]:
        """ë¬¸í—Œ ê²€ìƒ‰"""
        if source not in self.clients:
            return []
        
        # ìºì‹œ í™•ì¸
        cache_params = {'query': query, 'source': source, 'limit': limit}
        cached = self.cache.get('literature', cache_params)
        if cached:
            return cached
        
        # Rate limit í™•ì¸
        if source in self.rate_limiters:
            wait_time = self.rate_limiters[source].wait_if_needed()
            if wait_time > 0:
                await asyncio.sleep(wait_time)
        
        try:
            results = []
            
            if source == 'openalex':
                # OpenAlex ê²€ìƒ‰
                works = await asyncio.to_thread(
                    self.clients['openalex'].search,
                    query
                )
                for work in works[:limit]:
                    results.append({
                        'title': work.get('title'),
                        'doi': work.get('doi'),
                        'year': work.get('publication_year'),
                        'authors': [a.get('author', {}).get('display_name') 
                                   for a in work.get('authorships', [])],
                        'abstract': work.get('abstract'),
                        'source': 'OpenAlex'
                    })
            
            elif source == 'crossref':
                # Crossref ê²€ìƒ‰
                works = self.clients['crossref'].query(query).sort('relevance')
                for work in works[:limit]:
                    results.append({
                        'title': work.get('title', [''])[0],
                        'doi': work.get('DOI'),
                        'year': work.get('published-print', {}).get('date-parts', [[None]])[0][0],
                        'authors': [f"{a.get('given', '')} {a.get('family', '')}" 
                                   for a in work.get('author', [])],
                        'abstract': work.get('abstract'),
                        'source': 'Crossref'
                    })
            
            elif source == 'arxiv':
                # arXiv ê²€ìƒ‰
                search = arxiv.Search(
                    query=query,
                    max_results=limit,
                    sort_by=arxiv.SortCriterion.Relevance
                )
                for paper in search.results():
                    results.append({
                        'title': paper.title,
                        'doi': paper.doi,
                        'year': paper.published.year,
                        'authors': [author.name for author in paper.authors],
                        'abstract': paper.summary,
                        'pdf_url': paper.pdf_url,
                        'source': 'arXiv'
                    })
            
            elif source == 'pubmed':
                # PubMed ê²€ìƒ‰
                handle = Entrez.esearch(db="pubmed", term=query, retmax=limit)
                record = Entrez.read(handle)
                handle.close()
                
                if record['IdList']:
                    handle = Entrez.efetch(
                        db="pubmed",
                        id=record['IdList'],
                        rettype="abstract",
                        retmode="xml"
                    )
                    papers = Entrez.read(handle)
                    handle.close()
                    
                    for paper in papers['PubmedArticle']:
                        article = paper['MedlineCitation']['Article']
                        results.append({
                            'title': article.get('ArticleTitle', ''),
                            'doi': None,  # DOI ì¶”ì¶œ ë¡œì§ í•„ìš”
                            'year': article['Journal']['JournalIssue'].get('PubDate', {}).get('Year'),
                            'authors': [f"{author.get('ForeName', '')} {author.get('LastName', '')}"
                                       for author in article.get('AuthorList', [])],
                            'abstract': article.get('Abstract', {}).get('AbstractText', [''])[0],
                            'source': 'PubMed'
                        })
            
            # Rate limit ê¸°ë¡
            if source in self.rate_limiters:
                self.rate_limiters[source].record()
            
            # ìºì‹œ ì €ì¥
            self.cache.set('literature', cache_params, results)
            
            return results
            
        except Exception as e:
            logger.error(f"{source} ê²€ìƒ‰ ì—ëŸ¬: {str(e)}")
            return []

# ===========================================================================
# ğŸ”„ í´ë°± ë©”ì»¤ë‹ˆì¦˜
# ===========================================================================

class FallbackChain:
    """AI ì—”ì§„ í´ë°± ì²´ì¸"""
    
    def __init__(self, engines: List[BaseAIEngine]):
        self.engines = engines
    
    async def generate(self, prompt: str, user_id: str = "anonymous",
                      **kwargs) -> APIResponse:
        """í´ë°± ì²´ì¸ìœ¼ë¡œ ìƒì„±"""
        errors = []
        
        for i, engine in enumerate(self.engines):
            if not engine.is_available:
                continue
            
            try:
                response = await engine.generate(prompt, user_id, **kwargs)
                
                if response.status == ResponseStatus.SUCCESS:
                    if i > 0:  # í´ë°± ì‚¬ìš©
                        response.fallback_used = engine.engine_type.value
                    return response
                else:
                    errors.append({
                        'engine': engine.engine_type.value,
                        'error': response.error
                    })
                    
            except Exception as e:
                errors.append({
                    'engine': engine.engine_type.value,
                    'error': str(e)
                })
        
        # ëª¨ë“  ì—”ì§„ ì‹¤íŒ¨
        return APIResponse(
            status=ResponseStatus.ERROR,
            error="ëª¨ë“  AI ì—”ì§„ í˜¸ì¶œ ì‹¤íŒ¨",
            error_code='3100',
            metadata={'errors': errors}
        )

# ===========================================================================
# ğŸ¯ ë©”ì¸ API ê´€ë¦¬ì
# ===========================================================================

class APIManager:
    """í†µí•© API ê´€ë¦¬ì"""
    
    def __init__(self):
        self.encryption_manager = EncryptionManager()
        self.ai_engines: Dict[str, BaseAIEngine] = {}
        self.db_client = ScienceDBClient()
        self.usage_tracker = UsageTracker()
        self.fallback_chains: Dict[str, FallbackChain] = {}
        self._init_engines()
        self._init_fallback_chains()
        
        logger.info("API Manager ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _init_engines(self):
        """AI ì—”ì§„ ì´ˆê¸°í™”"""
        # Gemini
        if GEMINI_AVAILABLE:
            try:
                engine = GeminiEngine()
                if engine.is_available:
                    self.ai_engines['gemini'] = engine
                    logger.info("Gemini ì—”ì§„ ì´ˆê¸°í™” ì„±ê³µ")
            except Exception as e:
                logger.error(f"Gemini ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        # Groq
        if GROQ_AVAILABLE:
            try:
                engine = GroqEngine()
                if engine.is_available:
                    self.ai_engines['groq'] = engine
                    logger.info("Groq ì—”ì§„ ì´ˆê¸°í™” ì„±ê³µ")
            except Exception as e:
                logger.error(f"Groq ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        # HuggingFace
        if HUGGINGFACE_AVAILABLE:
            try:
                engine = HuggingFaceEngine()
                if engine.is_available:
                    self.ai_engines['huggingface'] = engine
                    logger.info("HuggingFace ì—”ì§„ ì´ˆê¸°í™” ì„±ê³µ")
            except Exception as e:
                logger.error(f"HuggingFace ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        # xAI Grok
        if OPENAI_AVAILABLE:
            try:
                engine = OpenAICompatibleEngine(
                    AIEngineType.GROK,
                    "grok-beta",
                    "https://api.x.ai/v1"
                )
                if engine.is_available:
                    self.ai_engines['grok'] = engine
                    logger.info("xAI Grok ì—”ì§„ ì´ˆê¸°í™” ì„±ê³µ")
            except Exception as e:
                logger.error(f"Grok ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        # DeepSeek
        if OPENAI_AVAILABLE:
            try:
                engine = OpenAICompatibleEngine(
                    AIEngineType.DEEPSEEK,
                    "deepseek-chat",
                    "https://api.deepseek.com/v1"
                )
                if engine.is_available:
                    self.ai_engines['deepseek'] = engine
                    logger.info("DeepSeek ì—”ì§„ ì´ˆê¸°í™” ì„±ê³µ")
            except Exception as e:
                logger.error(f"DeepSeek ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        # SambaNova
        if ANTHROPIC_AVAILABLE:
            try:
                engine = SambaNovaEngine()
                if engine.is_available:
                    self.ai_engines['sambanova'] = engine
                    logger.info("SambaNova ì—”ì§„ ì´ˆê¸°í™” ì„±ê³µ")
            except Exception as e:
                logger.error(f"SambaNova ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def _init_fallback_chains(self):
        """í´ë°± ì²´ì¸ ì´ˆê¸°í™”"""
        # ë¬´ë£Œ ì—”ì§„ ìš°ì„  ì²´ì¸
        free_engines = []
        for engine_id in ['gemini', 'groq', 'sambanova', 'huggingface']:
            if engine_id in self.ai_engines:
                free_engines.append(self.ai_engines[engine_id])
        
        if free_engines:
            self.fallback_chains['free'] = FallbackChain(free_engines)
        
        # ì „ì²´ ì—”ì§„ ì²´ì¸
        all_engines = list(self.ai_engines.values())
        if all_engines:
            self.fallback_chains['all'] = FallbackChain(all_engines)
    
    def get_available_engines(self) -> List[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ AI ì—”ì§„ ëª©ë¡"""
        return list(self.ai_engines.keys())
    
    def set_api_key(self, service: str, api_key: str) -> bool:
        """API í‚¤ ì„¤ì •"""
        try:
            # ì„¸ì…˜ ìƒíƒœì— ì €ì¥
            if 'api_keys' not in st.session_state:
                st.session_state.api_keys = {}
            
            st.session_state.api_keys[service] = api_key
            
            # ì—”ì§„ ì¬ì´ˆê¸°í™”
            self._init_engines()
            self._init_fallback_chains()
            
            return True
        except Exception as e:
            logger.error(f"API í‚¤ ì„¤ì • ì‹¤íŒ¨: {e}")
            return False
    
    async def generate_text(self, engine_id: str, prompt: str,
                          user_id: str = "anonymous", **kwargs) -> APIResponse:
        """AI í…ìŠ¤íŠ¸ ìƒì„±"""
        # ì˜¤í”„ë¼ì¸ ì²´í¬
        if not self._check_connection():
            # ìºì‹œ í™•ì¸
            for engine in self.ai_engines.values():
                cache_params = {'prompt': prompt, **kwargs}
                cached = engine.cache.get(engine_id, cache_params)
                if cached:
                    return APIResponse(
                        status=ResponseStatus.OFFLINE,
                        data=cached,
                        cached=True,
                        metadata={'offline_mode': True}
                    )
            
            return APIResponse(
                status=ResponseStatus.OFFLINE,
                error="ì˜¤í”„ë¼ì¸ ëª¨ë“œ: ìºì‹œëœ ì‘ë‹µì´ ì—†ìŠµë‹ˆë‹¤",
                error_code='5001'  # ë„¤íŠ¸ì›Œí¬ ì—ëŸ¬
            )
        
        # ì—”ì§„ í™•ì¸
        if engine_id not in self.ai_engines:
            # í´ë°± ì²´ì¸ ì‚¬ìš©
            if 'free' in self.fallback_chains:
                response = await self.fallback_chains['free'].generate(
                    prompt, user_id, **kwargs
                )
                if response.status == ResponseStatus.SUCCESS:
                    response.status = ResponseStatus.FALLBACK
                return response
            else:
                return APIResponse(
                    status=ResponseStatus.ERROR,
                    error="ì‚¬ìš© ê°€ëŠ¥í•œ AI ì—”ì§„ì´ ì—†ìŠµë‹ˆë‹¤",
                    error_code='3000'
                )
        
        engine = self.ai_engines[engine_id]
        
        # ìƒì„± ìš”ì²­
        response = await engine.generate(prompt, user_id, **kwargs)
        
        # ì‚¬ìš©ëŸ‰ ê¸°ë¡
        if response.status in [ResponseStatus.SUCCESS, ResponseStatus.CACHED]:
            record = UsageRecord(
                user_id=user_id,
                api_type=engine_id,
                timestamp=datetime.now(),
                tokens_used=response.metadata.get('tokens', 0),
                cost=response.metadata.get('cost', 0.0),
                success=True,
                response_time=response.metadata.get('response_time', 0.0)
            )
            self.usage_tracker.record(record)
        
        return response
    
    async def extract_protocol(self, text: str, file_type: str = "mixed_format",
                             user_id: str = "anonymous", 
                             preferred_engine: str = "gemini") -> APIResponse:
        """í”„ë¡œí† ì½œ ì¶”ì¶œ"""
        # Gemini ìš°ì„  ì‚¬ìš© (í”„ë¡œí† ì½œ ì¶”ì¶œì— ìµœì í™”)
        if preferred_engine in self.ai_engines:
            engine = self.ai_engines[preferred_engine]
            if hasattr(engine, 'extract_protocol'):
                return await engine.extract_protocol(text, file_type, user_id)
        
        # ë‹¤ë¥¸ ì—”ì§„ìœ¼ë¡œ í´ë°±
        prompt = f"""
{EXTRACTION_PROMPTS.get(file_type, EXTRACTION_PROMPTS['mixed_format'])}

í…ìŠ¤íŠ¸:
{text[:10000]}

JSON í˜•ì‹ìœ¼ë¡œ í”„ë¡œí† ì½œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.
"""
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ì—”ì§„ìœ¼ë¡œ ì‹œë„
        if self.ai_engines:
            return await self.generate_text(
                list(self.ai_engines.keys())[0],
                prompt,
                user_id
            )
        else:
            return APIResponse(
                status=ResponseStatus.ERROR,
                error="í”„ë¡œí† ì½œ ì¶”ì¶œì„ ìœ„í•œ AI ì—”ì§„ì´ ì—†ìŠµë‹ˆë‹¤",
                error_code='4202'
            )
    
    async def analyze_experiment(self, experiment_data: Dict,
                               user_id: str = "anonymous",
                               engine_id: Optional[str] = None) -> APIResponse:
        """ì‹¤í—˜ ë°ì´í„° ë¶„ì„"""
        prompt = f"""
ë‹¤ìŒ ì‹¤í—˜ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•˜ì„¸ìš”:

ì‹¤í—˜ ì •ë³´:
{json.dumps(experiment_data, indent=2, ensure_ascii=False)}

ë‹¤ìŒ í•­ëª©ë“¤ì„ í¬í•¨í•˜ì—¬ ë¶„ì„í•˜ì„¸ìš”:
1. ì£¼ìš” ë°œê²¬ì‚¬í•­
2. í†µê³„ì  ìœ ì˜ì„±
3. ê°œì„  ì œì•ˆ
4. ë‹¤ìŒ ì‹¤í—˜ ì¶”ì²œ
5. ë¬¸í—Œê³¼ì˜ ë¹„êµ (ê°€ëŠ¥í•œ ê²½ìš°)
"""
        
        # ì—”ì§„ ì„ íƒ
        if not engine_id:
            # ë¶„ì„ì— ì í•©í•œ ì—”ì§„ ìš°ì„ ìˆœìœ„
            for preferred in ['gemini', 'sambanova', 'groq']:
                if preferred in self.ai_engines:
                    engine_id = preferred
                    break
        
        return await self.generate_text(engine_id or 'gemini', prompt, user_id)
    
    async def search_benchmark_data(self, query: str, 
                                  limit: int = 20) -> List[Dict]:
        """ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° ê²€ìƒ‰"""
        results = []
        
        # ì—¬ëŸ¬ ì†ŒìŠ¤ì—ì„œ ë³‘ë ¬ ê²€ìƒ‰
        tasks = []
        for source in ['openalex', 'crossref', 'arxiv']:
            if source in self.db_client.clients:
                task = self.db_client.search_literature(query, source, limit)
                tasks.append(task)
        
        if tasks:
            all_results = await asyncio.gather(*tasks, return_exceptions=True)
            for result in all_results:
                if isinstance(result, list):
                    results.extend(result)
        
        # ì¤‘ë³µ ì œê±° ë° ê´€ë ¨ì„± ì •ë ¬
        seen = set()
        unique_results = []
        for item in results:
            title = item.get('title', '').lower()
            if title and title not in seen:
                seen.add(title)
                unique_results.append(item)
        
        return unique_results[:limit]
    
    def get_usage_summary(self, user_id: Optional[str] = None,
                         period: str = 'day') -> Dict[str, Any]:
        """ì‚¬ìš©ëŸ‰ ìš”ì•½"""
        return self.usage_tracker.get_summary(user_id, period)
    
    def get_api_status(self) -> Dict[str, Any]:
        """API ìƒíƒœ í™•ì¸"""
        status = {
            'ai_engines': {},
            'databases': {},
            'fallback_chains': list(self.fallback_chains.keys()),
            'total_available': 0,
            'cache_stats': {}
        }
        
        # AI ì—”ì§„ ìƒíƒœ
        for engine_id, engine in self.ai_engines.items():
            status['ai_engines'][engine_id] = {
                'available': engine.is_available,
                'model': engine.model_name,
                'rate_limit': engine.rate_limiter.get_remaining(),
                'cache_stats': engine.cache.get_stats()
            }
            if engine.is_available:
                status['total_available'] += 1
        
        # ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ
        for db_name, client in self.db_client.clients.items():
            status['databases'][db_name] = {
                'available': client is not None
            }
        
        # ì „ì²´ ìºì‹œ í†µê³„
        total_cache_stats = {'hits': 0, 'misses': 0, 'evictions': 0}
        for engine in self.ai_engines.values():
            stats = engine.cache.get_stats()
            for key, value in stats.items():
                total_cache_stats[key] += value
        
        status['cache_stats'] = total_cache_stats
        
        return status
    
    def _check_connection(self) -> bool:
        """ì¸í„°ë„· ì—°ê²° í™•ì¸"""
        try:
            response = requests.get('https://www.google.com', timeout=3)
            return response.status_code == 200
        except:
            return False
    
    def clear_cache(self, cache_type: Optional[str] = None):
        """ìºì‹œ ì´ˆê¸°í™”"""
        for engine in self.ai_engines.values():
            engine.cache.clear(cache_type)
        
        self.db_client.cache.clear(cache_type)
        
        logger.info(f"ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ: {cache_type or 'ì „ì²´'}")
    
    async def multi_engine_consensus(self, prompt: str, 
                                   engines: Optional[List[str]] = None,
                                   user_id: str = "anonymous") -> APIResponse:
        """ì—¬ëŸ¬ AI ì—”ì§„ì˜ í•©ì˜ ì‘ë‹µ ìƒì„±"""
        if not engines:
            engines = list(self.ai_engines.keys())[:3]  # ìµœëŒ€ 3ê°œ
        
        # ë³‘ë ¬ë¡œ ì—¬ëŸ¬ ì—”ì§„ í˜¸ì¶œ
        tasks = []
        for engine_id in engines:
            if engine_id in self.ai_engines:
                task = self.generate_text(engine_id, prompt, user_id)
                tasks.append((engine_id, task))
        
        if not tasks:
            return APIResponse(
                status=ResponseStatus.ERROR,
                error="ì‚¬ìš© ê°€ëŠ¥í•œ ì—”ì§„ì´ ì—†ìŠµë‹ˆë‹¤",
                error_code='3000'
            )
        
        # ê²°ê³¼ ìˆ˜ì§‘
        results = []
        for engine_id, task in tasks:
            try:
                response = await task
                if response.status == ResponseStatus.SUCCESS:
                    results.append({
                        'engine': engine_id,
                        'response': response.data
                    })
            except Exception as e:
                logger.error(f"{engine_id} í•©ì˜ ìƒì„± ì‹¤íŒ¨: {e}")
        
        if not results:
            return APIResponse(
                status=ResponseStatus.ERROR,
                error="ëª¨ë“  ì—”ì§„ í˜¸ì¶œ ì‹¤íŒ¨",
                error_code='3100'
            )
        
        # ê²°ê³¼ í†µí•©
        consensus_data = {
            'responses': results,
            'consensus': self._generate_consensus(results),
            'engines_used': [r['engine'] for r in results]
        }
        
        return APIResponse(
            status=ResponseStatus.SUCCESS,
            data=consensus_data,
            metadata={'consensus_count': len(results)}
        )
    
    def _generate_consensus(self, results: List[Dict]) -> str:
        """ì‘ë‹µë“¤ë¡œë¶€í„° í•©ì˜ ìƒì„±"""
        if len(results) == 1:
            return results[0]['response']
        
        # ê°„ë‹¨í•œ í•©ì˜: ê³µí†µ ë‚´ìš© ì¶”ì¶œ
        # ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ ì•Œê³ ë¦¬ì¦˜ í•„ìš”
        consensus = "ë‹¤ìŒì€ ì—¬ëŸ¬ AI ì—”ì§„ì˜ ì¢…í•©ì ì¸ ê²¬í•´ì…ë‹ˆë‹¤:\n\n"
        for i, result in enumerate(results, 1):
            consensus += f"{i}. {result['engine']} ì˜ê²¬:\n"
            consensus += f"{result['response'][:200]}...\n\n"
        
        return consensus

# ===========================================================================
# ğŸ”§ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
# ===========================================================================

_api_manager: Optional[APIManager] = None

def get_api_manager() -> APIManager:
    """API Manager ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _api_manager
    
    if _api_manager is None:
        _api_manager = APIManager()
    
    return _api_manager

# ===========================================================================
# ğŸ¯ í—¬í¼ í•¨ìˆ˜
# ===========================================================================

async def ask_ai(prompt: str, engine: str = "auto",
                user_id: str = "anonymous", **kwargs) -> str:
    """ê°„í¸ AI ì§ˆë¬¸ í•¨ìˆ˜"""
    manager = get_api_manager()
    
    if engine == "auto":
        # ìë™ ì—”ì§„ ì„ íƒ
        available = manager.get_available_engines()
        if available:
            engine = available[0]
        else:
            return "ì˜¤ë¥˜: ì‚¬ìš© ê°€ëŠ¥í•œ AI ì—”ì§„ì´ ì—†ìŠµë‹ˆë‹¤"
    
    response = await manager.generate_text(engine, prompt, user_id, **kwargs)
    
    if response.status in [ResponseStatus.SUCCESS, ResponseStatus.CACHED, 
                          ResponseStatus.FALLBACK]:
        return response.data
    else:
        return f"ì˜¤ë¥˜: {response.error}"

def get_available_ai_engines() -> List[str]:
    """ì‚¬ìš© ê°€ëŠ¥í•œ AI ì—”ì§„ ëª©ë¡"""
    manager = get_api_manager()
    return manager.get_available_engines()

async def search_materials(formula: str = None, **criteria) -> List[Dict]:
    """ì¬ë£Œ ê²€ìƒ‰ í—¬í¼"""
    manager = get_api_manager()
    return await manager.db_client.search_materials(formula, **criteria)

async def search_compounds(name: str = None, formula: str = None) -> List[Dict]:
    """í™”í•©ë¬¼ ê²€ìƒ‰ í—¬í¼"""
    manager = get_api_manager()
    return await manager.db_client.search_compounds(name, formula)

async def search_literature(query: str, sources: List[str] = None, 
                          limit: int = 10) -> List[Dict]:
    """ë¬¸í—Œ ê²€ìƒ‰ í—¬í¼"""
    manager = get_api_manager()
    
    if not sources:
        sources = ['openalex', 'crossref']
    
    all_results = []
    for source in sources:
        results = await manager.db_client.search_literature(query, source, limit)
        all_results.extend(results)
    
    return all_results

# ===========================================================================
# ğŸ§ª í…ŒìŠ¤íŠ¸ ì½”ë“œ
# ===========================================================================

if __name__ == "__main__":
    import asyncio
    
    async def test_api_manager():
        """API Manager í…ŒìŠ¤íŠ¸"""
        manager = get_api_manager()
        
        # ìƒíƒœ í™•ì¸
        print("API ìƒíƒœ:", json.dumps(manager.get_api_status(), indent=2))
        
        # í…ìŠ¤íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸
        if manager.get_available_engines():
            engine = manager.get_available_engines()[0]
            response = await manager.generate_text(
                engine,
                "ê³ ë¶„ì í•©ì„±ì˜ ê¸°ë³¸ ì›ë¦¬ë¥¼ ì„¤ëª…í•˜ì„¸ìš”.",
                "test_user"
            )
            print(f"\n{engine} ì‘ë‹µ:", response.data[:200] if response.data else response.error)
        
        # í”„ë¡œí† ì½œ ì¶”ì¶œ í…ŒìŠ¤íŠ¸
        test_text = """
        Materials: Polymer A (98% purity, Sigma-Aldrich), Solvent B
        
        Procedure:
        1. Dissolve 5g of Polymer A in 100mL of Solvent B
        2. Heat to 80Â°C for 2 hours
        3. Cool to room temperature
        """
        
        protocol_response = await manager.extract_protocol(test_text)
        print("\ní”„ë¡œí† ì½œ ì¶”ì¶œ:", protocol_response.data)
        
        # ë¬¸í—Œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        if 'openalex' in manager.db_client.clients:
            papers = await manager.db_client.search_literature(
                "polymer synthesis electrospinning",
                "openalex",
                5
            )
            print(f"\në¬¸í—Œ ê²€ìƒ‰ ê²°ê³¼: {len(papers)}ê°œ")
            if papers:
                print(f"ì²« ë²ˆì§¸ ë…¼ë¬¸: {papers[0].get('title')}")
        
        # ì‚¬ìš©ëŸ‰ ìš”ì•½
        usage = manager.get_usage_summary("test_user", "day")
        print("\nì‚¬ìš©ëŸ‰ ìš”ì•½:", json.dumps(usage, indent=2))
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    asyncio.run(test_api_manager())
